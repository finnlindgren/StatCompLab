[{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Lab 4, consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] lab, one considered parameterisation alternatives \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) confidence interval construction motivated ratio \\(\\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\lambda/n}}\\), approximated \\(\\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\widehat{\\lambda}/n}}\\). now wish see achieve better construction using non-approximated version. Solve inequalities \\[ = z_{\\alpha/2} < \\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\lambda/n}} < z_{1-\\alpha/2} = b \\] respect \\(\\lambda\\), implement resulting confidence interval construction function (code.R; also document function) taking arguments y alpha. Also include another function method used lab, call syntax. Hint: may help mathematical derivation several steps; e.g. introducing \\(x=\\sqrt{\\lambda}\\), solve two separate quadratic equations (\\(\\) one \\(b\\)) work intervals two inequalities fulfilled. , likely need fact \\(\\alpha<0.5\\), \\(z_{\\alpha/2}<0\\) \\(z_{1-\\alpha/2}>0\\). function estimate_coverage, defined code.R, can used estimate coverage construction confidence intervals samples \\(\\mathsf{Poisson}(\\lambda)\\), given interval construction R function. Use discuss following questions: two alternatives coverage closest nominal coverage 90% case \\(n=2\\), \\(\\lambda=3\\)? happens larger values \\(n\\)?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"d-printer-materials-prediction","dir":"Articles","previous_headings":"","what":"3D printer materials prediction","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"aim estimate parameters Bayesian statistical model material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator made simple physics analysis, settled model connection CAD_Weight Actual_Weight follows linear model, variance increases square CAD_Weight. denote CAD weight observations \\(\\) x_i, corresponding actual weight \\(y_i\\), model can defined \\[ y_i \\sim \\mathsf{Normal}[\\beta_1 + \\beta_2 x_i, \\beta_3 + \\beta_4 x_i^2)] . \\] ensure positivity variance, parameterisation \\(\\boldsymbol{\\theta}=[\\theta_1,\\theta_2,\\theta_3,\\theta_4]=[\\beta_1,\\beta_2,\\log(\\beta_3),\\log(\\beta_4)]\\) introduced, printer operator assigns independent prior distributions follows: \\[ \\begin{aligned} \\theta_1 &\\sim \\mathsf{Normal}(0,\\gamma_1), \\\\ \\theta_2 &\\sim \\mathsf{Normal}(1,\\gamma_2), \\\\ \\theta_3 &\\sim \\mathsf{LogExp}(\\gamma_3), \\\\ \\theta_4 &\\sim \\mathsf{LogExp}(\\gamma_4), \\end{aligned} \\] \\(\\mathsf{LogExp}()\\) denotes logarithm exponentially distributed random variable rate parameter \\(\\), seen Tutorial 4. \\(\\boldsymbol{\\gamma}=(\\gamma_1,\\gamma_2,\\gamma_3,\\gamma_4)\\) values positive parameters. printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, lead model, approximation . basic physics assumption error CAD software calculation weight proportional weight . Start loading data plot .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"prior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Prior density","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"help dnorm dlogexp function (see code.R file documentation), define document (code.R) function log_prior_density arguments theta params, theta \\(\\boldsymbol{\\theta}\\) parameter vector, params vector \\(\\boldsymbol{\\gamma}\\) parameters. function evaluate logarithm joint prior density \\(p(\\boldsymbol{\\theta})\\) four \\(\\theta_i\\) parameters.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"observation-likelihood","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Observation likelihood","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"help dnorm, define document function log_like, taking arguments theta, x, y, evaluates observation log-likelihood \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) model defined .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior density","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Define document function log_posterior_density arguments theta, x, y, params, evaluates logarithm posterior density \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\), apart unevaluated normalisation constant.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-mode-and-gaussian-approximation","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior mode and Gaussian approximation","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Let \\(\\gamma_i=1\\), \\(=1,2,3,4\\), use optim() together log_posterior_density filament data find posterior mode \\(\\boldsymbol{\\theta}\\). Also evaluate inverse negated Hessian mode, order obtain multivariate Normal approximation \\(\\mathsf{Normal}(\\boldsymbol{\\mu},\\boldsymbol{S})\\) posterior distribution \\(\\boldsymbol{\\theta}\\). See documentation optim maximisation instead minimisation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling function","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"aim construct 90% Bayesian credible interval \\(\\beta_j\\) using importance sampling, similarly method used lab 4. , one dimensional Gaussian approximation posterior parameter used. , instead use multivariate Normal approximation importance sampling distribution. functions rmvnorm dmvnorm mvtnorm package can used sample evaluate densities. Define document function do_importance taking arguments N (number samples generate), mu (mean vector importance distribution), S (covariance matrix), additional parameters needed function code. function output data.frame five columns, beta1, beta2, beta3, beta4, log_weights, containing \\(\\beta_i\\) samples normalised \\(\\log\\)-importance-weights, sum(exp(log_weights)) \\(1\\). Use log_sum_exp function compute needed normalisation information.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Use defined functions compute importance sample size \\(N=10000\\). Plot empirical weighted CDFs together un-weighted CDFs parameter, help stat_ewcdf, discuss results. achieve simpler ggplot code, may find pivot_longer(???, starts_with(\"beta\")) facet_wrap(vars(name)) useful. Construct 90% credible intervals four model parameters, based importance sample. addition wquantile pivot_longer, methods group_by summarise helpful. may wish define function make_CI taking arguments x, weights, prob (control intended coverage probability), generating 1-row, 2-column data.frame help structure code. Discuss results sampling method point view 3D printer application point view (may also involve e.g. plotting prediction intervals based point estimates parameters, plotting importance log-weights see depend sampled \\(\\beta\\)-values, like T4sol document).","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-problem","dir":"Articles","previous_headings":"Avoiding long running times","what":"The problem","title":"StatComp Project 1 (2021/22): Hints","text":"Collecting values data.frame inside loop can slow. common issue combining results computed loops using rbind step loop add one rows large vector data.frame can slow; instead linear computational cost size output, cost can become quadratic; needs reallocate memory copy previous results, new version result object.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-solution","dir":"Articles","previous_headings":"Avoiding long running times","what":"The solution","title":"StatComp Project 1 (2021/22): Hints","text":"key avoid reallocating memory instead pre-allocating whole result object loop. However subtleties , examples blow show differences approaches. common solution work simpler intermediate data structures within loop, collect results end. Indexing simple vectors much cheaper reallocating copying memory. Slow: Better, even data.frame indexing assignments causes memory reallocation: Fast; several orders magnitude faster (see timings ) avoiding memory reallocation inside loop: fast, bit compact; using matrix indexing doesn’t need memory reallocation, bit slower vector indexing. potential drawback method works columns output data.frame type (numeric), cases works code simple, readable, generally quick run: Benchmark timing comparisons:","code":"slow_fun <- function(N) {   result <- data.frame()   for (loop in seq_len(N)) {     result <- rbind(       result,       data.frame(A = cos(loop), B = sin(loop))     )   }   result } better_fun <- function(N) {   result <- data.frame(A = numeric(N), B = numeric(N))   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   result } fast_fun <- function(N) {   result_A <- numeric(N)   result_B <- numeric(N)   for (loop in seq_len(N)) {     result_A[loop] <- cos(loop)     result_B[loop] <- sin(loop)   }   data.frame(     A = result_A,     B = result_B   ) } ok_fun <- function(N) {   result <- matrix(0, N, 2)   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   colnames(result) <- c(\"A\", \"B\")   as.data.frame(result) } N <- 10000 bench::mark(   slow = slow_fun(N),   better = better_fun(N),   fast = fast_fun(N),   ok = ok_fun(N) ) #> Warning: Some expressions had a GC in every iteration; so filtering is #> disabled. #> # A tibble: 4 × 6 #>   expression      min   median `itr/sec` mem_alloc `gc/sec` #>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #> 1 slow          3.89s    3.89s     0.257    2.28GB    14.2  #> 2 better     580.14ms 580.14ms     1.72     1.49GB    60.3  #> 3 fast         1.85ms   1.88ms   529.     229.67KB     2.00 #> 4 ok           5.84ms   5.97ms   160.     458.21KB    12.0"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"dynamically-generated-column-names","dir":"Articles","previous_headings":"","what":"Dynamically generated column names","title":"StatComp Project 1 (2021/22): Hints","text":"size matrix data.frame depends parameter, multiple columns similar names, can convenient dynamically generate column names. paste() function can used . example, want data.frame size \\(n\\times m\\), column names Name1, Name2, etc, can use resulting object looks like :","code":"df <- matrix(runif(15), 5, 3) # Create a matrix with some values in it colnames(df) <- paste(\"Name\", seq_len(ncol(df)), sep = \"\") df <- as.data.frame(df) df #>       Name1     Name2     Name3 #> 1 0.3957881 0.9171955 0.8133229 #> 2 0.2645212 0.6913644 0.4541224 #> 3 0.4075499 0.6832017 0.6217285 #> 4 0.1401019 0.6203553 0.1091926 #> 5 0.2916905 0.3251430 0.1603317"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"multi-column-summarise-constructions","dir":"Articles","previous_headings":"","what":"Multi-column summarise constructions","title":"StatComp Project 1 (2021/22): Hints","text":"summarise() method dplyr package usually used construct simple summaries function computes mu sigma? make return data.frame, can used summarise:","code":"suppressPackageStartupMessages(library(tidyverse)) df <- data.frame(x = rnorm(100)) df %>%   summarise(     mu = mean(x),     sigma = sd(x)   ) #>            mu    sigma #> 1 -0.04112715 1.050919 my_fun <- function(x) {   data.frame(     mu = mean(x),     sigma = sd(x)   ) } df %>%   summarise(     my_fun(x)   ) #>            mu    sigma #> 1 -0.04112715 1.050919"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"avoiding-unnecessary-for-loops","dir":"Articles","previous_headings":"","what":"Avoiding unnecessary for-loops","title":"StatComp Project 1 (2021/22): Hints","text":"Since operations functions R vectorised, code can often written clear way avoiding unnecessary -loops. simple example, say want compute \\(\\sum_{k=1}^{100} \\cos(k)\\). -loop solution style good solution high performance low-level language like C might look like : taking advantage vectorised calling cos(), function sum(), can reformulate shorter clear vectorised solution: commonly used functions involving vectors TRUE FALSE () (). example, sum, , also operate across elements matrix. row-wise column-wise sums, see rowSums()colSums().","code":"result <- 0 for (k in seq_len(100)) {   result <- result + cos(k) } result <- sum(cos(seq_len(100))) vec <- c(TRUE, TRUE, FALSE, TRUE) all(vec) #> [1] FALSE any(vec) #> [1] TRUE sum(vec) # FALSE == 0, TRUE == 1 #> [1] 3"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"aligning-equations-in-multi-step-derivations","dir":"Articles","previous_headings":"LaTeX equations","what":"Aligning equations in multi-step derivations","title":"StatComp Project 1 (2021/22): Hints","text":"typesetting multi-step derivations, one align equations. LaTeX, can done align align* environments. case, might work RMarkdown, one can use aligned environment instead.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-1","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 1","title":"StatComp Project 1 (2021/22): Hints","text":"Result: \\[\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}\\]","code":"\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-2","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 2","title":"StatComp Project 1 (2021/22): Hints","text":"Result: \\[ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} \\]","code":"$$ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} $$"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example: Numerical statistics","text":"Lab 4, consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] lab, considered three parameterisation alternatives: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) know inverse expected Fisher information \\(\\lambda/n\\) case 1, \\(1/(4n)\\) case 2, \\(1/(n\\lambda)\\) case 3. lab, three functions, CI1, CI2, CI3, defined constructing confidence intervals based asymptotic Normal approximation Maximum Likelihood estimators parameterisations. project, use function pois_CI defined code.R file project repository. code file imported method shown RMdemo.Rmd file, displayed report. Look file documentation functions. task simulation study assess accuracy three interval construction methods. my_code.R file, define function multi_pois_CI(m, n, lambda, alpha,type) takes inputs m, indicating number simulations perform, n, number observations used sample, lambda, true parameter value, well alpha type interpretation pois_CI. function must return data frame m rows two columns, named Lower Upper containing confidence intervals, one row. Show mathematically widths \\(\\lambda\\)-intervals parameterisation type 1 2 always identical , given observation vector \\(\\boldsymbol{y}\\) confidence level \\(1-\\alpha\\). Note: ignore cases \\(\\theta\\)- \\(\\lambda\\)-intervals extend \\(0\\) (case 1, normal quantile \\(z\\) \\(\\overline{y} < z^2/n\\), case 2, \\(4\\overline{y} < z^2/n\\)). my_code.R file, define function tidy_multi_pois_CI(m, n, lambda, alpha) takes inputs multi_pois_CI except type argument. function must return data frame 3*m rows three columns, named Lower, Upper, Type containing confidence intervals three calls multi_pois_CI, Type column containing values 1, 2, 3 indicate method calculated interval. help tidy_multi_pois_CI, estimate coverage probability nominal confidence level \\(1-\\alpha\\)=90%, three interval types, \\(n=2\\), \\(\\lambda=3\\), using \\(m\\) least \\(100000\\). well coverage probabilities match nominal confidence level method? tidyverse package loaded, group_by summarise functions can used avoid direct indexing loops. Summarise discuss results. Plot empirical CDFs interval widths three methods, common figure. Use factor(Type) group information colour. Also compute median interval width method. Summarise discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic Sea","title":"StatComp Project Example: Numerical statistics","text":"Lab 3, investigated data archaeological excavation Gotland Baltic sea. 1920s, battle gravesite 1361 subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. wanted figure many persons likely buried gravesite. must reasonably least \\(256\\), many ? assignment, use importance sampling estimate credible intervals problem. lab, seemed difficult extract much information sample, since two observations two parameters. investigate including data excavations type might decrease uncertainty number persons buried, improved information average detection probability, \\(\\phi\\). function arch_data code.R returns data frame data four different excavations (note: one real; three synthetic data generated assignment). assume model lab excavation, unknown \\(N_j\\), \\(j=1,\\dots,J\\), \\(J\\) excavations, common detection probability \\(\\phi\\). assume model , conditionally \\(\\{N_j\\}\\) \\(\\phi\\), \\((Y_{j,}|N_j,\\phi)\\sim\\mathsf{Bin}(N_j,\\phi)\\), \\(=1,2\\), independent, prior distributions \\(N_j\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\), \\[ \\begin{aligned} p_{Y_{j,}|N_j=n,\\phi}(y) = \\mathsf{P}(Y_{j,}=y|N_j=n,\\phi) &= {n \\choose y} \\phi^y(1-\\phi)^{n-y}, \\quad y=0,\\dots,n, \\\\ p_{N_j}(n) = \\mathsf{P}(N_j=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\(0,1) , \\end{aligned} \\] \\(B(,b)\\) Beta function, see ?beta R.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"joint-probability-function-for-boldsymbolnboldsymboly","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Joint probability function for \\((\\boldsymbol{N},\\boldsymbol{Y})\\)","title":"StatComp Project Example: Numerical statistics","text":"Conditionally \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), observations \\(\\boldsymbol{Y}\\), conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\) \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) \\(\\widetilde{}=+\\sum_{j,} Y_{j,}\\) \\(\\widetilde{b}=b+2\\sum_j N_j -\\sum_{j,} Y_{j,}\\), sums go \\(j=1,\\dots,J\\) \\(=1,2\\). Show joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\), \\(N_j\\geq\\max(Y_{j,1},Y_{j,2})\\) \\(Y_{j,}=0,1,2,\\dots\\), given by1 \\[ p(\\boldsymbol{N},\\boldsymbol{Y}) = \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)} \\prod_{j=1}^J \\left[ p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\right], \\] \\(p(N_j)\\) probability mass function Geometric distribution prior \\(N_j\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"probability-function-implementation","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Probability function implementation","title":"StatComp Project Example: Numerical statistics","text":"help functions lbeta, lchoose, dgeom, define (my_code.R) function log_prob_NY(N,Y,xi,,b) arguments N (vector length \\(J\\)) \\(Y\\) (data frame size \\(J\\times 2\\), formatted like output arch_data(J)). arguments xi, , b model hyperparameters \\(\\xi\\), \\(\\), \\(b\\). function return logarithm \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\) combination \\(\\boldsymbol{N}\\) \\(\\boldsymbol{Y}\\) valid, otherwise return -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Importance sampling function","title":"StatComp Project Example: Numerical statistics","text":"aim construct Bayesian credible interval \\(N_j\\) using importance sampling, similarly method used lab 4. , Gaussian approximation posterior parameter used. , instead use prior distribution \\(\\boldsymbol{N}\\) sampling distribution, samples \\(N_j^{[k]}\\sim\\mathsf{Geom}(\\xi)\\). Since observations \\(\\boldsymbol{Y}\\) fixed, posterior probability function \\(p(\\boldsymbol{N}|\\boldsymbol{Y})\\) \\(\\boldsymbol{N}\\) proportional \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\). define logarithm unnormalised importance weights, \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[p(\\boldsymbol{N}^{[k]})]\\), \\(p(\\boldsymbol{N}^{[k]})\\) product \\(J\\) geometric distribution probabilities. Define (my_code.R) function arch_importance(K,Y,xi,,b) arguments Y, xi, , b , first argument K defining number samples generate. function return data frame \\(K\\) rows \\(J+1\\) columns, named N1, N2, etc, \\(j=1,\\dots,J\\), containing samples prior distributions \\(N_j\\), final column called Log_Weights, containing re-normalised log-importance-weights, constructed shifting \\(\\log[w^{[k]}]\\) subtracting largest \\(\\log[w^{[k]}]\\) value, lab 4.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"more-efficient-alternative","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea > Importance sampling function","what":"More efficient alternative","title":"StatComp Project Example: Numerical statistics","text":"Using \\(\\mathsf{Geom}(\\xi)\\) prior distribution \\(N_j\\) leads rather inefficient importance sampler, especially \\(J=4\\); weights become zero (\\(N_j^{[k]}<Y_{j,}\\)) near zeros, samples influence practical estimates credible intervals. alternative, can use sampling distributions better adapted posterior distributions. Let \\(N^\\text{min}_j=\\max(Y_{j,1}, Y_{j,2})\\) smallest allowed value sample, \\(j=1,\\dots,J\\). eliminate impossible combinations, weights (theory numerically) positive. improve method, can choose different values probability parameter samples, also adapt differently \\(j\\). Without motivation, can use values \\(\\xi_j=1/(1 + 4 N^\\text{min}_j)\\) sampling, \\(N_j^{[k]}=N^\\text{min}_j + \\mathsf{Geom}(\\xi_j)\\). Note: Due nature geometric distribution, also equivalent ordinary \\(\\mathsf{Geom}(\\xi_j)\\) conditioned least \\(N^\\text{min}_j\\). take different sampling method account, log-weights need defined \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[\\widetilde{p}(\\boldsymbol{N}^{[k]})]\\), \\(\\widetilde{p}(\\boldsymbol{N}^{[k]})=\\prod_{j=1}^J p_{\\mathsf{Geom}(\\xi_j)}(N_j^{[k]}-N^\\text{min}_j)\\), product \\(J\\) different geometric distribution probabilities. may adapt example code (details depend vectorisation method use): Using method, \\(K\\) doesn’t need larger \\(100000\\), small \\(K=1000\\) starts give reasonable values code testing purposes, \\(K=10000\\) take seconds run code fully vectorised (vapply function can used, basic -loop also fine case).","code":"# matrix of minimum allowed N for each j, repeated K times N_min <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J) xi_sample <- 1 / (1 + 4 * N_min) # Sample values and call log_prob_NY for each sample, # store N-values in a K-by-J matrix, and log(p_NY)-values in a vector log_PY N <- matrix(rgeom(..., prob = xi_sample), K, J) + N_min log_PY <- ... # Subtract the sampling log-probabilities log_PY - rowSums(dgeom(N - N_min, prob = xi_sample, log = TRUE))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-n_j","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(N_j\\)","title":"StatComp Project Example: Numerical statistics","text":"lab, let \\(\\xi=1/1001\\), \\(=b=1/2\\). Use arch_importance wquantile function type=1 (see help text information) compute credible intervals \\(N_j\\). Start including data single excavation, observe interval \\(N_1\\) changes data excavations added. Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(\\phi\\)","title":"StatComp Project Example: Numerical statistics","text":"Update2 arch_importance function add column Phi samples conditional distribution \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) row importance sample. Use updated function construct credible intervals \\(\\phi\\), one \\(J=1\\), one \\(J=4\\). Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"investigate behaviour three approximate methods confidence interval construction expectation parameter \\(\\lambda\\) model observations \\(y_i\\sim\\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\). denote three methods given assignment \\(\\textrm{CI}_1\\), \\(\\textrm{CI}_2\\), \\(\\textrm{CI}_3\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"theoretical-interval-width-equivalence","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Theoretical interval width equivalence","title":"StatComp Project Example Solution: Numerical Statistics","text":"Let \\(z=z_{1-\\alpha/2}=-z_{\\alpha/2}\\), \\(z_p\\) (lower) \\(p\\)-quantile standard \\(\\textrm{N}(0,1)\\) distribution. , \\(\\overline{y} > z^2/n\\), Methods 1 2 pois_CI constructing confidence interval \\(\\lambda\\) observations \\(y_i\\sim \\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\), given \\[\\begin{align*} \\textrm{CI}_1 &= (\\overline{y} - z\\sqrt{\\overline{y}/n}, \\overline{y} + z\\sqrt{\\overline{y}/n}), \\\\ \\textrm{CI}_2 &= ([\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2, [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2). \\end{align*}\\] interval width \\(\\textrm{CI}_1\\) \\(2z\\sqrt{\\overline{y}/n}\\). \\(\\textrm{CI}_2\\), width \\[\\begin{align*} [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2 - [\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2 &= [\\overline{y} + 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] - [\\overline{y} - 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] \\\\ &= 4z\\sqrt{\\overline{y}}/\\sqrt{4n} \\end{align*}\\] since remaining terms cancel . Finally \\(4/\\sqrt{4}=2\\), see width \\(\\textrm{CI}_2\\) interval \\(\\textrm{CI}_1\\), observed value \\(\\overline{y}>z^2/n\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-coverage-and-width-assessment","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Confidence interval coverage and width assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"interested behaviour three confidence interval methods far away asymptotic limit. Instead assuming \\(n\\rightarrow\\infty\\) \\(\\lambda\\rightarrow\\infty\\) instead look case \\(n=2\\), \\(\\lambda=3\\), discrete nature sampling distribution maximum likelihood estimator \\(\\widehat{\\lambda}=\\overline{y}\\) still apparent. my_code.R file, ’ve defined two helper functions, multi_pos_CI tidy_multi_pois_CI, order simulate \\(m\\) realisations model, compute three types approximate confidence intervals desired confidence level (also called target nominal confidence level) 90%, corresponding nominal error probability \\(\\alpha=0.1\\). get reasonable precision results, use \\(m=100000\\). See code appendix short analysis code. assess quality methods, compute estimate coverage probability proportion \\(m\\) simulations intervals cover true \\(\\lambda\\) value, Coverage table . also compute median width intervals, denoted MedWidth. extra details, estimate probability lower endpoint large, OutsideLower, upper endpoint low, OutsideUpper. Finally, compute median lower upper endpoints, denoted MedLower MedUpper table. expected theoretical calculations intervals don’t reach \\(0\\) left, median width methods 1 2 identical, whereas method 3 widths slightly larger. see methods 1 3 low high coverage, respectively, compared nominal overage \\(90\\%\\), method 2 coverage close nominal, \\(89.58\\%\\). ’s clear method 1 make ’s mistakes upper end, much \\(15\\%\\) probability low upper endpoint. contrast, method 3 overly cautious upper end, just \\(1.5\\%\\) error probability upper tail, expense wider intervals, sometimes stretching way \\(+\\infty\\). Method 2 appears strike good balance coverage probability interval widths, lower tail error probability almost method 3 (ca \\(4\\%\\)), moving upper endpoint left, thereby reducing interval widths increasing upper tail error probability around \\(6\\%\\). result, method 2 nearly equal tail error probabilities (perfect equality 5%) test case \\(n=2\\), \\(\\lambda=3\\), appears best three methods capturing behaviour sampling distribution \\(\\lambda\\)-estimator. figure , ’ve plotted empirical cumulative distribution functions interval widths three methods, clearly shows discrete nature interval contructions, since based \\(\\overline{y}\\), clear discrete set possible outcomes. equal width property methods 1 2 breaks small values \\(\\overline{y}\\), due handle intervals gone zero simplistic implementation. summary, method 2 appears preferable, despite three methods based asympototic maximum likelihood theory. small sample sizes, asymptotic theory doesn’t tell whole story, instead one look carefully problem structure order get reliable estimates.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"StatComp Project Example Solution: Numerical Statistics","text":"Pairs counts \\((Y_{j,1},Y_{j,2})\\) left right femurs archaeological excavations modelled conditionally independent realisations \\(\\textrm{Binom}(N_j,\\phi)\\), \\(N_j\\) original number people buried excavtion site, \\(\\phi\\) probability finding buried femur. prior distribution \\(N_j\\), \\(j=1,\\dots,J\\), \\(\\textrm{Geom}(\\xi)\\), \\(\\xi=1/(1+1000)\\) (\\(\\textrm{E}(N_j)=1000\\) priori), detection probability, \\(\\phi\\sim\\textrm{Beta}(\\frac{1}{2},\\frac{1}{2})\\). denote \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\) \\(\\boldsymbol{Y}=\\{(Y_{1,1},Y_{1,2}),\\dots,(Y_{J,1},Y_{J,2})\\}\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"joint-probability-function","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Joint probability function","title":"StatComp Project Example Solution: Numerical Statistics","text":"Taking product prior density \\(\\phi\\), prior probabilities \\(N_j\\), conditional observation probabilities \\(N_{j,}\\) integrating \\(\\phi\\), get joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\) \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\int_0^1 p_\\phi(\\phi) \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 p(Y_{j,}|N_j,\\phi)\\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\phi^{Y_{j,}}(1-\\phi)^{N_j-Y_{j,}}   \\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(,b)} \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} , \\end{aligned} \\] \\(\\widetilde{} = +\\sum_{j=1}^J\\sum_{=1}^2 Y_{j,}\\) \\(\\widetilde{b} = b+\\sum_{j=1}^J\\sum_{=1}^2 (N_j - Y_{j,})\\). integral can renormalised integral distribution probability density function \\(\\textrm{Beta}(\\widetilde{},\\widetilde{b})\\) distribution: \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(\\widetilde{},\\widetilde{b})}   \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\\\ &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} . \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"credible-intervals-for-n_j-and-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Credible intervals for \\(N_j\\) and \\(\\phi\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"order obtain posterior credible intervals unknown quantities \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), \\(\\phi\\), implement importance sampling method sample \\(N_j\\) shifted Geometric distribution smallest value equal smallest valid \\(N_j\\), given observed counts, expectation corresponding \\(\\phi=1/4\\). sampling \\(\\boldsymbol{N}\\) values, corresponding importance weights \\(w_k\\), stored Log_Weights equal \\(\\log(w_k)\\) normalised subtracting larges \\(\\log(w_k)\\) value. Finally, \\(\\phi\\) values sampled directly conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\sim\\textrm{Beta}\\left(\\frac{1}{2}+\\sum_{j,} Y_{,j},\\frac{1}{2}+\\sum_{j,} (N_j-Y_{,j})\\right)\\). implementation arch_importance() function my_code.R shown code appendix, produces importance sample size \\(K\\). achieve relatively reliable results, use \\(K=100000\\), \\(J=1\\), \\(2\\), \\(3\\), \\(4\\), takes ca 4 seconds computer. First, use stat_ewcdf plot emprirical cumulative distribution functions importance weighted posterior samples \\(N_1\\), \\(J=1,\\dots,4\\). can see random variability increases larger \\(J\\), indicating lower efficiency importance sampler. Also included CDF prior distribution \\(N_1\\). Except lower end distribution, ’s immediately clear effect observations distribution.  Next corresponding plot \\(\\phi\\). contrast \\(N_1\\), ’s clear observation pairs get, concentrated distribution \\(\\phi\\) gets. new observation pair also influences particular lower end distribution, lower quantiles clearly increasing \\(J=4\\) even though upper quantiles remain relatively stable \\(J\\geq 2\\).  Using wquantile extract lower upper \\(5\\%\\) quantiles obtain approximate \\(90\\%\\) posterior credible intervals \\(N_1\\) \\(\\phi\\) shown following table. can see credible intervals \\(N_1\\) strongly linked information posterior distribution \\(\\phi\\). Interestingly, due lowering lower endpoint \\(\\phi\\) \\(J=2\\) \\(3\\), width \\(N_1\\) interval actually increases add information second third. stark contrast common intuition expect uncertainty decrease collect data. behaviour partly due random variability inherent small sample, also due strong non-linearity estimation problem, products \\(\\phi N_j\\) much easier estimate individual \\(\\phi\\) \\(N_j\\) parameters. \\(J=4\\), interval width \\(N_1\\) finally decreases, well width \\(\\phi\\) interval. Looking values observations, can see \\(J=4\\) case much larger values cases, leads providing relatively information \\(\\phi\\) parameter. practical point view, ’s clear estimation problem easy solution. Even multiple excavations, assuming model appropriate value \\(\\phi\\) \\(\\phi\\) every excavation, gain modest amount additional information. positive side, model good job approximating reality, borrowing strength across multiple excavations improve estimates uncertainty information excavation. improvements, one likely need closely study excavation precedures, gain better understanding data generating process, might lead improved statistical methods archaeologically relevant outcomes.","code":"## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in ## dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` ##   always returns an ungrouped data frame and adjust accordingly. ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in ## dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` ##   always returns an ungrouped data frame and adjust accordingly. ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in ## dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` ##   always returns an ungrouped data frame and adjust accordingly. ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in ## dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` ##   always returns an ungrouped data frame and adjust accordingly."},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"effective-sample-size","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Effective sample size","title":"StatComp Project Example Solution: Numerical Statistics","text":"Using method computing effective importance sample size later course can see even efficient method, sampling inefficient \\(J=4\\), indicating improved choice importance distribution \\(N_j\\) improve precision accuracy results: , EffSampleSize approximate effective sample size \\((\\sum_k w_k)^2/\\sum w_k^2\\), RelEff relative efficiency, defined ratio EffSampleSize \\(K=100000\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"profile-likelihood-method-for-frequentist-confidence-interval-for-textrmbinomnp","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Profile likelihood method for frequentist confidence interval for \\(\\textrm{Binom}(n,p)\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"(actually range(y) depends n may mean l0 justified example)","code":"## [1] 274"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"function-definitions","dir":"Articles","previous_headings":"Code appendix","what":"Function definitions","title":"StatComp Project Example Solution: Numerical Statistics","text":"","code":"# Finn Lindgren (s0000000, finnlindgren)  # Part 1 ####  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using one of the three methods implemented in pois_CI # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals #   type: 1, 2, 3, or 4, indicating which CI method to use # # Output: #   A data frame with `m` rows and columns `Lower` and `Upper`, with one #   confidence interval per row  multi_pois_CI <- function(m, n, lambda, alpha, type) {   Lower = numeric(m)   Upper = numeric(m)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     CI <- pois_CI(y = y, alpha = alpha, type = type)     Lower[m_idx] <- CI[1]     Upper[m_idx] <- CI[2]   }   result <- data.frame(Lower = Lower, Upper = Upper) }  tidy_multi_pois_CI_alt <- function(m, n, lambda, alpha) {   Lower <- matrix(0, m, 4)   Upper <- matrix(0, m, 4)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     for (type in 1:4) {       CI <- pois_CI(y = y, alpha = alpha, type = type)       Lower[m_idx, type] <- CI[1]       Upper[m_idx, type] <- CI[2]     }   }   data.frame(     Index = rep(1:m, times = 4),     Type = rep(1:4, each = m),     Lower = as.vector(Lower),     Upper = as.vector(Upper)   ) }  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using all three methods implemented in pois_CI. # This implementation uses independent simulations for the three method, by # calling multi_pois_CI for each interval type. An alternative implementation # would be to use the same samples for all three methods. # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals # # Output: #   A data frame with `3m` rows and columns `Lower`, `Upper`, and `Type`, #   with one confidence interval per row, calculated with the method indicated #   in the `Type` column  tidy_multi_pois_CI <- function(m, n, lambda, alpha) {   result <-     rbind(       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 1         ),         Type = 1       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 2         ),         Type = 2       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 3         ),         Type = 3       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 4         ),         Type = 4       )     )   result }   # Part 2 ####  # Joint log-probability # # Usage: #   log_prob_NY(N, Y, xi, a, b) # # Calculates the joint probability for N and Y, when #   N_j ~ Geom(xi) #   phi ~ Beta(a, b) #   Y_{ji} ~ Binom(N_j, phi) #   For j = 1,...,J # # Inputs: #   N: vector of length J #   Y: data.frame or matrix of dimension Jx2 #   xi: The probability parameter for the N_j priors #   a, b: the parameters for the phi-prior # # Output: #   The log of the joint probability for N and Y #   For impossible N,Y combinations, returns -Inf  log_prob_NY <- function(N, Y, xi, a, b) {   # Convert Y to a matrix to allow more vectorised calculations:   Y <- as.matrix(Y)   if (any(N < Y) || any(Y < 0)) {     return(-Inf)   }   atilde <- a + sum(Y)   btilde <- b + 2 * sum(N) - sum(Y) # Note: b + sum(N - Y) would be more general   lbeta(atilde, btilde) - lbeta(a, b) +     sum(dgeom(N, xi, log = TRUE)) +     sum(lchoose(N, Y)) }  # Importance sampling for N,phi conditionally on Y # # Usage: #   arch_importance(K,Y,xi,a,b) # # Inputs: #   Y,xi,a,b have the same meaning and syntax as for log_prob_NY(N,Y,xi,a,b) #   The argument K defines the number of samples to generate. # # Output: #  A data frame with K rows and J+1 columns, named N1, N2, etc, #  for each j=1,…,J, containing samples from an importance sampling #  distribution for Nj, #  a column Phi with sampled phi-values, #  and a final column called Log_Weights, containing re-normalised #  log-importance-weights, constructed by shifting log(w[k]) by subtracting the #  largest log(w[k]) value, as in lab 4.  arch_importance <- function(K, Y, xi, a, b) {   Y <- as.matrix(Y)   J <- nrow(Y)   N_sample <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J)   xi_sample <- 1 / (1 + 4 * N_sample)    N <- matrix(rgeom(K * J, prob = xi_sample), K, J) + N_sample   colnames(N) <- paste0(\"N\", seq_len(J))    Log_Weights <- vapply(     seq_len(K),     function(k) log_prob_NY(N[k, , drop = TRUE], Y, xi, a, b),     0.0) -     rowSums(dgeom(N - N_sample, prob = xi_sample, log = TRUE))    phi <- rbeta(K, a + sum(Y), b + 2 * rowSums(N) - sum(Y))    cbind(as.data.frame(N),         Phi = phi,         Log_Weights = Log_Weights - max(Log_Weights)) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-intervals","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Confidence intervals","title":"StatComp Project Example Solution: Numerical Statistics","text":"Poisson parameter interval simulations: Summarise coverage probabilities interval widths: Plot CDFs CI width distributions:","code":"n <- 2 lambda_true <- 3 result <- tidy_multi_pois_CI_alt(100000, n, lambda_true, alpha = 0.1) result %>%   group_by(Type) %>%   summarise(     Coverage = mean((Lower <= lambda_true) & (lambda_true <= Upper)),     MedWidth = median(Upper - Lower),     OutsideLower = mean(lambda_true < Lower),     OutsideUpper = mean(lambda_true > Upper),     MedLower = median(Lower),     MedUpper = median(Upper)   ) %>%   knitr::kable()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Archaeology","title":"StatComp Project Example Solution: Numerical Statistics","text":"Compute credible intervals \\(N_j\\) different amounts data, \\(J\\): completeness, compute credible intervals \\(N_j\\), \\(1\\leq J\\), \\(J=1,\\dots,4\\), analysis focuses \\(N_1\\) \\(\\phi\\): Restructure interval data.frame focus \\(N_1\\) \\(\\phi\\) :","code":"xi <- 1 / 1001 a <- 0.5 b <- 0.5  K <- 1000000 NW1 <- arch_importance(K, arch_data(1), xi, a, b) NW2 <- arch_importance(K, arch_data(2), xi, a, b) NW3 <- arch_importance(K, arch_data(3), xi, a, b) NW4 <- arch_importance(K, arch_data(4), xi, a, b) CI <- rbind(   NW1 %>%     summarise(       J = 1,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = NA,       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW2 %>%     summarise(       J = 2,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW3 %>%     summarise(       J = 3,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW4 %>%     summarise(       J = 4,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = wquantile(N4,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ) ) CI_ <- CI %>%   pivot_longer(     cols = c(N1, N2, N3, N4, Phi),     names_to = \"Variable\",     values_to = \"value\"   ) %>%   pivot_wider(     names_from = End,     values_from = value   ) %>%   filter(!is.na(Lower)) %>%   mutate(Width = Upper - Lower) knitr::kable(CI_ %>% filter(Variable %in% c(\"N1\", \"Phi\")) %>%   arrange(Variable, J))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"deriving-the-hessian","dir":"Articles","previous_headings":"","what":"Deriving the Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Define individual terms \\(f_k=y_k\\log(1+e^{-\\eta_k}) + (N-y_k)\\log(1+e^{\\eta_k})\\), \\(f=\\sum_{k=1}^n f_k\\). know \\(\\partial\\eta_k/\\partial\\theta_1\\equiv 1\\) \\(\\partial\\eta_k/\\partial\\theta_2=k\\). derivatives respect \\(\\theta_1\\) \\(\\theta_2\\) can obtain chain rule: \\[\\begin{align*} \\frac{df_k}{d\\theta_i} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\frac{\\partial f_k}{\\partial\\eta_k} \\\\   &= \\frac{\\partial \\eta_k}{\\partial \\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k}}{1+e^{-\\eta_k}} +   (N-y_k) \\frac{e^{\\eta_k}}{1+e^{\\eta_k}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   (N-y_k) \\frac{e^{\\eta_k/2}}{e^{-\\eta_k/2}+e^{\\eta_k/2}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   - y_k \\frac{e^{-\\eta_k/2}+e^{\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   N \\frac{1}{e^{-\\eta_k}+1}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\ \\end{align*}\\] Since derivatives \\(\\eta_k\\) respect \\(\\theta_1\\) \\(\\theta_2\\) depend values \\(\\theta_1\\) \\(\\theta_2\\), second order derivatives \\[\\begin{align*} \\frac{d^2f_k}{d\\theta_i d\\theta_j} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{\\partial}{\\partial\\eta_k}\\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N e^{-\\eta_k}}{(e^{-\\eta_k}+1)^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{(e^{-\\eta_k/2}+e^{\\eta_k/2})^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{4\\cosh(\\eta_k/2)^2}   \\\\ \\end{align*}\\] Plugging \\(\\theta\\)-derivatives gives Hessain contribution term \\(f_k\\) \\[   \\frac{N}{4\\cosh(\\eta_k/2)^2} \\begin{bmatrix}1 & k \\\\ k & k^2\\end{bmatrix} . \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"positive-definite-hessian","dir":"Articles","previous_headings":"","what":"Positive definite Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"show total Hessian \\(f\\) positive definite \\(n \\geq 2\\), define vectors \\(\\boldsymbol{u}_k=\\begin{bmatrix}1 \\\\ k\\end{bmatrix}\\) \\(d_k=\\frac{N}{4\\cosh(\\eta_k/2)^2}\\). Define 2-\\(n\\) matrix \\(\\boldsymbol{U}=\\begin{bmatrix}\\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\cdots & \\boldsymbol{u}_n\\end{bmatrix}\\) diagonal matrix \\(\\boldsymbol{D}\\) \\(D_{ii}=d_i\\). product \\(\\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{U}^\\top\\) another way writing Hessian \\(f\\). Since vectors \\(\\boldsymbol{u}_k\\) non-parallel, \\(d_i\\) strictly positive combinations \\(\\theta_1\\) \\(\\theta_2\\), matrix full rank (rank 2) \\(n \\geq 2\\), positive definite.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"alternative-reasoning","dir":"Articles","previous_headings":"","what":"Alternative reasoning","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Hessian \\(f_k\\) positive semi-definite, since can written \\(d_k\\boldsymbol{u}_k\\boldsymbol{u}_k^\\top\\) \\(d_k > 0\\) vector \\(\\boldsymbol{u}_k\\). sum positive definite matrix positive semi-definite matrix positive definite, ’s sufficient prove sum first two terms positive definite. positive scaling constant \\(w\\), determinant \\[ \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}+w\\begin{bmatrix}1 & 2\\\\2 & 4\\end{bmatrix} \\] \\((1+w)(1+4w)-(1+2w)^2=1+5w+4w^2-1-4w-4w^2=w > 0\\). means (positively) weighted sum two matrices positive definite (since positive semi-definite, positive determinant rules ot sum positive semi-definite). proves total Hessian positive definite \\(n\\geq 2\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"remark","dir":"Articles","previous_headings":"","what":"Remark","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Note first proof positive definiteness, didn’t actually need know specific values \\(\\boldsymbol{u}_k\\) vectors, proportional gradients fo \\(\\eta_k\\) respect \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)\\); sufficient non-parallel, ensuring \\(\\boldsymbol{U}\\) full rank. means linear model \\(\\eta_k\\) set parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_p)\\) leads positive definite Hessian model, collection gradient vectors \\((\\eta_1,\\dots,\\eta_n)\\) respect \\(\\boldsymbol{\\theta}\\) collective rank least \\(p\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"case 3, negated log-likelihood \\[ \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant} \\] 1st order derivative \\[ \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i \\] shows \\(\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y})\\), 2nd order derivative \\[ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta \\] equal \\(n\\lambda\\) \\(y_i\\) values, inverse expected Hessian \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"can test interval construction methods following code. methods always produce valid intervals. \\(\\overline{y}=0\\), first method produces single point “interval”. third method fails \\(\\overline{y}=0\\), due log zero. can happen \\(n\\) \\(\\lambda\\) close zero.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"probability theory, know \\(p(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}\\), \\[ p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right) \\]","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"First, sample Gaussian approximation posterior distribution. avoid numerical problems, calculate log unnormalised weights, shift results get numerically sensible scale exponentiate. credible interval \\(\\theta\\) can now extracted quantiles weighted sample, \\(\\lambda\\)-interval obtained transformation \\(\\lambda=\\exp(\\theta)\\): code steps following:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043 a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval lambda_interval <- exp(theta_interval) lambda_interval"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"figure shows empirical distributions unweighted weighted samples \\(\\{x_k\\}\\) (weights \\(\\{w_k\\}\\)), together theoretical posterior empirical distribution function. weighted, importance sampling, version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution \\(\\log(\\lambda)\\) excellent approximation even add importance sampling step.  lower figure, importance weights normalised average, values 1 mean corresponding sample values occur often true, target, distribution raw samples, values 1 mean sample values occur less frequently target distribution. scaled weights shown logarithmic scale.","code":"# Plotting updated to include a plot of the importance weights p1 <-   ggplot(data.frame(lambda = exp(x), weights = weights)) +   ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\")) +   scale_x_log10(limits = c(3, 20)) +   labs(colour = \"Type\")  p2 <- ggplot(data.frame(lambda = exp(x), weights = weights),              mapping = aes(lambda, weights / mean(weights))) +   ylab(\"Importance weights\") +   geom_point() +   geom_line() +   geom_hline(yintercept = 1) +   scale_y_log10() +   scale_x_log10(limits = c(3, 20))  # The patchwork library provides a versatile framework # for combining multiple ggplot figures into one. library(patchwork) (p1 / p2) ## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor?"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation.","code":"newdata <- data.frame(z = -10:20)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models","text":"can now use something like following code plot results: Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models (solutions)","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models (solutions)","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models (solutions)","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models (solutions)","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models (solutions)","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models (solutions)","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation. Solution:","code":"newdata <- data.frame(z = -10:20) data_pred <- cbind(newdata,                    data.frame(fit = predict(mod, newdata))) ggplot(data) +   geom_point(aes(z, y)) +   geom_line(aes(z, fit), data = data_pred) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models (solutions)","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Solution: Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"pred <- cbind(   newdata,   predict(mod, newdata, interval = \"prediction\") ) plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: Solution: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions <- function(x, newdata, xname = \"x\") {   pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   ggplot(pred) +     geom_line(aes_string(xname, \"fit\")) +     geom_ribbon(aes_string(x = xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25) } plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models (solutions)","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot. Solution:","code":"# Extended version of the solution: plot_prediction <- function(x, newdata, xname = \"x\",                             xlab = NULL, ylab = NULL, ...) {   if (is.null(xlab)) {     xlab <- xname   }   if (is.null(ylab)) {     ylab <- \"Response\"   }    pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   pl <- ggplot() +     geom_line(data = pred,               aes_string(xname, \"fit\")) +     geom_ribbon(data = pred,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Also add the confidence intervals for the predictor curve   conf <- cbind(newdata,                 predict(x, newdata, interval = \"confidence\"))   pl <- pl +     geom_ribbon(data = conf,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Add lables:   pl + xlab(xlab) + ylab(ylab) }"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models (solutions)","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data) plot_prediction(mod2, newdata, xname = \"z\", ylab = \"y\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") +   ggtitle(\"Confidence and prediction intervals\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods. Solution:","code":"formulas <- c(y ~ 1,               y ~ z,               y ~ z + I(z^2),               y ~ z + I(z^2) + I(z^3)) mods <- lapply(formulas, function(x) lm(x, data))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models (solutions)","text":"can now use something like following code plot results:  Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization","text":"example, ’ll look model connection values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Check ?optim help text information result object contains. optimisation converge? Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals.","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1)"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization (solutions)","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization (solutions)","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization (solutions)","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization (solutions)","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization (solutions)","text":"example, ’ll look model connection values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. Solution: aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Solution: Check ?optim help text information result object contains. optimisation converge? Solution: Answer: optimisation converged opt$convergence 0. least optim() thinks coverged, since triggered ’s convergence tolerances. Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) neg_log_lik <- function(theta, y, X) {   -sum(dnorm(x = y,              mean = X %*% theta[1:2],              sd = exp(X %*% theta[3:4]),              log = TRUE)) } opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik,              y = y, X = X,              method = \"BFGS\") data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization (solutions)","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals. Solution:","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1) opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik, y = y, X = X,              method = \"BFGS\",              hessian = TRUE) covar <- solve(opt$hessian) covar ##               [,1]         [,2]          [,3]         [,4] ## [1,]  0.0017940969 -0.005724814 -0.0005605767  0.001121155 ## [2,] -0.0057248136  0.036111726  0.0035360846 -0.007072178 ## [3,] -0.0005605767  0.003536085  0.0190674071 -0.028134823 ## [4,]  0.0011211550 -0.007072178 -0.0281348225  0.056269649"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks. Solution: accompanying Tutorial03Solutions tutorial document contains solutions explicitly, make easier review material workshops. can also run document Tutorials pane RStudio.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms ","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~} + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Solution: \\[ \\begin{aligned} \\mathsf{Var}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{Var}(Y|X)] + \\mathsf{Var}[\\mathsf{E}(Y|X)] \\\\&= \\mathsf{E}[\\lambda(\\mu,X)] + \\mathsf{Var}[\\lambda(\\mu,X)] \\\\&= \\mathsf{E}[\\exp(\\mu+X)] +\\mathsf{Var}[\\exp(\\mu+X)] \\\\&= \\exp(\\mu + \\sigma^2/2) + (\\exp(\\sigma^2)-1) \\exp(2\\mu + \\sigma^2) \\\\&= \\exp(\\mu + \\sigma^2/2) \\left[ 1 + (\\exp(\\sigma^2)-1) \\exp(\\mu + \\sigma^2/2) \\right] \\end{aligned} \\] Since second factor always \\(\\geq 1\\), shows variance never smaller expectation. Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Solution:  plot, overdispersed Poisson probability function (just computed) compared plain Poisson probability function expectation value. Lines values included clarity. Note: case, second approach faster, situations first approach required, since doesn’t require storing random numbers time, thus allowing much larger \\(K=n_{\\text{mc}}\\) value used. Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Solution: Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms Solution:","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) # Vectorised over m mu <- log(2) - 1/2 K <- 10000 m <- 0:15 P_Y <- numeric(length(m)) for (loop in seq_len(K)) {   x <- rnorm(1, sd = 1)   P_Y <- P_Y + dpois(m, lambda = exp(mu + x)) } P_Y <- P_Y / K  # Plot the results suppressPackageStartupMessages(library(ggplot2)) ggplot(data.frame(m = m,                   P_Y = P_Y,                   P_Poisson = dpois(m, lambda = 2))) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\")) doverpois <- function(m, mu, sigma, K) {   P_Y <- numeric(length(m))   for (loop in seq_len(K)) {     x <- rnorm(1, sd = sigma)     P_Y <- P_Y + dpois(m, lambda = exp(mu + x))   }   P_Y <- P_Y / K    data.frame(m = m,              P_Y = P_Y,              P_Poisson = dpois(m, lambda = exp(mu + sigma^2/2))) } ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000)) suppressPackageStartupMessages(library(ggplot2)) ggplot(doverpois(m = 0:30, mu = log(8)-0.125, sigma = 0.5, K = 10000)) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~} + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Solution: Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":"estimate <- function(y, xi, a, b, K) {   samples <- data.frame(     N = rgeom(K, prob = xi),     phi = rbeta(K, shape1 = a, shape2 = b)   )   loglike <- arch_loglike(param = samples, y = y)   p_y <- mean(exp(loglike))   c(p_y = p_y,     E_N = mean(samples$N * exp(loglike) / p_y),     E_phi = mean(samples$phi * exp(loglike) / p_y)) } estimate(y = c(237, 256), xi = 1/1001, a = 0.5, b = 0.5, K = 10000) ##          p_y          E_N        E_phi  ## 6.360064e-06 9.884083e+02 3.694532e-01"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk. Solution: accompanying Tutorial04Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. separate T4sol.Rmd document https://github.com/finnlindgren/StatCompLab/blob/main/vignettes/articles/T4sol.Rmd source document standalone solution shown T4sol StatCompLab website.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration","text":"Consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] week 4 lecture, two parameterisations considered. now add third option: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) week 4 lecture, know inverse expected Fisher information \\(\\lambda/n\\) case 1 \\(1/(4n)\\) case 2. case 3, show inverse expected Fisher information \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration","text":"Use approximation method large \\(n\\) lecture construct approximate confidence intervals \\(\\lambda\\) using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values \\(\\overline{y}\\). Experiment different values n lambda simulation y. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; \\(\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha\\) \\(\\theta\\) (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration","text":"Assume true value \\(\\lambda=10\\), simulate sample \\(\\boldsymbol{y}\\) size \\(n=5\\). Now consider Bayesian version Poisson model, prior model \\[ \\lambda \\sim \\mathsf{Exp}() \\] probability density function \\(p(\\lambda) = \\exp(-\\lambda)\\). One can show exact posterior distribution \\(\\lambda\\) given \\(\\boldsymbol{y}\\) \\(\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n)\\) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let \\(\\theta=\\log(\\lambda)\\), \\(\\lambda=\\exp(\\theta)\\). Show prior probability density \\(\\theta\\) \\(p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration","text":"posterior density function \\(\\theta\\) \\[ p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})} \\] log-density \\[ \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) , \\] taking derivatives find mode \\(\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right)\\), negated Hessian \\(1+n\\overline{y}\\) mode. information can construct Gaussian approximation posterior distribution, \\(\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}})\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration","text":"Simulate sample \\(\\boldsymbol{x}=\\{x_1,\\dots,x_m\\}\\) Gaussian approximation posterior distribution, large \\(m > 10000\\), hyperparameter \\(=1/5\\). need calculate unnormalised importance weights \\(w_k\\), \\(k=1,\\dots,m\\), \\[ w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} . \\] Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, \\(\\log(w_k)\\), new, equivalent unnormalised weights \\(\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]\\). Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval \\(\\theta\\) using \\(\\boldsymbol{x}\\) sample associate weights, transform credible interval \\(\\lambda\\)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function \\(\\lambda\\) (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample \\(\\lambda_k=\\exp(x_k)\\), (unnormalised) weights \\(w_k\\). Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Finn Lindgren. Author, maintainer.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lindgren F (2023). StatCompLab: Workshop Material UoE Statistical Computing. https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab.","code":"@Manual{,   title = {StatCompLab: Workshop Material For UoE Statistical Computing},   author = {Finn Lindgren},   year = {2023},   note = {https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab}, }"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"statcomplab","dir":"","previous_headings":"","what":"Workshop Material For UoE Statistical Computing","title":"Workshop Material For UoE Statistical Computing","text":"package collects workshop materials Statistical Computing (MATH10093) University Edinburgh 2021/22. tutorial documents browsable online https://finnlindgren.github.io/StatCompLab/ Contact Finn Lindgren, finn.lindgren@ed.ac.uk information. package version number system YearOfStudy.StudyWeek.MinorUpdate","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Workshop Material For UoE Statistical Computing","text":"can install (later upgrade) package GitHub : want view vignette versions tutorials within RStudio, need add build_vignettes argument:","code":"# If devtools isn't installed, first run install.packages(\"remotes\") remotes::install_github(\"finnlindgren/StatCompLab\") remotes::install_github(\"finnlindgren/StatCompLab\", build_vignettes = TRUE)"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Workshop Material For UoE Statistical Computing","text":"convenient way access tutorial documents run Tutorial tab upper right part RStudio. allows see tutorial text code hints code files, running code Console viewing plots. alternative methods also allows viewing documents, less convenient; vignettes show space plots, run_tutorial blocks Console window used run code.","code":"library(StatCompLab) # To install the vignette versions, add build_vignettes=TRUE to the install_github() call above. vignette(package = \"StatCompLab\") # List available vignettes vignette(\"Tutorial01\", \"StatCompLab\") # View a specific vignette # The following method blocks the Console from running other code; better to use the Tutorials pane instead library(learnr) available_tutorials(\"StatCompLab\") # List available tutorials run_tutorial(\"Tutorial01\", \"StatCompLab\") # Run a specific tutorial"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/StatEwcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"StatEwcdf ggproto object — StatEwcdf","title":"StatEwcdf ggproto object — StatEwcdf","text":"StatEwcdf ggproto object","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple binomial model likelihood — arch_loglike","title":"Multiple binomial model likelihood — arch_loglike","text":"Compute log-likelihood multiple Binom(N,phi) observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(param, y)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple binomial model likelihood — arch_loglike","text":"param Either vector two elements, N phi, data.frame two columns, named N phi y data vector","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple binomial model likelihood — arch_loglike","text":"log-likelihood N,phi combination. implementation internally uses log-gamma function provide differentiable function N, allows optim() treat N continuous variable. N < max(y), log-likelihood defined -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(   param = data.frame(N = c(5, 10, 20, 40),                      phi = c(0.1, 0.2, 0.3, 0.4)),   y = c(2, 4)) #> [1] -10.324930  -3.626867  -5.618058 -25.216655"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate filament models — filament1_estimate","title":"Estimate filament models — filament1_estimate","text":"Estimate filament models different variance structure","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate filament models — filament1_estimate","text":"","code":"filament1_estimate(data, model)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate filament models — filament1_estimate","text":"data data.frame variables filament1 data set. Must columns CAD_Weight Actual_Weight model Either \"\" log-linear variance model, \"B\" proportional scaling error model","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate filament models — filament1_estimate","text":"estimation object suitable use filament1_predict()","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate filament models — filament1_estimate","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict values for a filament model — filament1_predict","title":"Predict values for a filament model — filament1_predict","text":"Uses estimated filament model predict new observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict values for a filament model — filament1_predict","text":"","code":"filament1_predict(object, newdata, alpha = 0.05, df = Inf, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict values for a filament model — filament1_predict","text":"object estimation object, like output filament1_estimate() newdata data use prediction. Must column CAD_Weight alpha target coverage error prediction intervals, Default: 0.05 df degrees freedom t-quantiles prediction interval construction, Default: Inf ... Additional arguments, currently ignored","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict values for a filament model — filament1_predict","text":"data.frame variables mean, sd, lwr, upr, summarising prediction distribution row newdata.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict values for a filament model — filament1_predict","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive numerical optimisation explorer — optimisation","title":"Interactive numerical optimisation explorer — optimisation","text":"Shiny app exploring several numerical optimisation methods work","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive numerical optimisation explorer — optimisation","text":"","code":"optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate proper scores — proper_score","title":"Calculate proper scores — proper_score","text":"Calculates proper scores probabilistic predictions","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate proper scores — proper_score","text":"","code":"proper_score(type, obs, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate proper scores — proper_score","text":"type string indicating type score calculate. One \"se\", \"ae\", \"ds\", \"interval\". See Details . obs vector observations ... Additional named arguments needed type score, see Details .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate proper scores — proper_score","text":"vector calculated scores, one observation","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate proper scores — proper_score","text":"score type takes additional arguments define prediction model information used score calculations. Unless specified otherwise, extra arguments either scalars (applying predictions) vectors length obs vector. se Squared error score. Requires mean, defining prediction mean ae Absolute error score. Requires median, defining prediction median ds Dawid-Sebastiani score. Requires mean sd, defining mean standard deviation predictions interval Squared error score. Requires lwr upr, defining lower upper prediction interval endpoints, alpha, defining prediction error probability targeted prediction intervals scores \\(S_{type}(F_i,y_i)\\) defined lecture notes, obs[] equal \\(y_i\\) prediction distribution \\(F_i\\) defined additional named arguments.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate proper scores — proper_score","text":"","code":"# Five realisations of N(3, 3^2) obs <- rnorm(5, mean = 3, sd = 3)  # One prediction for each observation. Only one of the predictions describes # the true model F_mean <- 1:5 F_median <- 1:5 F_sd <- 1:5 F_lwr <- F_mean - F_sd * qnorm(0.9) F_upr <- F_mean - F_sd * qnorm(0.1)  # Compute the scores data.frame(   se = proper_score(\"se\", obs, mean = F_mean),   ae = proper_score(\"ae\", obs, median = F_median),   ds = proper_score(\"ds\", obs, mean = F_mean, sd = F_sd),   interval = proper_score(\"interval\", obs,                           lwr = F_lwr, upr = F_upr, alpha = 0.2) ) #>          se       ae        ds  interval #> 1 21.917353 4.681597 21.917353 36.563560 #> 2 25.490106 5.048773  7.758821 29.982902 #> 3  5.903027 2.429615  2.853116  7.689309 #> 4  3.432645 1.852740  2.987129 10.252413 #> 5 88.963205 9.432031  6.777404 43.058245"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute empirical weighted cumulative distribution — stat_ewcdf","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"Version ggplot2::stat_ecdf adds weights property observation, produce empirical weighted cumulative distribution function. empirical cumulative distribution function (ECDF) provides alternative visualisation distribution. Compared visualisations rely density (like geom_histogram()), ECDF require tuning parameters handles continuous discrete variables. downside requires training accurately interpret, underlying visual tasks somewhat challenging.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"stat_ewcdf(   mapping = NULL,   data = NULL,   geom = \"step\",   position = \"identity\",   ...,   n = NULL,   pad = TRUE,   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"n NULL, interpolate. NULL, number points interpolate . pad TRUE, pad ecdf additional points (-Inf, 0) (Inf, 1) na.rm FALSE (default), removes missing values warning.  TRUE silently removes missing values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"computed-variables","dir":"Reference","previous_headings":"","what":"Computed variables","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"x x data y cumulative density corresponding x","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"library(ggplot2)  n <- 100 df <- data.frame(   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),   g = gl(2, n),   w = c(rep(1/n, n), sort(runif(n))^sqrt(n)) ) ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\") #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Don't go to positive/negative infinity ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Multiple ECDFs ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf() #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?  ggplot(df, aes(x, colour = g, weights = w)) +   stat_ewcdf() +   facet_wrap(vars(g), ncol = 1) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted sample quantiles — wquantile","title":"Weighted sample quantiles — wquantile","text":"Calculates empirical sample quantiles optional weights, given probabilities. Like quantile(), smallest observation corresponds probability 0 largest probability 1. Interpolation discrete values done type=7, quantile(). Use type=1 generate quantile values raw input samples.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted sample quantiles — wquantile","text":"","code":"wquantile(   x,   probs = seq(0, 1, 0.25),   na.rm = FALSE,   type = 7,   weights = NULL,   ... )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted sample quantiles — wquantile","text":"x numeric vector whose sample quantiles wanted. NA NaN values allowed numeric vectors unless na.rm TRUE. probs numeric vector probabilities values \\([0,1]\\). na.rm logical; true, NA NaN's removed x quantiles computed. type numeric, 1 interpolation, 7, interpolated quantiles. Default 7. weights numeric vector non-negative weights, length x, NULL. weights normalised sum 1. NULL, wquantile(x) behaves quantile(x), equal weight sample value. ... Additional arguments, currently ignored","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted sample quantiles — wquantile","text":"","code":"# Some random numbers x <- rnorm(100)  # Plain quantiles: quantile(x) #>          0%         25%         50%         75%        100%  #> -2.03833647 -0.68727096 -0.07239232  0.57043162  2.91914416   # Larger values given larger weight, on average shifting the quantiles upward: wquantile(x, weights = sort(runif(length(x)))) #> [1] -2.0383365 -0.6693281 -0.1565127  0.6270383  2.9191442"}]
