[{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"Lab 4, consider Poisson model observations 𝐲={y1,…,yn}\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}: yi∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ),independent =1,…,n. \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned}  joint probability mass function p(𝐲|λ)=exp(−nλ)∏=1nλyiyi! p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!}  Lab 4, one considered parameterisation alternatives θ=λ\\theta = \\lambda, θ̂ML=1n∑=1nyi=y¯\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y} Create function called estimate_coverage (code.R; also document function) perform interval coverage estimation taking arguments CI (function object confidence interval construction taking arguments y alpha returning 2-element vector [see Lab 4]), N (number simulation replications use coverage estimate), alpha (1-alpha intended coverage probability), n (sample size) lambda (true lambda values Poisson model). Use function estimate_coverage estimate coverage construction confidence intervals samples 𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ)\\mathsf{Poisson}(\\lambda) discuss following: Since model involves discrete Poisson distribution, one might ask sensitive results precise values λ\\lambda nn. investigate , run coverage estimation different combinations model parameters λ\\lambda nn (fix N=10000N = 10000 α=0.1\\alpha=0.1) Present results estimated coverage two plots, (1) function λ\\lambda fixed n=2n=2, (2) function nn, fixed λ=3\\lambda=3. Discuss plots regards whether coverage intervals achieve desired 90% confidence level, identify cases provide suggestion .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"d-printer-materials-prediction","dir":"Articles","previous_headings":"","what":"3D printer materials prediction","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"aim estimate parameters Bayesian statistical model material use 3D printer. printer uses rolls filament get heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design) also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (grams) CAD software calculated Actual_Weight: actual weight object (grams) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variations, example, due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator made simple physics analysis, settled model connection CAD_Weight Actual_Weight follows linear model, variance increases square CAD_Weight. denote CAD weight observations ii x_i, corresponding actual weight yiy_i, model can defined yi∼𝖭𝗈𝗋𝗆𝖺𝗅[β1+β2xi,β3+β4xi2)]. y_i \\sim \\mathsf{Normal}[\\beta_1 + \\beta_2 x_i, \\beta_3 + \\beta_4 x_i^2)] .  ensure positivity variance, parameterisation 𝛉=[θ1,θ2,θ3,θ4]=[β1,β2,log(β3),log(β4)]\\boldsymbol{\\theta}=[\\theta_1,\\theta_2,\\theta_3,\\theta_4]=[\\beta_1,\\beta_2,\\log(\\beta_3),\\log(\\beta_4)] introduced, printer operator assigns independent prior distributions follows: θ1∼𝖭𝗈𝗋𝗆𝖺𝗅(0,γ1),θ2∼𝖭𝗈𝗋𝗆𝖺𝗅(1,γ2),θ3∼𝖫𝗈𝗀𝖤𝗑𝗉(γ3),θ4∼𝖫𝗈𝗀𝖤𝗑𝗉(γ4), \\begin{aligned} \\theta_1 &\\sim \\mathsf{Normal}(0,\\gamma_1), \\\\ \\theta_2 &\\sim \\mathsf{Normal}(1,\\gamma_2), \\\\ \\theta_3 &\\sim \\mathsf{LogExp}(\\gamma_3), \\\\ \\theta_4 &\\sim \\mathsf{LogExp}(\\gamma_4), \\end{aligned}  𝖫𝗈𝗀𝖤𝗑𝗉()\\mathsf{LogExp}() denotes logarithm exponentially distributed random variable rate parameter aa, seen Tutorial 4. 𝛄=(γ1,γ2,γ3,γ4)\\boldsymbol{\\gamma}=(\\gamma_1,\\gamma_2,\\gamma_3,\\gamma_4) values positive parameters. printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, leads model approximation . basic physics assumption error CAD software calculation weight proportional weight . Start loading data plotting .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"prior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Prior density","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"help dnorm dlogexp function (see code.R file documentation), define document (code.R) function log_prior_density arguments theta params, theta 𝛉\\boldsymbol{\\theta} parameter vector, params vector 𝛄\\boldsymbol{\\gamma} parameters. function evaluate logarithm joint prior density p(𝛉)p(\\boldsymbol{\\theta}) four θi\\theta_i parameters.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"observation-likelihood","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Observation likelihood","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"help dnorm, define document function log_like, taking arguments theta, x, y, evaluates observation log-likelihood p(𝐲|𝛉)p(\\boldsymbol{y}|\\boldsymbol{\\theta}) model defined .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior density","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"Define document function log_posterior_density arguments theta, x, y, params, evaluates logarithm posterior density p(𝛉|𝐲)p(\\boldsymbol{\\theta}|\\boldsymbol{y}), apart unevaluated normalisation constant.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-mode","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior mode","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"Define function posterior_mode arguments theta_start, x, y, params, uses optim together log_posterior_density filament data find mode 𝛍\\boldsymbol{\\mu} log-posterior-density evaluates Hessian mode well inverse negated Hessian, 𝐒\\boldsymbol{S}. function return list elements mode (posterior mode location), hessian (Hessian log-density mode), S (inverse negated Hessian mode). See documentation optim maximisation instead minimisation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Gaussian approximation","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"Let γi=1\\gamma_i=1, =1,2,3,4i=1,2,3,4, use posterior_mode evaluate inverse negated Hessian mode, order obtain multivariate Normal approximation 𝖭𝗈𝗋𝗆𝖺𝗅(𝛍,𝐒)\\mathsf{Normal}(\\boldsymbol{\\mu},\\boldsymbol{S}) posterior distribution 𝛉\\boldsymbol{\\theta}. Use start values 𝛉=𝟎\\boldsymbol{\\theta} = \\boldsymbol{0}.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling function","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"aim construct 90% Bayesian credible interval βj\\beta_j using importance sampling, similarly method used lab 4. , one dimensional Gaussian approximation posterior parameter used. , instead use multivariate Normal approximation importance sampling distribution. functions rmvnorm dmvnorm mvtnorm package can used sample evaluate densities. Define document function do_importance taking arguments N (number samples generate), mu (mean vector importance distribution), S (covariance matrix), additional parameters needed function code. function output data.frame five columns, beta1, beta2, beta3, beta4, log_weights, containing βi\\beta_i samples normalised log\\log-importance-weights, sum(exp(log_weights)) 11. Use log_sum_exp function (see code.R file documentation) compute needed normalisation information.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling","title":"StatComp Project 1 (2022/23): Simulation and sampling","text":"Use defined functions compute importance sample size N=10000N=10000. Plot empirical weighted CDFs together un-weighted CDFs parameter, help stat_ewcdf, discuss results. achieve simpler ggplot code, may find pivot_longer(???, starts_with(\"beta\")) facet_wrap(vars(name)) useful. Construct 90% credible intervals four model parameters, based importance sample. addition wquantile pivot_longer, methods group_by summarise helpful. may wish define function make_CI taking arguments x, weights, prob (control intended","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-problem","dir":"Articles","previous_headings":"Avoiding long running times","what":"The problem","title":"StatComp Project 1 (2022/23): Hints","text":"Collecting values data.frame inside loop can slow. common issue combining results computed loops using rbind step loop add one rows large vector data.frame can slow; instead linear computational cost size output, cost can become quadratic; needs reallocate memory copy previous results, new version result object.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-solution","dir":"Articles","previous_headings":"Avoiding long running times","what":"The solution","title":"StatComp Project 1 (2022/23): Hints","text":"key avoid reallocating memory instead pre-allocating whole result object loop. However subtleties , examples blow show differences approaches. common solution work simpler intermediate data structures within loop, collect results end. Indexing simple vectors much cheaper reallocating copying memory. Slow: Better, even data.frame indexing assignments causes memory reallocation: Fast; several orders magnitude faster (see timings ) avoiding memory reallocation inside loop: fast, bit compact; using matrix indexing doesn’t need memory reallocation, bit slower vector indexing. potential drawback method works columns output data.frame type (numeric), cases works code simple, readable, generally quick run: Benchmark timing comparisons:","code":"slow_fun <- function(N) {   result <- data.frame()   for (loop in seq_len(N)) {     result <- rbind(       result,       data.frame(A = cos(loop), B = sin(loop))     )   }   result } better_fun <- function(N) {   result <- data.frame(A = numeric(N), B = numeric(N))   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   result } fast_fun <- function(N) {   result_A <- numeric(N)   result_B <- numeric(N)   for (loop in seq_len(N)) {     result_A[loop] <- cos(loop)     result_B[loop] <- sin(loop)   }   data.frame(     A = result_A,     B = result_B   ) } ok_fun <- function(N) {   result <- matrix(0, N, 2)   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   colnames(result) <- c(\"A\", \"B\")   as.data.frame(result) } N <- 10000 bench::mark(   slow = slow_fun(N),   better = better_fun(N),   fast = fast_fun(N),   ok = ok_fun(N) ) #> Warning: Some expressions had a GC in every iteration; so filtering is #> disabled. #> # A tibble: 4 × 6 #>   expression      min   median `itr/sec` mem_alloc `gc/sec` #>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #> 1 slow          3.63s    3.63s     0.276    2.28GB    16.0  #> 2 better      560.5ms  560.5ms     1.78     1.49GB    67.8  #> 3 fast         1.49ms   1.52ms   652.     191.38KB     2.00 #> 4 ok           4.33ms   4.44ms   216.     472.09KB    16.0"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"dynamically-generated-column-names","dir":"Articles","previous_headings":"","what":"Dynamically generated column names","title":"StatComp Project 1 (2022/23): Hints","text":"size matrix data.frame depends parameter, multiple columns similar names, can convenient dynamically generate column names. paste() function can used . example, want data.frame size n×mn\\times m, column names Name1, Name2, etc, can use resulting object looks like :","code":"df <- matrix(runif(15), 5, 3) # Create a matrix with some values in it colnames(df) <- paste(\"Name\", seq_len(ncol(df)), sep = \"\") df <- as.data.frame(df) df #>         Name1     Name2      Name3 #> 1 0.080750138 0.4663935 0.87460066 #> 2 0.834333037 0.4977774 0.17494063 #> 3 0.600760886 0.2897672 0.03424133 #> 4 0.157208442 0.7328820 0.32038573 #> 5 0.007399441 0.7725215 0.40232824"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"multi-column-summarise-constructions","dir":"Articles","previous_headings":"","what":"Multi-column summarise constructions","title":"StatComp Project 1 (2022/23): Hints","text":"summarise() method dplyr package usually used construct simple summaries function computes mu sigma? make return data.frame, can used summarise:","code":"suppressPackageStartupMessages(library(tidyverse)) df <- data.frame(x = rnorm(100)) df %>%   summarise(     mu = mean(x),     sigma = sd(x)   ) #>          mu    sigma #> 1 0.1311013 1.062345 my_fun <- function(x) {   data.frame(     mu = mean(x),     sigma = sd(x)   ) } df %>%   summarise(     my_fun(x)   ) #>          mu    sigma #> 1 0.1311013 1.062345"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"avoiding-unnecessary-for-loops","dir":"Articles","previous_headings":"","what":"Avoiding unnecessary for-loops","title":"StatComp Project 1 (2022/23): Hints","text":"Since operations functions R vectorised, code can often written clear way avoiding unnecessary -loops. simple example, say want compute ∑k=1100cos(k)\\sum_{k=1}^{100} \\cos(k). -loop solution style good solution high performance low-level language like C might look like : taking advantage vectorised calling cos(), function sum(), can reformulate shorter clear vectorised solution: commonly used functions involving vectors TRUE FALSE () (). example, sum, , also operate across elements matrix. row-wise column-wise sums, see rowSums()colSums().","code":"result <- 0 for (k in seq_len(100)) {   result <- result + cos(k) } result <- sum(cos(seq_len(100))) vec <- c(TRUE, TRUE, FALSE, TRUE) all(vec) #> [1] FALSE any(vec) #> [1] TRUE sum(vec) # FALSE == 0, TRUE == 1 #> [1] 3"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"aligning-equations-in-multi-step-derivations","dir":"Articles","previous_headings":"LaTeX equations","what":"Aligning equations in multi-step derivations","title":"StatComp Project 1 (2022/23): Hints","text":"typesetting multi-step derivations, one align equations. LaTeX, can done align align* environments. case, might work RMarkdown, one can use aligned environment instead.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-1","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 1","title":"StatComp Project 1 (2022/23): Hints","text":"Result: ∂∂xf(x,y)=∂∂xexp(2x+3y)=2exp(2x+3y)\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}","code":"\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-2","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 2","title":"StatComp Project 1 (2022/23): Hints","text":"Result: ∂∂xf(x,y)=∂∂xexp(2x+3y)=2exp(2x+3y) \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned}","code":"$$ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} $$"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"instructions","dir":"Articles","previous_headings":"","what":"Instructions:","title":"StatComp Project 2: Scottish weather","text":"Make sure include name student number report.Rmd functions.R. Code displays results (e.g., tables figures) must placed code chunks report.Rmd. Code template files includes example save load results long-running calculations. Code chunks included report.Rmd load function definitions functions.R (function definitions). Appendix code chunks included display code functions.R without running . Use styler package Addin restyling code better consistent readability works .R .Rmd files. project 1, use echo=TRUE analysis code chunks echo=FALSE table display plot-generating code chunks. submitting, include generated html version report file name report.html. project marked whole, 00 4040, using marking guide posted Learn.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"StatComp Project 2: Scottish weather","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. See Tutorial07 exploratory introduction data, techniques wrangling data. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (Google maps online map systems can usually interpret latitude longitude searches). ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm)","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"task-1-seasonal-variability","dir":"Articles","previous_headings":"","what":"Task 1: Seasonal variability","title":"StatComp Project 2: Scottish weather","text":"Note: first half assignment, can essentially ignore data missing; defining averages, just take average whatever observations available relevant data subset. construct averages group_by() summarise() & mean(), happen .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"is-precipitation-seasonally-varying","dir":"Articles","previous_headings":"Task 1: Seasonal variability","what":"Is precipitation seasonally varying?","title":"StatComp Project 2: Scottish weather","text":"daily precipitation data challenging model temperature mix zeros positive values. Plot temperature precipitation data show behaviour, e.g., one years, stations. plots, seasonal effect temperature (“TMIN” “TMAX”) precipitation (“PRCP”)? e.g., colder winter summer? Let winter {Jan, Feb, Mar, Oct, Nov, Dec}\\{\\text{Jan, Feb, Mar, Oct, Nov, Dec}\\}, let summer {Apr, May, Jun, Jul, Aug, Sep}\\{\\text{Apr, May, Jun, Jul, Aug, Sep}\\}. easier code structure, add season information weather data object. Construct Monte Carlo permutation test hypotheses: H0:rainfall distribution winter summerH1:winter summer distributions different expected values \\begin{aligned} H_0:&\\text{rainfall distribution winter summer} \\\\ H_1:&\\text{winter summer distributions different expected values} \\end{aligned}  Use T=|winter average−summer average|T=|\\text{winter average} - \\text{summer average}| test statistic add Summer column data TRUE data defined summer months. Compute separate p-values weather station respective Monte Carlo standard deviations. Construct function p_value_CI construct interval. Collect results suitable data structure, present discuss results. See Project2Hints hints constructing approximate 95%95\\% confidence interval p-value observed counts zero.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"task-2-spatial-weather-prediction","dir":"Articles","previous_headings":"","what":"Task 2: Spatial weather prediction","title":"StatComp Project 2: Scottish weather","text":"second half project, first construct version data set new variable, Value_sqrt_avg, defined square root monthly averaged precipitation values. noted Project2Hints document (Handling non-Gaussian prediction), precipitation values skewed variance increases mean value. Taking square root monthly averages helps alleviate issue, making constant-variance model plausible.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"estimation-and-prediction","dir":"Articles","previous_headings":"Task 2: Spatial weather prediction","what":"Estimation and prediction","title":"StatComp Project 2: Scottish weather","text":", define estimate models square root monthly averaged precipitation values Scotland. basic model (M0M_0) square root monthly average precipitation defined : M0M_0: Value_sqrt_avg ~ Intercept + Longitude + Latitude + Elevation + DecYear Covariates spatial coordinates, elevation, suitable cos\\cos sin\\sin functions used capture seasonal variability. adding covariates cos(2πkt)\\cos(2\\pi k t) sin(2πkt)\\sin(2\\pi k t) frequency k=1,2,…k=1,2,\\dots can also model seasonal variability, defining models M1M_1, M2M_2, M3M_3, M4M_4 predictor expression model MKM_K adds ∑k=1K[γc,kcos(2πkt)+γs,ksin(2πkt)] \\sum_{k=1}^K \\left[ \\gamma_{c,k} \\cos(2\\pi k t) + \\gamma_{s,k} \\sin(2\\pi k t) \\right]  linear predictor M0M_0, coefficients γc,k\\gamma_{c,k} γs,k\\gamma_{s,k} estimated. time variable tt defined DecYear lowest frequency k=1k=1 corresponds cosine function period one full year. Organise code can easily change input data without change estimation code predict new locations times. ’ll need able run estimation different subsets data, well manually constructed covariate values, e.g., illustrate prediction new location present available weather data. Estimate model parameters M0,M1,M2,M3M_0, M_1, M_2, M_3 M4M_4. Present discuss different models results model estimation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"assessment-station-and-season-differences","dir":"Articles","previous_headings":"Task 2: Spatial weather prediction","what":"Assessment: Station and season differences","title":"StatComp Project 2: Scottish weather","text":"interested well model can predict precipitation new locations. Therefore, construct stratified cross-validation groups data weather station computes prediction scores station, well overall cross-validated average scores, aggregated 12 months year. Present discuss results score assessments Squared Error Dawid-Sebastiani scores. model equally good predicting different stations? prediction accuracy across whole year?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"monte-carlo-standard-errors-and-confidence-intervals","dir":"Articles","previous_headings":"","what":"Monte Carlo standard errors and confidence intervals","title":"StatComp Project 2 (2022/23): Hints","text":"plain Monte Carlo estimation expectation value, can usually construct approximate confidence intervals estimating standard deviation estimator construct interval based Normal distribution estimator. However, breaks cases. example, using randomisation test estimate p-value, Normal approximation works p-value pp number samples NN large enough, actually observe non-zero counts. Fortunately, observe zero counts, can construct confidence interval using exact method instead relying Normal approximation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"monte-carlo-randomisation-test-variability","dir":"Articles","previous_headings":"Monte Carlo standard errors and confidence intervals","what":"Monte Carlo randomisation test variability","title":"StatComp Project 2 (2022/23): Hints","text":"Let pp unknown p-value, X∼𝖡𝗂𝗇(N,p)X\\sim\\mathsf{Bin}(N,p) random variable many times observe randomised test statistic extreme extreme observed test statistic. observe X=xX=x estimate p-value p̂=x/N\\widehat{p}=x/N. 𝖤(X)=Np,𝖵𝖺𝗋(X)=Np(1−p),𝖤(p̂)=p,𝖵𝖺𝗋(p̂)=p(1−p)N. \\begin{aligned} \\mathsf{E}(X) &= N p, \\\\ \\mathsf{Var}(X) &= N p (1 - p), \\\\ \\mathsf{E}(\\widehat{p}) &= p, \\\\ \\mathsf{Var}(\\widehat{p}) &= \\frac{p(1-p)}{N} . \\end{aligned}  see variance decreases towards 00 p→0p\\rightarrow 0. can control expectation absolute error. Due Jensen’s inequality (may may heard !) 𝖤(|p̂−p|)≤𝖤[|p̂−p|2]=𝖵𝖺𝗋(p̂)≤12N\\mathsf{E}(|\\widehat{p}-p|)\\leq\\sqrt{\\mathsf{E}[|\\widehat{p}-p|^2]}=\\sqrt{\\mathsf{Var}(\\widehat{p})}\\leq\\frac{1}{2\\sqrt{N}}, last step uses variance maximised p=1/2p=1/2. Thus, guarantee 𝖤(|p̂−p|)≤ϵ\\mathsf{E}(|\\widehat{p}-p|)\\leq \\epsilon ϵ>0\\epsilon>0, can choose N≥14ϵ2N \\geq \\frac{1}{4 \\epsilon^2}. relative error variance 𝖵𝖺𝗋[p̂−pp]=1−pNp\\mathsf{Var}\\left[\\frac{\\widehat{p}-p}{p}\\right]=\\frac{1-p}{Np} goes ∞\\infty p→0p\\rightarrow 0, control relative error uniformly pp increasing NN. therefore need careful assessing results small values pp.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"normal-approximation","dir":"Articles","previous_headings":"Monte Carlo standard errors and confidence intervals","what":"Normal approximation","title":"StatComp Project 2 (2022/23): Hints","text":"x>0x>0, basic approximate 95% confidence interval pp given CIp=p̂±z0.975p̂(1−p̂)N=xN±z0.975x(N−x)N3. CI_p = \\widehat{p} \\pm z_{0.975} \\sqrt{\\frac{\\widehat{p} (1-\\widehat{p})}{N}} = \\frac{x}{N} \\pm z_{0.975} \\sqrt{\\frac{x (N-x)}{N^3}} .  approximation z0.975≈2z_{0.975}\\approx 2, can limit interval width 414N≤ϵ4\\sqrt{\\frac{1}{4N}}\\leq \\epsilon taking N≥4ϵ2N\\geq \\frac{4}{\\epsilon^2}. ϵ=0.02\\epsilon=0.02, get N≥10,000N\\geq 10,000.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"exact-interval-for-x0","dir":"Articles","previous_headings":"Monte Carlo standard errors and confidence intervals","what":"Exact interval for x=0x=0","title":"StatComp Project 2 (2022/23): Hints","text":"observed count x=0x=0, can go back definition confidence interval can construct confidence intervals “inverting test”; interval taken set corresponding null hypothesis rejected. Imagine reject hypothesis H0:p=p0H_0:p=p_0 p0p_0 𝖯X(X≤x∣p0)<0.025\\mathsf{P}_X(X \\leq x \\mid p_0) < 0.025 𝖯X(X≥x∣p0)<0.025\\mathsf{P}_X(X \\geq x \\mid p_0) < 0.025 (nominally give equal tail error probability). x=0x=0, second probability equal 11, condition never used. test therefore rejected 𝖯X(X=0∣p0)<0.025\\mathsf{P}_X(X = 0 \\mid p_0) < 0.025. solve p0p_0: 𝖯X(X=0∣p0)=(1−p0)N<0.0251−p0<0.0251/Np0>1−0.0251/N. \\begin{aligned} \\mathsf{P}_X(X = 0 \\mid p_0) = (1-p_0)^N &< 0.025 \\\\ 1-p_0 < 0.025^{1/N} \\\\ p_0 > 1 - 0.025^{1/N} . \\end{aligned}  set p0p_0 values test rejected p0≤1−0.0251/Np_0 \\leq 1-0.025^{1/N}, x=0x=0 can define confidence interval pp CIp=(0,1−0.0251/N). CI_p = (0, 1-0.025^{1/N}) .  limit width confidence intervals ϵ\\epsilon, need 1−0.0251/N≤ϵ1-0.025^{1/N}\\leq \\epsilon, N≥log(0.025)log(1−ϵ)N\\geq \\frac{\\log(0.025)}{\\log(1-\\epsilon)}. grows much slowly 4ϵ2\\frac{4}{\\epsilon^2} ϵ→0\\epsilon\\rightarrow 0, can safely use NN ’s required bound Normal approximation interval width still fulfill interval width criterion x=0x=0 interval construction.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"bayesian-credible-interval-construction","dir":"Articles","previous_headings":"Monte Carlo standard errors and confidence intervals","what":"Bayesian credible interval construction","title":"StatComp Project 2 (2022/23): Hints","text":"alternative approach construct Bayesian credible interval pp. Let p∼𝖴𝗇𝗂𝖿(0,1)p\\sim\\mathsf{Unif}(0,1) priori. posterior density pp given xx proportional 𝖯(X=x|p)=(Nx)px(1−p)N−x, \\mathsf{P}(X=x|p) = {N \\choose x} p^x(1-p)^{N-x},  shows posterior distribution pp 𝖡𝖾𝗍𝖺(1+x,1+N−x)\\mathsf{Beta}(1+x, 1+N-x), credible interval provided 0.0250.025 0.9750.975 quantiles. R, qbeta(c(0.025, 0.975), shape1 = 1 + x, shape2 = 1 + N - x). construction works N≥1N\\geq 1 0≤x≤N0\\leq x \\leq N.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"prediction-standard-deviations","dir":"Articles","previous_headings":"","what":"Prediction standard deviations","title":"StatComp Project 2 (2022/23): Hints","text":"predict() function can provide prediction standard errors linear predictor, se.fit, half story predicting new data. standard errors include uncertainty information linear predictor curve. full prediction uncertainty, need take observation variation account, lm() estimated via variance residuals. Since residuals new observations assumed conditionally independent predictor curve, prediction variance can estimated sum square prediction standard error residual variance, degrees freedom large. help text lm() see se.fit=TRUE, output list contains elements fit: vector matrix (depending interval argument) se.fit: standard error predicted means residual.scale: residual standard deviations df: degrees freedom residual  also ask prediction intervals, need modify code bit. comparing interval width results predict() interval assuming t-distributions, see identical floating point accuracy.","code":"# When creating a tibble, the construct can use variables defined # first in the later variables; here we use x when constructing y: df <- tibble(x = rnorm(10),              y = 2 + x + rnorm(10, sd = 0.1)) # Estimate a model: fit <- lm(y ~ x, data = df) # Compute prediction mean and standard deviations and add to df_pred: df_pred <- data.frame(x = seq(-2, 2, length.out = 100)) pred <- predict(fit, newdata = df_pred, se.fit = TRUE) df_pred <- df_pred %>%   mutate(mean = pred$fit,          se.fit = pred$se.fit,          sd = sqrt(pred$se.fit^2 + pred$residual.scale^2)) ggplot(df_pred) +   geom_line(aes(x, se.fit, colour = \"se.fit\")) +   geom_line(aes(x, sd, colour = \"sd\")) +   ylab(\"Std. deviations\") # Compute prediction mean and standard deviations and add to df_pred: df_pred <- data.frame(x = seq(-2, 2, length.out = 100)) pred <- predict(fit, newdata = df_pred, se.fit = TRUE, interval = \"prediction\") df_pred <- df_pred %>%   mutate(mean = pred$fit[, \"fit\"],          lwr = pred$fit[, \"lwr\"],          upr = pred$fit[, \"upr\"],          se.fit = pred$se.fit,          sd = sqrt(pred$se.fit^2 + pred$residual.scale^2)) ggplot(df_pred) +   geom_line(aes(x, upr - lwr - (qt(0.975, pred$df) - qt(0.025, pred$df)) * sd)) +   ylab(\"Interval width difference\") +   xlab(\"x\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02Hints.html","id":"handling-non-gaussian-precipitation","dir":"Articles","previous_headings":"","what":"Handling non-Gaussian precipitation","title":"StatComp Project 2 (2022/23): Hints","text":"assessment methods requested project description valid non-Gaussian predictions, non-Gaussianity precipitation data still noticeable monthly average scale. effect constant variance assumption basic lm() model isn’t good fit data. improve , can take square root monthly averages applying modelling part 2. may also model prediction assessment square-root version data.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic Sea","title":"StatComp Project Example: Numerical statistics","text":"Lab 3, investigated data archaeological excavation Gotland Baltic sea. 1920s, battle gravesite 1361 subject several archaeological excavations. total 493493 femurs (thigh bones) (256256 left, 237237 right) found. wanted figure many persons likely buried gravesite. must reasonably least 256256, many ? assignment, use importance sampling estimate credible intervals problem. lab, seemed difficult extract much information sample, since two observations two parameters. investigate including data excavations type might decrease uncertainty number persons buried, improved information average detection probability, ϕ\\phi. function arch_data code.R returns data frame data four different excavations (note: one real; three synthetic data generated assignment). assume model lab excavation, unknown NjN_j, j=1,…,Jj=1,\\dots,J, JJ excavations, common detection probability ϕ\\phi. assume model , conditionally {Nj}\\{N_j\\} ϕ\\phi, (Yj,|Nj,ϕ)∼𝖡𝗂𝗇(Nj,ϕ)(Y_{j,}|N_j,\\phi)\\sim\\mathsf{Bin}(N_j,\\phi), =1,2i=1,2, independent, prior distributions Nj∼𝖦𝖾𝗈𝗆(ξ)N_j\\sim\\mathsf{Geom}(\\xi), 0<ξ<10<\\xi<1, ϕ∼𝖡𝖾𝗍𝖺(,b)\\phi\\sim\\mathsf{Beta}(, b), ,b>0a,b>0, pYj,|Nj=n,ϕ(y)=𝖯(Yj,=y|Nj=n,ϕ)=(ny)ϕy(1−ϕ)n−y,y=0,…,n,pNj(n)=𝖯(Nj=n)=ξ(1−ξ)n,n=0,1,2,3,…,pϕ(ϕ)=ϕa−1(1−ϕ)b−1B(,b),ϕ∈(0,1), \\begin{aligned} p_{Y_{j,}|N_j=n,\\phi}(y) = \\mathsf{P}(Y_{j,}=y|N_j=n,\\phi) &= {n \\choose y} \\phi^y(1-\\phi)^{n-y}, \\quad y=0,\\dots,n, \\\\ p_{N_j}(n) = \\mathsf{P}(N_j=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\(0,1) , \\end{aligned}  B(,b)B(,b) Beta function, see ?beta R.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"joint-probability-function-for-boldsymbolnboldsymboly","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Joint probability function for (𝐍,𝐘)(\\boldsymbol{N},\\boldsymbol{Y})","title":"StatComp Project Example: Numerical statistics","text":"Conditionally 𝐍={N1,…,NJ}\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}, observations 𝐘\\boldsymbol{Y}, conditional posterior distribution (ϕ|𝐍,𝐘)(\\phi|\\boldsymbol{N},\\boldsymbol{Y}) 𝖡𝖾𝗍𝖺(̃,b̃)\\mathsf{Beta}(\\widetilde{},\\widetilde{b}) ̃=+∑j,iYj,\\widetilde{}=+\\sum_{j,} Y_{j,} b̃=b+2∑jNj−∑j,iYj,\\widetilde{b}=b+2\\sum_j N_j -\\sum_{j,} Y_{j,}, sums go j=1,…,Jj=1,\\dots,J =1,2i=1,2. Show joint probability function (𝐍,𝐘)(\\boldsymbol{N},\\boldsymbol{Y}), Nj≥max(Yj,1,Yj,2)N_j\\geq\\max(Y_{j,1},Y_{j,2}) Yj,=0,1,2,…Y_{j,}=0,1,2,\\dots, given by1 p(𝐍,𝐘)=B(̃,b̃)B(,b)∏j=1J[p(Nj)∏=12(NjYj,)], p(\\boldsymbol{N},\\boldsymbol{Y}) = \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)} \\prod_{j=1}^J \\left[ p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\right],  p(Nj)p(N_j) probability mass function Geometric distribution prior NjN_j.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"probability-function-implementation","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Probability function implementation","title":"StatComp Project Example: Numerical statistics","text":"help functions lbeta, lchoose, dgeom, define (my_code.R) function log_prob_NY(N,Y,xi,,b) arguments N (vector length JJ) YY (data frame size J×2J\\times 2, formatted like output arch_data(J)). arguments xi, , b model hyperparameters ξ\\xi, aa, bb. function return logarithm p(𝐍,𝐘)p(\\boldsymbol{N},\\boldsymbol{Y}) combination 𝐍\\boldsymbol{N} 𝐘\\boldsymbol{Y} valid, otherwise return -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Importance sampling function","title":"StatComp Project Example: Numerical statistics","text":"aim construct Bayesian credible interval NjN_j using importance sampling, similarly method used lab 4. , Gaussian approximation posterior parameter used. , instead use prior distribution 𝐍\\boldsymbol{N} sampling distribution, samples Nj[k]∼𝖦𝖾𝗈𝗆(ξ)N_j^{[k]}\\sim\\mathsf{Geom}(\\xi). Since observations 𝐘\\boldsymbol{Y} fixed, posterior probability function p(𝐍|𝐘)p(\\boldsymbol{N}|\\boldsymbol{Y}) 𝐍\\boldsymbol{N} proportional p(𝐍,𝐘)p(\\boldsymbol{N},\\boldsymbol{Y}). define logarithm unnormalised importance weights, log[w[k]]=log[p(𝐍[k],𝐘)]−log[p(𝐍[k])]\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[p(\\boldsymbol{N}^{[k]})], p(𝐍[k])p(\\boldsymbol{N}^{[k]}) product JJ geometric distribution probabilities. Define (my_code.R) function arch_importance(K,Y,xi,,b) arguments Y, xi, , b , first argument K defining number samples generate. function return data frame KK rows J+1J+1 columns, named N1, N2, etc, j=1,…,Jj=1,\\dots,J, containing samples prior distributions NjN_j, final column called Log_Weights, containing re-normalised log-importance-weights, constructed shifting log[w[k]]\\log[w^{[k]}] subtracting largest log[w[k]]\\log[w^{[k]}] value, lab 4.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"more-efficient-alternative","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea > Importance sampling function","what":"More efficient alternative","title":"StatComp Project Example: Numerical statistics","text":"Using 𝖦𝖾𝗈𝗆(ξ)\\mathsf{Geom}(\\xi) prior distribution NjN_j leads rather inefficient importance sampler, especially J=4J=4; weights become zero (Nj[k]<Yj,iN_j^{[k]}<Y_{j,}) near zeros, samples influence practical estimates credible intervals. alternative, can use sampling distributions better adapted posterior distributions. Let Njmin=max(Yj,1,Yj,2)N^\\text{min}_j=\\max(Y_{j,1}, Y_{j,2}) smallest allowed value sample, j=1,…,Jj=1,\\dots,J. eliminate impossible combinations, weights (theory numerically) positive. improve method, can choose different values probability parameter samples, also adapt differently jj. Without motivation, can use values ξj=1/(1+4Njmin)\\xi_j=1/(1 + 4 N^\\text{min}_j) sampling, Nj[k]=Njmin+𝖦𝖾𝗈𝗆(ξj)N_j^{[k]}=N^\\text{min}_j + \\mathsf{Geom}(\\xi_j). Note: Due nature geometric distribution, also equivalent ordinary 𝖦𝖾𝗈𝗆(ξj)\\mathsf{Geom}(\\xi_j) conditioned least NjminN^\\text{min}_j. take different sampling method account, log-weights need defined log[w[k]]=log[p(𝐍[k],𝐘)]−log[p̃(𝐍[k])]\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[\\widetilde{p}(\\boldsymbol{N}^{[k]})], p̃(𝐍[k])=∏j=1Jp𝖦𝖾𝗈𝗆(ξj)(Nj[k]−Njmin)\\widetilde{p}(\\boldsymbol{N}^{[k]})=\\prod_{j=1}^J p_{\\mathsf{Geom}(\\xi_j)}(N_j^{[k]}-N^\\text{min}_j), product JJ different geometric distribution probabilities. may adapt example code (details depend vectorisation method use): Using method, KK doesn’t need larger 100000100000, small K=1000K=1000 starts give reasonable values code testing purposes, K=10000K=10000 take seconds run code fully vectorised (vapply function can used, basic -loop also fine case).","code":"# matrix of minimum allowed N for each j, repeated K times N_min <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J) xi_sample <- 1 / (1 + 4 * N_min) # Sample values and call log_prob_NY for each sample, # store N-values in a K-by-J matrix, and log(p_NY)-values in a vector log_PY N <- matrix(rgeom(..., prob = xi_sample), K, J) + N_min log_PY <- ... # Subtract the sampling log-probabilities log_PY - rowSums(dgeom(N - N_min, prob = xi_sample, log = TRUE))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-n_j","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for NjN_j","title":"StatComp Project Example: Numerical statistics","text":"lab, let ξ=1/1001\\xi=1/1001, =b=1/2a=b=1/2. Use arch_importance wquantile function type=1 (see help text information) compute credible intervals NjN_j. Start including data single excavation, observe interval N1N_1 changes data excavations added. Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for ϕ\\phi","title":"StatComp Project Example: Numerical statistics","text":"Update2 arch_importance function add column Phi samples conditional distribution 𝖡𝖾𝗍𝖺(̃,b̃)\\mathsf{Beta}(\\widetilde{},\\widetilde{b}) row importance sample. Use updated function construct credible intervals ϕ\\phi, one J=1J=1, one J=4J=4. Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"StatComp Project Example Solution: Numerical Statistics","text":"Pairs counts (Yj,1,Yj,2)(Y_{j,1},Y_{j,2}) left right femurs archaeological excavations modelled conditionally independent realisations Binom(Nj,ϕ)\\textrm{Binom}(N_j,\\phi), NjN_j original number people buried excavtion site, ϕ\\phi probability finding buried femur. prior distribution NjN_j, j=1,…,Jj=1,\\dots,J, Geom(ξ)\\textrm{Geom}(\\xi), ξ=1/(1+1000)\\xi=1/(1+1000) (E(Nj)=1000\\textrm{E}(N_j)=1000a priori), detection probability, ϕ∼Beta(12,12)\\phi\\sim\\textrm{Beta}(\\frac{1}{2},\\frac{1}{2}). denote 𝐍={N1,…,NJ}\\boldsymbol{N}=\\{N_1,\\dots,N_J\\} 𝐘={(Y1,1,Y1,2),…,(YJ,1,YJ,2)}\\boldsymbol{Y}=\\{(Y_{1,1},Y_{1,2}),\\dots,(Y_{J,1},Y_{J,2})\\}.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"joint-probability-function","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Joint probability function","title":"StatComp Project Example Solution: Numerical Statistics","text":"Taking product prior density ϕ\\phi, prior probabilities NjN_j, conditional observation probabilities Nj,iN_{j,} integrating ϕ\\phi, get joint probability function (𝐍,𝐘)(\\boldsymbol{N},\\boldsymbol{Y})P(𝐍,𝐘)=∫01pϕ(ϕ)∏j=1Jp(Nj)∏=12p(Yj,|Nj,ϕ)dϕ=∫01ϕa−1(1−ϕ)b−1B(,b)∏j=1Jp(Nj)∏=12(NjYj,)ϕYj,(1−ϕ)Nj−Yj,idϕ=∫01ϕã−1(1−ϕ)b̃−1B(,b)dϕ∏j=1Jp(Nj)∏=12(NjYj,), \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\int_0^1 p_\\phi(\\phi) \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 p(Y_{j,}|N_j,\\phi)\\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\phi^{Y_{j,}}(1-\\phi)^{N_j-Y_{j,}}   \\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(,b)} \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} , \\end{aligned}  ̃=+∑j=1J∑=12Yj,\\widetilde{} = +\\sum_{j=1}^J\\sum_{=1}^2 Y_{j,} b̃=b+∑j=1J∑=12(Nj−Yj,)\\widetilde{b} = b+\\sum_{j=1}^J\\sum_{=1}^2 (N_j - Y_{j,}). integral can renormalised integral distribution probability density function Beta(̃,b̃)\\textrm{Beta}(\\widetilde{},\\widetilde{b}) distribution: P(𝐍,𝐘)=B(̃,b̃)B(,b)∫01ϕã−1(1−ϕ)b̃−1B(̃,b̃)dϕ∏j=1Jp(Nj)∏=12(NjYj,)=B(̃,b̃)B(,b)∏j=1Jp(Nj)∏=12(NjYj,). \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(\\widetilde{},\\widetilde{b})}   \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\\\ &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} . \\end{aligned}","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"credible-intervals-for-n_j-and-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Credible intervals for NjN_j and ϕ\\phi","title":"StatComp Project Example Solution: Numerical Statistics","text":"order obtain posterior credible intervals unknown quantities 𝐍={N1,…,NJ}\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}, ϕ\\phi, implement importance sampling method sample NjN_j shifted Geometric distribution smallest value equal smallest valid NjN_j, given observed counts, expectation corresponding ϕ=1/4\\phi=1/4. sampling 𝐍\\boldsymbol{N} values, corresponding importance weights wkw_k, stored Log_Weights equal log(wk)\\log(w_k) normalised subtracting larges log(wk)\\log(w_k) value. Finally, ϕ\\phi values sampled directly conditional posterior distribution (ϕ|𝐍,𝐘)∼Beta(12+∑j,iYi,j,12+∑j,(Nj−Yi,j))(\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\sim\\textrm{Beta}\\left(\\frac{1}{2}+\\sum_{j,} Y_{,j},\\frac{1}{2}+\\sum_{j,} (N_j-Y_{,j})\\right). implementation arch_importance() function my_code.R shown code appendix, produces importance sample size KK. achieve relatively reliable results, use K=100000K=100000, J=1J=1, 22, 33, 44, takes ca 4 seconds computer. First, use stat_ewcdf plot emprirical cumulative distribution functions importance weighted posterior samples N1N_1, J=1,…,4J=1,\\dots,4. can see random variability increases larger JJ, indicating lower efficiency importance sampler. Also included CDF prior distribution N1N_1. Except lower end distribution, ’s immediately clear effect observations distribution.  Next corresponding plot ϕ\\phi. contrast N1N_1, ’s clear observation pairs get, concentrated distribution ϕ\\phi gets. new observation pair also influences particular lower end distribution, lower quantiles clearly increasing J=4J=4 even though upper quantiles remain relatively stable J≥2J\\geq 2.  Using wquantile extract lower upper 5%5\\% quantiles obtain approximate 90%90\\% posterior credible intervals N1N_1 ϕ\\phi shown following table. can see credible intervals N1N_1 strongly linked information posterior distribution ϕ\\phi. Interestingly, due lowering lower endpoint ϕ\\phi J=2J=2 33, width N1N_1 interval actually increases add information second third. stark contrast common intuition expect uncertainty decrease collect data. behaviour partly due random variability inherent small sample, also due strong non-linearity estimation problem, products ϕNj\\phi N_j much easier estimate individual ϕ\\phi NjN_j parameters. J=4J=4, interval width N1N_1 finally decreases, well width ϕ\\phi interval. Looking values observations, can see J=4J=4 case much larger values cases, leads providing relatively information ϕ\\phi parameter. practical point view, ’s clear estimation problem easy solution. Even multiple excavations, assuming model appropriate value ϕ\\phi ϕ\\phi every excavation, gain modest amount additional information. positive side, model good job approximating reality, borrowing strength across multiple excavations improve estimates uncertainty information excavation. improvements, one likely need closely study excavation precedures, gain better understanding data generating process, might lead improved statistical methods archaeologically relevant outcomes.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"effective-sample-size","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Effective sample size","title":"StatComp Project Example Solution: Numerical Statistics","text":"Using method computing effective importance sample size later course can see even efficient method, sampling inefficient J=4J=4, indicating improved choice importance distribution NjN_j improve precision accuracy results: , EffSampleSize approximate effective sample size (∑kwk)2/∑wk2(\\sum_k w_k)^2/\\sum w_k^2, RelEff relative efficiency, defined ratio EffSampleSize K=100000K=100000.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"profile-likelihood-method-for-frequentist-confidence-interval-for-textrmbinomnp","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Profile likelihood method for frequentist confidence interval for Binom(n,p)\\textrm{Binom}(n,p)","title":"StatComp Project Example Solution: Numerical Statistics","text":"(actually range(y) depends n may mean l0 justified example)","code":"## [1] 274"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"function-definitions","dir":"Articles","previous_headings":"Code appendix","what":"Function definitions","title":"StatComp Project Example Solution: Numerical Statistics","text":"","code":"# Finn Lindgren (s0000000, finnlindgren)    # Joint log-probability # # Usage: #   log_prob_NY(N, Y, xi, a, b) # # Calculates the joint probability for N and Y, when #   N_j ~ Geom(xi) #   phi ~ Beta(a, b) #   Y_{ji} ~ Binom(N_j, phi) #   For j = 1,...,J # # Inputs: #   N: vector of length J #   Y: data.frame or matrix of dimension Jx2 #   xi: The probability parameter for the N_j priors #   a, b: the parameters for the phi-prior # # Output: #   The log of the joint probability for N and Y #   For impossible N,Y combinations, returns -Inf  log_prob_NY <- function(N, Y, xi, a, b) {   # Convert Y to a matrix to allow more vectorised calculations:   Y <- as.matrix(Y)   if (any(N < Y) || any(Y < 0)) {     return(-Inf)   }   atilde <- a + sum(Y)   btilde <- b + 2 * sum(N) - sum(Y) # Note: b + sum(N - Y) would be more general   lbeta(atilde, btilde) - lbeta(a, b) +     sum(dgeom(N, xi, log = TRUE)) +     sum(lchoose(N, Y)) }  # Importance sampling for N,phi conditionally on Y # # Usage: #   arch_importance(K,Y,xi,a,b) # # Inputs: #   Y,xi,a,b have the same meaning and syntax as for log_prob_NY(N,Y,xi,a,b) #   The argument K defines the number of samples to generate. # # Output: #  A data frame with K rows and J+1 columns, named N1, N2, etc, #  for each j=1,…,J, containing samples from an importance sampling #  distribution for Nj, #  a column Phi with sampled phi-values, #  and a final column called Log_Weights, containing re-normalised #  log-importance-weights, constructed by shifting log(w[k]) by subtracting the #  largest log(w[k]) value, as in lab 4.  arch_importance <- function(K, Y, xi, a, b) {   Y <- as.matrix(Y)   J <- nrow(Y)   N_sample <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J)   xi_sample <- 1 / (1 + 4 * N_sample)    N <- matrix(rgeom(K * J, prob = xi_sample), K, J) + N_sample   colnames(N) <- paste0(\"N\", seq_len(J))    Log_Weights <- vapply(     seq_len(K),     function(k) log_prob_NY(N[k, , drop = TRUE], Y, xi, a, b),     0.0) -     rowSums(dgeom(N - N_sample, prob = xi_sample, log = TRUE))    phi <- rbeta(K, a + sum(Y), b + 2 * rowSums(N) - sum(Y))    cbind(as.data.frame(N),         Phi = phi,         Log_Weights = Log_Weights - max(Log_Weights)) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-intervals","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Confidence intervals","title":"StatComp Project Example Solution: Numerical Statistics","text":"Poisson parameter interval simulations: Summarise coverage probabilities interval widths: Plot CDFs CI width distributions:","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Archaeology","title":"StatComp Project Example Solution: Numerical Statistics","text":"Compute credible intervals NjN_j different amounts data, JJ: completeness, compute credible intervals NjN_j, 1≤J1\\leq J, J=1,…,4J=1,\\dots,4, analysis focuses N1N_1 ϕ\\phi: Restructure interval data.frame focus N1N_1 ϕ\\phi :","code":"xi <- 1 / 1001 a <- 0.5 b <- 0.5  K <- 1000000 NW1 <- arch_importance(K, arch_data(1), xi, a, b) NW2 <- arch_importance(K, arch_data(2), xi, a, b) NW3 <- arch_importance(K, arch_data(3), xi, a, b) NW4 <- arch_importance(K, arch_data(4), xi, a, b) CI <- rbind(   NW1 %>%     reframe(       J = 1,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = NA,       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW2 %>%     reframe(       J = 2,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW3 %>%     reframe(       J = 3,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW4 %>%     reframe(       J = 4,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = wquantile(N4,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ) ) CI_ <- CI %>%   pivot_longer(     cols = c(N1, N2, N3, N4, Phi),     names_to = \"Variable\",     values_to = \"value\"   ) %>%   pivot_wider(     names_from = End,     values_from = value   ) %>%   filter(!is.na(Lower)) %>%   mutate(Width = Upper - Lower) knitr::kable(CI_ %>% filter(Variable %in% c(\"N1\", \"Phi\")) %>%   arrange(Variable, J))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"deriving-the-hessian","dir":"Articles","previous_headings":"","what":"Deriving the Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Define individual terms fk=yklog(1+e−ηk)+(N−yk)log(1+eηk)f_k=y_k\\log(1+e^{-\\eta_k}) + (N-y_k)\\log(1+e^{\\eta_k}), f=∑k=1nfkf=\\sum_{k=1}^n f_k. know ∂ηk/∂θ1≡1\\partial\\eta_k/\\partial\\theta_1\\equiv 1 ∂ηk/∂θ2=k\\partial\\eta_k/\\partial\\theta_2=k. derivatives respect θ1\\theta_1 θ2\\theta_2 can obtain chain rule: dfkdθi=∂ηk∂θi∂fk∂ηk=∂ηk∂θi[yk−e−ηk1+e−ηk+(N−yk)eηk1+eηk]=∂ηk∂θi[yk−e−ηk/2eηk/2+e−ηk/2+(N−yk)eηk/2e−ηk/2+eηk/2]=∂ηk∂θi[−yke−ηk/2+eηk/2eηk/2+e−ηk/2+N1e−ηk+1]=∂ηk∂θi[Ne−ηk+1−yk]\\begin{align*} \\frac{df_k}{d\\theta_i} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\frac{\\partial f_k}{\\partial\\eta_k} \\\\   &= \\frac{\\partial \\eta_k}{\\partial \\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k}}{1+e^{-\\eta_k}} +   (N-y_k) \\frac{e^{\\eta_k}}{1+e^{\\eta_k}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   (N-y_k) \\frac{e^{\\eta_k/2}}{e^{-\\eta_k/2}+e^{\\eta_k/2}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   - y_k \\frac{e^{-\\eta_k/2}+e^{\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   N \\frac{1}{e^{-\\eta_k}+1}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\ \\end{align*} Since derivatives ηk\\eta_k respect θ1\\theta_1 θ2\\theta_2 depend values θ1\\theta_1 θ2\\theta_2, second order derivatives d2fkdθidθj=∂ηk∂θi∂ηk∂θj∂∂ηk[Ne−ηk+1−yk]=∂ηk∂θi∂ηk∂θjNe−ηk(e−ηk+1)2=∂ηk∂θi∂ηk∂θjN(e−ηk/2+eηk/2)2=∂ηk∂θi∂ηk∂θjN4cosh(ηk/2)2\\begin{align*} \\frac{d^2f_k}{d\\theta_i d\\theta_j} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{\\partial}{\\partial\\eta_k}\\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N e^{-\\eta_k}}{(e^{-\\eta_k}+1)^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{(e^{-\\eta_k/2}+e^{\\eta_k/2})^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{4\\cosh(\\eta_k/2)^2}   \\\\ \\end{align*} Plugging θ\\theta-derivatives gives Hessain contribution term fkf_k N4cosh(ηk/2)2[1kkk2].   \\frac{N}{4\\cosh(\\eta_k/2)^2} \\begin{bmatrix}1 & k \\\\ k & k^2\\end{bmatrix} .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"positive-definite-hessian","dir":"Articles","previous_headings":"","what":"Positive definite Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"show total Hessian ff positive definite n≥2n \\geq 2, define vectors 𝐮k=[1k]\\boldsymbol{u}_k=\\begin{bmatrix}1 \\\\ k\\end{bmatrix} dk=N4cosh(ηk/2)2d_k=\\frac{N}{4\\cosh(\\eta_k/2)^2}. Define 2-nn matrix 𝐔=[𝐮1𝐮2⋯𝐮n]\\boldsymbol{U}=\\begin{bmatrix}\\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\cdots & \\boldsymbol{u}_n\\end{bmatrix} diagonal matrix 𝐃\\boldsymbol{D} Dii=diD_{ii}=d_i. product 𝐔𝐃𝐔⊤\\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{U}^\\top another way writing Hessian ff. Since vectors 𝐮k\\boldsymbol{u}_k non-parallel, did_i strictly positive combinations θ1\\theta_1 θ2\\theta_2, matrix full rank (rank 2) n≥2n \\geq 2, positive definite.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"alternative-reasoning","dir":"Articles","previous_headings":"","what":"Alternative reasoning","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Hessian fkf_k positive semi-definite, since can written dk𝐮k𝐮k⊤d_k\\boldsymbol{u}_k\\boldsymbol{u}_k^\\top dk>0d_k > 0 vector 𝐮k\\boldsymbol{u}_k. sum positive definite matrix positive semi-definite matrix positive definite, ’s sufficient prove sum first two terms positive definite. positive scaling constant ww, determinant [1111]+w[1224] \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}+w\\begin{bmatrix}1 & 2\\\\2 & 4\\end{bmatrix}  (1+w)(1+4w)−(1+2w)2=1+5w+4w2−1−4w−4w2=w>0(1+w)(1+4w)-(1+2w)^2=1+5w+4w^2-1-4w-4w^2=w > 0. means (positively) weighted sum two matrices positive definite (since positive semi-definite, positive determinant rules ot sum positive semi-definite). proves total Hessian positive definite n≥2n\\geq 2.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"remark","dir":"Articles","previous_headings":"","what":"Remark","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Note first proof positive definiteness, didn’t actually need know specific values 𝐮k\\boldsymbol{u}_k vectors, proportional gradients fo ηk\\eta_k respect 𝛉=(θ1,θ2)\\boldsymbol{\\theta}=(\\theta_1,\\theta_2); sufficient non-parallel, ensuring 𝐔\\boldsymbol{U} full rank. means linear model ηk\\eta_k set parameters 𝛉=(θ1,…,θp)\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_p) leads positive definite Hessian model, collection gradient vectors (η1,…,ηn)(\\eta_1,\\dots,\\eta_n) respect 𝛉\\boldsymbol{\\theta} collective rank least pp.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"case 3, negated log-likelihood l̃(θ)=neθ−θ∑=1nyi+constant \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant}  1st order derivative ∂∂θl̃(θ)=neθ−∑=1nyi \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i  shows θ̂ML=log(y¯)\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y}), 2nd order derivative ∂2∂θ2l̃(θ)=neθ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta  equal nλn\\lambda yiy_i values, inverse expected Hessian 1/(nλ)1/(n\\lambda).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"can test interval construction methods following code. methods always produce valid intervals. y¯=0\\overline{y}=0, first method produces single point “interval”. third method fails y¯=0\\overline{y}=0, due log zero. can happen nn λ\\lambda close zero.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"probability theory, know p(θ)=p(λ)dλ(θ)dθp(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}, p(θ)=aexp(−aλ)exp(θ)=aexp(θ−aeθ) p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right)","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"First, sample Gaussian approximation posterior distribution. avoid numerical problems, calculate log unnormalised weights, shift results get numerically sensible scale exponentiate. credible interval θ\\theta can now extracted quantiles weighted sample, λ\\lambda-interval obtained transformation λ=exp(θ)\\lambda=\\exp(\\theta): code steps following:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043 a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval lambda_interval <- exp(theta_interval) lambda_interval"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"figure shows empirical distributions unweighted weighted samples {xk}\\{x_k\\} (weights {wk}\\{w_k\\}), together theoretical posterior empirical distribution function. weighted, importance sampling, version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution log(λ)\\log(\\lambda) excellent approximation even add importance sampling step.  lower figure, importance weights normalised average, values 1 mean corresponding sample values occur often true, target, distribution raw samples, values 1 mean sample values occur less frequently target distribution. scaled weights shown logarithmic scale.","code":"# Plotting updated to include a plot of the importance weights p1 <-   ggplot(data.frame(lambda = exp(x), weights = weights)) +   ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\")) +   scale_x_log10(limits = c(3, 20)) +   labs(colour = \"Type\")  p2 <- ggplot(data.frame(lambda = exp(x), weights = weights),              mapping = aes(lambda, weights / mean(weights))) +   ylab(\"Importance weights\") +   geom_point() +   geom_line() +   geom_hline(yintercept = 1) +   scale_y_log10() +   scale_x_log10(limits = c(3, 20))  # The patchwork library provides a versatile framework # for combining multiple ggplot figures into one. library(patchwork) (p1 / p2) ## Warning in geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + : All aesthetics have length 1, but the data has 20000 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ##   a single row."},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File→\\rightarrowNew Project→\\rightarrowNew Directory→\\rightarrowNew Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File→\\rightarrowOpen Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models","text":"Configuring code syntax diagnostics: Tools →\\rightarrowGlobal Options →\\rightarrowCode →\\rightarrowDiagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models","text":"Create new R script file: File→\\rightarrowNew File→\\rightarrowR Script Save new R script file: File→\\rightarrowSave, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models","text":"start simple linear model yi=β0+ziβz+ei, \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned}  yiy_i observed values, ziz_i observed values (“covariates”) believe linear relationship yiy_i, eie_i observation noise components variance σe2\\sigma_e^2, β0\\beta_0, βz\\beta_z, σe2\\sigma_e^2 model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models","text":"Now want plot linear predictor model function zz, z=−10z=-10 z=20z=20. First, create new data.frame: Note don’t corresponding yy-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation.","code":"newdata <- data.frame(z = -10:20)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(\"ggplot2-in-packages\")` for more information. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated."},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models","text":"can now use something like following code plot results: Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models (solutions)","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File→\\rightarrowNew Project→\\rightarrowNew Directory→\\rightarrowNew Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File→\\rightarrowOpen Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models (solutions)","text":"Configuring code syntax diagnostics: Tools →\\rightarrowGlobal Options →\\rightarrowCode →\\rightarrowDiagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models (solutions)","text":"Create new R script file: File→\\rightarrowNew File→\\rightarrowR Script Save new R script file: File→\\rightarrowSave, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models (solutions)","text":"start simple linear model yi=β0+ziβz+ei, \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned}  yiy_i observed values, ziz_i observed values (“covariates”) believe linear relationship yiy_i, eie_i observation noise components variance σe2\\sigma_e^2, β0\\beta_0, βz\\beta_z, σe2\\sigma_e^2 model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models (solutions)","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models (solutions)","text":"Now want plot linear predictor model function zz, z=−10z=-10 z=20z=20. First, create new data.frame: Note don’t corresponding yy-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation. Solution:","code":"newdata <- data.frame(z = -10:20) data_pred <- cbind(newdata,                    data.frame(fit = predict(mod, newdata))) ggplot(data) +   geom_point(aes(z, y)) +   geom_line(aes(z, fit), data = data_pred) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models (solutions)","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Solution: Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"pred <- cbind(   newdata,   predict(mod, newdata, interval = \"prediction\") ) plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: Solution: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions <- function(x, newdata, xname = \"x\") {   pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   ggplot(pred) +     geom_line(aes_string(xname, \"fit\")) +     geom_ribbon(aes_string(x = xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25) } plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation idioms with `aes()`. ## ℹ See also `vignette(\"ggplot2-in-packages\")` for more information. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated."},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models (solutions)","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot. Solution:","code":"# Extended version of the solution: plot_prediction <- function(x, newdata, xname = \"x\",                             xlab = NULL, ylab = NULL, ...) {   if (is.null(xlab)) {     xlab <- xname   }   if (is.null(ylab)) {     ylab <- \"Response\"   }    pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   pl <- ggplot() +     geom_line(data = pred,               aes_string(xname, \"fit\")) +     geom_ribbon(data = pred,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Also add the confidence intervals for the predictor curve   conf <- cbind(newdata,                 predict(x, newdata, interval = \"confidence\"))   pl <- pl +     geom_ribbon(data = conf,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Add lables:   pl + xlab(xlab) + ylab(ylab) }"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models (solutions)","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data) plot_prediction(mod2, newdata, xname = \"z\", ylab = \"y\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") +   ggtitle(\"Confidence and prediction intervals\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods. Solution:","code":"formulas <- c(y ~ 1,               y ~ z,               y ~ z + I(z^2),               y ~ z + I(z^2) + I(z^3)) mods <- lapply(formulas, function(x) lm(x, data))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models (solutions)","text":"can now use something like following code plot results:  Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (m+1m+1 points, mm dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. mm-dimensional problems derivatives approximated finite differences, gradient calculation costs least mm extra function evaluations, Hessian costs least 2m22m^2 extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: m=2m=2, number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: 202202 (103 iterations) Gradient Descent: 14132+2⋅9405=3.2942×10414132+2\\cdot 9405 = 3.2942\\times 10^{4} (9404 iterations) Newton: 29+2⋅23+8⋅22=25129+2\\cdot 23 +8\\cdot 22 = 251 (22 iterations) BFGS: 54+2⋅41+8⋅1=14454+2\\cdot 41 +8\\cdot 1 = 144 (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization","text":"example, ’ll look model connection values (x1,…,xn)(x_1,\\dots,x_n) observations yiy_i (see Lecture 2) can written ∈{1,2,…,n}xi=−1n−1𝐗=[𝟏𝐱]𝛍=𝐗[θ1θ2]log(σi)=θ3+xiθ4yi=𝖭(μi,σi2),(independent). \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned}  Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use (0,0,0,0)(0,0,0,0) starting point optimisation. Check ?optim help text information result object contains. optimisation converge? Compute store estimated expectations σ\\sigma values like : estimates σi\\sigma_i function xix_i look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted θ3+xiθ4\\theta_3+x_i\\theta_4.","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals.","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # ℹ 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1)"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization (solutions)","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization (solutions)","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization (solutions)","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (m+1m+1 points, mm dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. mm-dimensional problems derivatives approximated finite differences, gradient calculation costs least mm extra function evaluations, Hessian costs least 2m22m^2 extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: m=2m=2, number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: 202202 (103 iterations) Gradient Descent: 14132+2⋅9405=3.2942×10414132+2\\cdot 9405 = 3.2942\\times 10^{4} (9404 iterations) Newton: 29+2⋅23+8⋅22=25129+2\\cdot 23 +8\\cdot 22 = 251 (22 iterations) BFGS: 54+2⋅41+8⋅1=14454+2\\cdot 41 +8\\cdot 1 = 144 (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization (solutions)","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization (solutions)","text":"example, ’ll look model connection values (x1,…,xn)(x_1,\\dots,x_n) observations yiy_i (see Lecture 2) can written ∈{1,2,…,n}xi=−1n−1𝐗=[𝟏𝐱]𝛍=𝐗[θ1θ2]log(σi)=θ3+xiθ4yi=𝖭(μi,σi2),(independent). \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned}  Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. Solution: aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use (0,0,0,0)(0,0,0,0) starting point optimisation. Solution: Check ?optim help text information result object contains. optimisation converge? Solution: Answer: optimisation converged opt$convergence 0. least optim() thinks coverged, since triggered ’s convergence tolerances. Compute store estimated expectations σ\\sigma values like : estimates σi\\sigma_i function xix_i look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted θ3+xiθ4\\theta_3+x_i\\theta_4.","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) neg_log_lik <- function(theta, y, X) {   -sum(dnorm(x = y,              mean = X %*% theta[1:2],              sd = exp(X %*% theta[3:4]),              log = TRUE)) } opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik,              y = y, X = X,              method = \"BFGS\") data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization (solutions)","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals. Solution:","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # ℹ 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1) opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik, y = y, X = X,              method = \"BFGS\",              hessian = TRUE) covar <- solve(opt$hessian) covar ##               [,1]         [,2]          [,3]         [,4] ## [1,]  0.0017940969 -0.005724814 -0.0005605767  0.001121155 ## [2,] -0.0057248136  0.036111726  0.0035360846 -0.007072178 ## [3,] -0.0005605767  0.003536085  0.0190674071 -0.028134823 ## [4,]  0.0011211550 -0.007072178 -0.0281348225  0.056269649"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks. Solution: accompanying Tutorial03Solutions tutorial document contains solutions explicitly, make easier review material workshops. can also run document Tutorials pane RStudio.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution Y∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ)Y\\sim\\mathsf{Poisson}(\\lambda) expectation λ\\lambda, variance also λ\\lambda. coefficient variation defined ratio standard deviation expectation, gives 1/λ1/\\sqrt{\\lambda}. real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: X∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2),(Y|X=x)∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇[exp(μ+x)], \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned}  conditional expectation YY given X=xX=x μ\\mu λ(μ,x)=exp(μ+x)\\lambda(\\mu,x)=\\exp(\\mu+x), μ\\mu (fixed) parameter. marginal expectation (still conditionally μ\\mu σ\\sigma) can obtained via tower property (law total expectation), 𝖤(Y|μ,σ)=𝖤[𝖤(Y|X)]=𝖤[λ(μ,X)]=𝖤[exp(μ+X)]=exp(μ+σ2/2) \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned}  last step comes expectation log-Normal distribution. multiple observations, can write model xi∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2), independent =1,…,n,(yi|xi)∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇[exp(μ+xi)], conditionally independent =1,…,n. \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} Consider following simulation code: expectation YY? Theory exercise: Using tower property (law total variance), theoretically derive expression 𝖵𝖺𝗋(Y)\\mathsf{Var}(Y). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Monte Carlo integration: Use Monte Carlo integration approximate probability mass function pY(m|μ,σ)=𝖯(Y=m|μ,σ)=∫−∞∞𝖯(Y=m|μ,σ,X=x)pX(x|σ)dx \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned}  m=0,1,2,3,…,15m=0,1,2,3,\\dots,15, μ=log(2)−1/2\\mu=\\log(2)-1/2 σ=1\\sigma=1. Check formulas resulting theoretical expectation YY parameter combination. efficient, vectorise calculations evaluating conditional probability function YY mm values , simulated value x[k]∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2)x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2) value, k=1,2,…,Kk=1,2,\\dots,K. Use K=10000K=10000 samples. Plot resulting probability mass function pY(m|μ,σ)p_Y(m|\\mu,\\sigma) together ordinary Poisson probability mass function expectation value, λ=2\\lambda = 2. (Use theory convince two models YY expectation.) Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Use function plot results μ=log(8)−1/8\\mu=\\log(8)-1/8, σ=1/2\\sigma = 1/2, m=0,1,…,30m=0,1,\\dots,30 adding P_Y P_Poisson geoms ","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration","text":"``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total 493493 femurs (thigh bones) (256256 left, 237237 right) found. want figure many persons likely buried gravesite. must reasonably least 256256, many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration","text":"build simple model problem, assume number left (y1=256y_1=256) right (y2=237y_2=237) femurs two independent observations 𝖡𝗂𝗇(N,ϕ)\\mathsf{Bin}(N,\\phi) distribution. NN total number people buried ϕ\\phi probability finding femur, left right, NN ϕ\\phi unknown parameters. probability function single observation y∼𝖡𝗂𝗇(N,ϕ)y\\sim\\mathsf{Bin}(N,\\phi) p(y|N,ϕ)=(Ny)ϕy(1−ϕ)N−y.\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*} function arch_loglike() StatCompLab package evaluates combined log-likelihood log[p(𝐲|N,ϕ)]\\log[p(\\boldsymbol{y}|N,\\phi)] collection 𝐲\\boldsymbol{y} yy-observations. data.frame columns N phi provided, log-likelihood row-pair (N,ϕ)(N,\\phi) returned. combined l(y1,y2|N,θ)=logp(y1,y2|N,ϕ)l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi) data set {y1,y2}\\{y_1,y_2\\} given l(y1,y2|N,θ)=−logΓ(y1+1)−logΓ(y2+1)=−logΓ(N−y1+1)−logΓ(N−y2+1)+2logΓ(N+1)=+(y1+y2)log(ϕ)+(2N−y1−y2)log(1−ϕ) \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~}  + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} task use Monte Carlo integration estimate posterior expectations NN ϕ\\phi, prior distributions N∼𝖦𝖾𝗈𝗆(ξ)N\\sim\\mathsf{Geom}(\\xi), 0<ξ<10<\\xi<1, ϕ∼𝖡𝖾𝗍𝖺(,b)\\phi\\sim\\mathsf{Beta}(, b), ,b>0a,b>0. mathematical definitions : Let NN 𝖦𝖾𝗈𝗆(ξ)\\mathsf{Geom}(\\xi), ξ>0\\xi>0, prior distribution, let ϕ\\phi 𝖡𝖾𝗍𝖺(,b)\\mathsf{Beta}(,b), ,b>0a,b>0, prior distribution: pN(n)=𝖯(N=n)=ξ(1−ξ)n,n=0,1,2,3,…,pϕ(ϕ)=ϕa−1(1−ϕ)b−1B(,b),ϕ∈[0,1]. \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned}  probability mass function pN(n)p_N(n) can evaluated dgeom() R, density pϕ(ϕ)p_\\phi(\\phi) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration","text":"excavation took place, archaeologist believed around 10001000 individuals buried, find around half femurs. encode belief Bayesian analysis, set ξ=1/(1+1000)\\xi=1/(1+1000), corresponds expected total count 10001000, =b=2a=b=2, makes ϕ\\phi likely close 1/21/2 00 11. posterior density/probability function (N,ϕ|𝐲)(N,\\phi|\\boldsymbol{y}) pN,ϕ|𝐲(n,ϕ|𝐲)=pN,ϕ,𝐲(n,ϕ,𝐲)p𝐲(𝐲)=pN(n)pϕ(ϕ)p(𝐲|n,ϕ)p𝐲(𝐲) \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} can estimate p𝐲(𝐲)p_{\\boldsymbol{y}}(\\boldsymbol{y}) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions p𝐲(𝐲)=∑n=max(y1,y2)∞∫01pN,ϕ,𝐲(n,ϕ,𝐲)dϕ=∑n=max(y1,y2)∞∫01p(𝐲|n,ϕ)pN(n)pϕ(ϕ)dϕ𝖤(N|𝐲)=∑n=max(y1,y2)∞∫01npN,ϕ|𝐲(n,ϕ)dϕ=∑n=max(y1,y2)∞∫01np(𝐲|n,ϕ)p𝐲(𝐲)pN(n)pϕ(ϕ)dϕ𝖤(ϕ|𝐲)=∑n=max(y1,y2)∞∫01ϕpN,ϕ|𝐲(n,ϕ)dϕ=∑n=max(y1,y2)∞∫01ϕp(𝐲|n,ϕ)p𝐲(𝐲)pN(n)pϕ(ϕ)dϕ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned}","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample n[k]∼𝖦𝖾𝗈𝗆(ξ)n^{[k]}\\sim\\mathsf{Geom}(\\xi) ϕ[k]∼𝖡𝖾𝗍𝖺(,b)\\phi^{[k]}\\sim\\mathsf{Beta}(,b), compute Monte Carlo estimates p̂𝐲(𝐲)=1K∑k=1Kp(𝐲|n[k],ϕ[k])𝖤̂(N|𝐲)=1Kp̂𝐲(𝐲)∑k=1Kn[k]p(𝐲|n[k],ϕ[k])𝖤̂(ϕ|𝐲)=1Kp̂𝐲(𝐲)∑k=1Kϕ[k]p(𝐲|n[k],ϕ[k]) \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned}  Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Note: shown lecture 3, conditional posterior distribution ϕ\\phi fixed N=nN=n (ϕ|n,𝐲)∼𝖡𝖾𝗍𝖺(+y1+y2,b+2n−y1−y2)(\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2). can used construct potentially efficient method, ϕ[k]\\phi^{[k]} sampled conditionally n[k]n^{[k]}, integration importance weights adjusted appropriately.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution Y∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ)Y\\sim\\mathsf{Poisson}(\\lambda) expectation λ\\lambda, variance also λ\\lambda. coefficient variation defined ratio standard deviation expectation, gives 1/λ1/\\sqrt{\\lambda}. real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: X∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2),(Y|X=x)∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇[exp(μ+x)], \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned}  conditional expectation YY given X=xX=x μ\\mu λ(μ,x)=exp(μ+x)\\lambda(\\mu,x)=\\exp(\\mu+x), μ\\mu (fixed) parameter. marginal expectation (still conditionally μ\\mu σ\\sigma) can obtained via tower property (law total expectation), 𝖤(Y|μ,σ)=𝖤[𝖤(Y|X)]=𝖤[λ(μ,X)]=𝖤[exp(μ+X)]=exp(μ+σ2/2) \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned}  last step comes expectation log-Normal distribution. multiple observations, can write model xi∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2), independent =1,…,n,(yi|xi)∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇[exp(μ+xi)], conditionally independent =1,…,n. \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} Consider following simulation code: expectation YY? Theory exercise: Using tower property (law total variance), theoretically derive expression 𝖵𝖺𝗋(Y)\\mathsf{Var}(Y). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Solution: 𝖵𝖺𝗋(Y|μ,σ)=𝖤[𝖵𝖺𝗋(Y|X)]+𝖵𝖺𝗋[𝖤(Y|X)]=𝖤[λ(μ,X)]+𝖵𝖺𝗋[λ(μ,X)]=𝖤[exp(μ+X)]+𝖵𝖺𝗋[exp(μ+X)]=exp(μ+σ2/2)+(exp(σ2)−1)exp(2μ+σ2)=exp(μ+σ2/2)[1+(exp(σ2)−1)exp(μ+σ2/2)] \\begin{aligned} \\mathsf{Var}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{Var}(Y|X)] + \\mathsf{Var}[\\mathsf{E}(Y|X)] \\\\&= \\mathsf{E}[\\lambda(\\mu,X)] + \\mathsf{Var}[\\lambda(\\mu,X)] \\\\&= \\mathsf{E}[\\exp(\\mu+X)] +\\mathsf{Var}[\\exp(\\mu+X)] \\\\&= \\exp(\\mu + \\sigma^2/2) + (\\exp(\\sigma^2)-1) \\exp(2\\mu + \\sigma^2) \\\\&= \\exp(\\mu + \\sigma^2/2) \\left[ 1 + (\\exp(\\sigma^2)-1) \\exp(\\mu + \\sigma^2/2) \\right] \\end{aligned}  Since second factor always ≥1\\geq 1, shows variance never smaller expectation. Monte Carlo integration: Use Monte Carlo integration approximate probability mass function pY(m|μ,σ)=𝖯(Y=m|μ,σ)=∫−∞∞𝖯(Y=m|μ,σ,X=x)pX(x|σ)dx \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned}  m=0,1,2,3,…,15m=0,1,2,3,\\dots,15, μ=log(2)−1/2\\mu=\\log(2)-1/2 σ=1\\sigma=1. Check formulas resulting theoretical expectation YY parameter combination. efficient, vectorise calculations evaluating conditional probability function YY mm values , simulated value x[k]∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2)x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2) value, k=1,2,…,Kk=1,2,\\dots,K. Use K=10000K=10000 samples. Plot resulting probability mass function pY(m|μ,σ)p_Y(m|\\mu,\\sigma) together ordinary Poisson probability mass function expectation value, λ=2\\lambda = 2. (Use theory convince two models YY expectation.) Solution:  plot, overdispersed Poisson probability function (just computed) compared plain Poisson probability function expectation value. Lines values included clarity. Note: case, second approach faster, situations first approach required, since doesn’t require storing random numbers time, thus allowing much larger K=nmcK=n_{\\text{mc}} value used. Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Solution: Use function plot results μ=log(8)−1/8\\mu=\\log(8)-1/8, σ=1/2\\sigma = 1/2, m=0,1,…,30m=0,1,\\dots,30 adding P_Y P_Poisson geoms Solution:","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) # Vectorised over m mu <- log(2) - 1/2 K <- 10000 m <- 0:15 P_Y <- numeric(length(m)) for (loop in seq_len(K)) {   x <- rnorm(1, sd = 1)   P_Y <- P_Y + dpois(m, lambda = exp(mu + x)) } P_Y <- P_Y / K  # Plot the results suppressPackageStartupMessages(library(ggplot2)) ggplot(data.frame(m = m,                   P_Y = P_Y,                   P_Poisson = dpois(m, lambda = 2))) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\")) doverpois <- function(m, mu, sigma, K) {   P_Y <- numeric(length(m))   for (loop in seq_len(K)) {     x <- rnorm(1, sd = sigma)     P_Y <- P_Y + dpois(m, lambda = exp(mu + x))   }   P_Y <- P_Y / K    data.frame(m = m,              P_Y = P_Y,              P_Poisson = dpois(m, lambda = exp(mu + sigma^2/2))) } ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000)) suppressPackageStartupMessages(library(ggplot2)) ggplot(doverpois(m = 0:30, mu = log(8)-0.125, sigma = 0.5, K = 10000)) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total 493493 femurs (thigh bones) (256256 left, 237237 right) found. want figure many persons likely buried gravesite. must reasonably least 256256, many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"build simple model problem, assume number left (y1=256y_1=256) right (y2=237y_2=237) femurs two independent observations 𝖡𝗂𝗇(N,ϕ)\\mathsf{Bin}(N,\\phi) distribution. NN total number people buried ϕ\\phi probability finding femur, left right, NN ϕ\\phi unknown parameters. probability function single observation y∼𝖡𝗂𝗇(N,ϕ)y\\sim\\mathsf{Bin}(N,\\phi) p(y|N,ϕ)=(Ny)ϕy(1−ϕ)N−y.\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*} function arch_loglike() StatCompLab package evaluates combined log-likelihood log[p(𝐲|N,ϕ)]\\log[p(\\boldsymbol{y}|N,\\phi)] collection 𝐲\\boldsymbol{y} yy-observations. data.frame columns N phi provided, log-likelihood row-pair (N,ϕ)(N,\\phi) returned. combined l(y1,y2|N,θ)=logp(y1,y2|N,ϕ)l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi) data set {y1,y2}\\{y_1,y_2\\} given l(y1,y2|N,θ)=−logΓ(y1+1)−logΓ(y2+1)=−logΓ(N−y1+1)−logΓ(N−y2+1)+2logΓ(N+1)=+(y1+y2)log(ϕ)+(2N−y1−y2)log(1−ϕ) \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~}  + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} task use Monte Carlo integration estimate posterior expectations NN ϕ\\phi, prior distributions N∼𝖦𝖾𝗈𝗆(ξ)N\\sim\\mathsf{Geom}(\\xi), 0<ξ<10<\\xi<1, ϕ∼𝖡𝖾𝗍𝖺(,b)\\phi\\sim\\mathsf{Beta}(, b), ,b>0a,b>0. mathematical definitions : Let NN 𝖦𝖾𝗈𝗆(ξ)\\mathsf{Geom}(\\xi), ξ>0\\xi>0, prior distribution, let ϕ\\phi 𝖡𝖾𝗍𝖺(,b)\\mathsf{Beta}(,b), ,b>0a,b>0, prior distribution: pN(n)=𝖯(N=n)=ξ(1−ξ)n,n=0,1,2,3,…,pϕ(ϕ)=ϕa−1(1−ϕ)b−1B(,b),ϕ∈[0,1]. \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned}  probability mass function pN(n)p_N(n) can evaluated dgeom() R, density pϕ(ϕ)p_\\phi(\\phi) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"excavation took place, archaeologist believed around 10001000 individuals buried, find around half femurs. encode belief Bayesian analysis, set ξ=1/(1+1000)\\xi=1/(1+1000), corresponds expected total count 10001000, =b=2a=b=2, makes ϕ\\phi likely close 1/21/2 00 11. posterior density/probability function (N,ϕ|𝐲)(N,\\phi|\\boldsymbol{y}) pN,ϕ|𝐲(n,ϕ|𝐲)=pN,ϕ,𝐲(n,ϕ,𝐲)p𝐲(𝐲)=pN(n)pϕ(ϕ)p(𝐲|n,ϕ)p𝐲(𝐲) \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} can estimate p𝐲(𝐲)p_{\\boldsymbol{y}}(\\boldsymbol{y}) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions p𝐲(𝐲)=∑n=max(y1,y2)∞∫01pN,ϕ,𝐲(n,ϕ,𝐲)dϕ=∑n=max(y1,y2)∞∫01p(𝐲|n,ϕ)pN(n)pϕ(ϕ)dϕ𝖤(N|𝐲)=∑n=max(y1,y2)∞∫01npN,ϕ|𝐲(n,ϕ)dϕ=∑n=max(y1,y2)∞∫01np(𝐲|n,ϕ)p𝐲(𝐲)pN(n)pϕ(ϕ)dϕ𝖤(ϕ|𝐲)=∑n=max(y1,y2)∞∫01ϕpN,ϕ|𝐲(n,ϕ)dϕ=∑n=max(y1,y2)∞∫01ϕp(𝐲|n,ϕ)p𝐲(𝐲)pN(n)pϕ(ϕ)dϕ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned}","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample n[k]∼𝖦𝖾𝗈𝗆(ξ)n^{[k]}\\sim\\mathsf{Geom}(\\xi) ϕ[k]∼𝖡𝖾𝗍𝖺(,b)\\phi^{[k]}\\sim\\mathsf{Beta}(,b), compute Monte Carlo estimates p̂𝐲(𝐲)=1K∑k=1Kp(𝐲|n[k],ϕ[k])𝖤̂(N|𝐲)=1Kp̂𝐲(𝐲)∑k=1Kn[k]p(𝐲|n[k],ϕ[k])𝖤̂(ϕ|𝐲)=1Kp̂𝐲(𝐲)∑k=1Kϕ[k]p(𝐲|n[k],ϕ[k]) \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned}  Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Solution: Note: shown lecture 3, conditional posterior distribution ϕ\\phi fixed N=nN=n (ϕ|n,𝐲)∼𝖡𝖾𝗍𝖺(+y1+y2,b+2n−y1−y2)(\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2). can used construct potentially efficient method, ϕ[k]\\phi^{[k]} sampled conditionally n[k]n^{[k]}, integration importance weights adjusted appropriately.","code":"estimate <- function(y, xi, a, b, K) {   samples <- data.frame(     N = rgeom(K, prob = xi),     phi = rbeta(K, shape1 = a, shape2 = b)   )   loglike <- arch_loglike(param = samples, y = y)   p_y <- mean(exp(loglike))   c(p_y = p_y,     E_N = mean(samples$N * exp(loglike) / p_y),     E_phi = mean(samples$phi * exp(loglike) / p_y)) } estimate(y = c(237, 256), xi = 1/1001, a = 0.5, b = 0.5, K = 10000) ##          p_y          E_N        E_phi  ## 6.360064e-06 9.884083e+02 3.694532e-01"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk. Solution: accompanying Tutorial04Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. separate T4sol.Rmd document https://github.com/finnlindgren/StatCompLab/blob/main/vignettes/articles/T4sol.Rmd source document standalone solution shown T4sol StatCompLab website.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration","text":"Consider Poisson model observations 𝐲={y1,…,yn}\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}: yi∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ),independent =1,…,n. \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned}  joint probability mass function p(𝐲|λ)=exp(−nλ)∏=1nλyiyi! p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!}  week 4 lecture, two parameterisations considered. now add third option: θ=λ\\theta = \\lambda, θ̂ML=1n∑=1nyi=y¯\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y} θ=λ\\theta = \\sqrt{\\lambda}, θ̂ML=y¯\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}} θ=log(λ)\\theta = \\log(\\lambda), θ̂ML=log(y¯)\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right) week 4 lecture, know inverse expected Fisher information λ/n\\lambda/n case 1 1/(4n)1/(4n) case 2. case 3, show inverse expected Fisher information 1/(nλ)1/(n\\lambda).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration","text":"Use approximation method large nn lecture construct approximate confidence intervals λ\\lambda using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values y¯\\overline{y}. Experiment different values n lambda simulation y. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; 𝖯𝐲|θ(θ∈CI(𝐲)|θ)≥1−α\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha θ\\theta (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration","text":"Assume true value λ=10\\lambda=10, simulate sample 𝐲\\boldsymbol{y} size n=5n=5. Now consider Bayesian version Poisson model, prior model λ∼𝖤𝗑𝗉() \\lambda \\sim \\mathsf{Exp}()  probability density function p(λ)=aexp(−aλ)p(\\lambda) = \\exp(-\\lambda). One can show exact posterior distribution λ\\lambda given 𝐲\\boldsymbol{y} 𝖦𝖺𝗆𝗆𝖺(1+∑=1nyi,+n)\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let θ=log(λ)\\theta=\\log(\\lambda), λ=exp(θ)\\lambda=\\exp(\\theta). Show prior probability density θ\\theta p(θ)=aexp(θ−aeθ)p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration","text":"posterior density function θ\\theta p(θ|𝐲)=p(θ)p(𝐲|θ)p(𝐲) p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})}  log-density logp(θ|𝐲)=const+θ(1+ny¯)−(+n)exp(θ), \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) ,  taking derivatives find mode θ̃=log(1+ny¯+n)\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right), negated Hessian 1+ny¯1+n\\overline{y} mode. information can construct Gaussian approximation posterior distribution, p̃(θ|𝐲)∼𝖭𝗈𝗋𝗆𝖺𝗅(θ̃,11+ny¯)\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}}).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration","text":"Simulate sample 𝐱={x1,…,xm}\\boldsymbol{x}=\\{x_1,\\dots,x_m\\} Gaussian approximation posterior distribution, large m>10000m > 10000, hyperparameter =1/5a=1/5. need calculate unnormalised importance weights wkw_k, k=1,…,mk=1,\\dots,m, wk=p(θ)p(𝐲|θ)p̃(θ|𝐲)|θ=xk. w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} .  Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, log(wk)\\log(w_k), new, equivalent unnormalised weights w̃k=exp[log(wk)−maxjlog(wj)]\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]. Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval θ\\theta using 𝐱\\boldsymbol{x} sample associate weights, transform credible interval λ\\lambda","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function λ\\lambda (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample λk=exp(xk)\\lambda_k=\\exp(x_k), (unnormalised) weights wkw_k. Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Consider Poisson model observations 𝐲={y1,…,yn}\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}: yi∼𝖯𝗈𝗂𝗌𝗌𝗈𝗇(λ),independent =1,…,n. \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned}  joint probability mass function p(𝐲|λ)=exp(−nλ)∏=1nλyiyi! p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!}  week 4 lecture, two parameterisations considered. now add third option: θ=λ\\theta = \\lambda, θ̂ML=1n∑=1nyi=y¯\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y} θ=λ\\theta = \\sqrt{\\lambda}, θ̂ML=y¯\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}} θ=log(λ)\\theta = \\log(\\lambda), θ̂ML=log(y¯)\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right) week 4 lecture, know inverse expected Fisher information λ/n\\lambda/n case 1 1/(4n)1/(4n) case 2. case 3, show inverse expected Fisher information 1/(nλ)1/(n\\lambda). Solution: case 3, negated log-likelihood l̃(θ)=neθ−θ∑=1nyi+constant \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant}  1st order derivative ∂∂θl̃(θ)=neθ−∑=1nyi \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i  shows θ̂ML=log(y¯)\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y}), 2nd order derivative ∂2∂θ2l̃(θ)=neθ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta  equal nλn\\lambda yiy_i values, inverse expected Hessian 1/(nλ)1/(n\\lambda).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Use approximation method large nn lecture construct approximate confidence intervals λ\\lambda using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). Solution: can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values y¯\\overline{y}. Experiment different values n lambda simulation y. Solution: y¯=0\\overline{y}=0, first method produces single point “interval”. third method fails y¯=0\\overline{y}=0, due log zero. can happen nn λ\\lambda close zero. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; 𝖯𝐲|θ(θ∈CI(𝐲)|θ)≥1−α\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha θ\\theta (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Assume true value λ=10\\lambda=10, simulate sample 𝐲\\boldsymbol{y} size n=5n=5. Solution: Now consider Bayesian version Poisson model, prior model λ∼𝖤𝗑𝗉() \\lambda \\sim \\mathsf{Exp}()  probability density function p(λ)=aexp(−aλ)p(\\lambda) = \\exp(-\\lambda). One can show exact posterior distribution λ\\lambda given 𝐲\\boldsymbol{y} 𝖦𝖺𝗆𝗆𝖺(1+∑=1nyi,+n)\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let θ=log(λ)\\theta=\\log(\\lambda), λ=exp(θ)\\lambda=\\exp(\\theta). Show prior probability density θ\\theta p(θ)=aexp(θ−aeθ)p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right).","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Solution: Alternative 1: probability theory, know p(θ)=p(λ)dλ(θ)dθp(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}, p(θ)=aexp(−aλ)exp(θ)=aexp(θ−aeθ) p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right) Alternative 2: First, derive CDF: 𝖯(θ≤x)=𝖯(log(λ)≤x)=𝖯(λ≤ex)=1−exp(−aex)\\mathsf{P}(\\theta \\leq x)=\\mathsf{P}(\\log(\\lambda) \\leq x)=\\mathsf{P}(\\lambda \\leq e^x) = 1 - \\exp(-e^x), CDF Exponential distribution. Taking derivative respect xx gives density pθ(x)=−exp(−aex)ddx(−aex)=exp(−aex)(aex)=aexp(x−aex)p_\\theta(x)=-\\exp(-e^x) \\frac{d}{dx} (-e^x) = \\exp(-e^x) (e^x) = \\exp(x - e^x), needed result. posterior density function θ\\theta p(θ|𝐲)=p(θ)p(𝐲|θ)p(𝐲) p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})}  log-density logp(θ|𝐲)=const+θ(1+ny¯)−(+n)exp(θ), \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) ,  taking derivatives find mode θ̃=log(1+ny¯+n)\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right), negated Hessian 1+ny¯1+n\\overline{y} mode. information can construct Gaussian approximation posterior distribution, p̃(θ|𝐲)∼𝖭𝗈𝗋𝗆𝖺𝗅(θ̃,11+ny¯)\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}}).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Simulate sample 𝐱={x1,…,xm}\\boldsymbol{x}=\\{x_1,\\dots,x_m\\} Gaussian approximation posterior distribution, large m>10000m > 10000, hyperparameter =1/5a=1/5. Solution: need calculate unnormalised importance weights wkw_k, k=1,…,mk=1,\\dots,m, wk=p(θ)p(𝐲|θ)p̃(θ|𝐲)|θ=xk. w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} .  Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, log(wk)\\log(w_k), new, equivalent unnormalised weights w̃k=exp[log(wk)−maxjlog(wj)]\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]. Solution: Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval θ\\theta using 𝐱\\boldsymbol{x} sample associate weights, transform credible interval λ\\lambda Solution:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function λ\\lambda (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample λk=exp(xk)\\lambda_k=\\exp(x_k), (unnormalised) weights wkw_k. Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution? Solution: importance sampling version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution log(λ)\\log(\\lambda) excellent approximation even add importance sampling step.","code":"ggplot(data.frame(lambda = exp(x), weights = weights)) +   xlim(0, 20) + ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\")) ## Warning in geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + : All aesthetics have length 1, but the data has 20000 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ##   a single row."},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 06: Prediction assessment with proper scores","text":"lab session explore Probabilistic prediction proper scores Open github repository clone project Tutorial 2 4 (either https://rstudio.cloud computer, upgrade StatCompLab package. lab, can either work .R file new .Rmd file. Solution: accompanying Tutorial06Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"d-printer","dir":"Articles","previous_headings":"","what":"3D printer","title":"Tutorial 06: Prediction assessment with proper scores","text":"aim build assess statistical models material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator wants know two models, named B, better capturing distributions random deviations. models use linear model coneection CAD_Weight Actual_Weight. denote CAD weight observations ii x_i, corresponding actual weight yiy_i. two models defined Model : yi∼𝖭𝗈𝗋𝗆𝖺𝗅[θ1+θ2xi,exp(θ3+θ4xi)]y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3 + \\theta_4 x_i)] Model B: yi∼𝖭𝗈𝗋𝗆𝖺𝗅[θ1+θ2xi,exp(θ3)+exp(θ4)xi2)]y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3) + \\exp(\\theta_4) x_i^2)] printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, leads model B approximation . basic physics assumption error CAD software calculation weight proportional weight . Model hand slightly mathematically convenient, motivation physics. Start loading data plot .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"estimate-and-predict","dir":"Articles","previous_headings":"","what":"Estimate and predict","title":"Tutorial 06: Prediction assessment with proper scores","text":"Next week, assess model predictions using cross validation, data split separate parts parameter estimation prediction assessment, order avoid reduce bias prediction assessments. simplicity week, start using entire data set parameter estimation prediction assessment.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"estimate","dir":"Articles","previous_headings":"Estimate and predict","what":"Estimate","title":"Tutorial 06: Prediction assessment with proper scores","text":"First, use filament1_estimate() StatCompLab package estimate two models B using filament1 data. See help text information.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"prediction","dir":"Articles","previous_headings":"Estimate and predict","what":"Prediction","title":"Tutorial 06: Prediction assessment with proper scores","text":"Next, use filament1_predict() compute probabilistic predictions Actual_Weight using two estimated models. Inspect predictions drawing figures, e.g. geom_ribbon(aes(CAD_Weight, ymin = lwr, ymax = upr), alpha = 0.25) (alpha transparency), together observed data. can useful join predictions data new data.frame object get access prediction information data plotting.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"prediction-scores","dir":"Articles","previous_headings":"","what":"Prediction Scores","title":"Tutorial 06: Prediction assessment with proper scores","text":"Now, use score calculator function proper_score() StatCompLab package compute squared error, Dawid-Sebastiani, Interval scores (target coverage probability 90%). ’s useful joint prediction information data set cbind, e.g. mutate() can access needed information. basic summary results, compute average score S¯({Fi,yi})\\overline{S}(\\{F_i,y_i\\}) model type score, present result knitr::kable(). scores indicate one models better worse ? three score types agree ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"data-splitting","dir":"Articles","previous_headings":"","what":"Data splitting","title":"Tutorial 06: Prediction assessment with proper scores","text":"Now, use sample() function generate random selection rows data set, split two parts, ca 50% used parameter estimation, 50% used prediction assessment. Redo previous analysis problem using division data. score results agree previous results?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"lab session explore Probabilistic prediction proper scores Open github repository clone project Tutorial 2 4 (either https://rstudio.cloud computer, upgrade StatCompLab package. lab, can either work .R file new .Rmd file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"d-printer","dir":"Articles","previous_headings":"","what":"3D printer","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"aim build assess statistical models material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator wants know two models, named B, better capturing distributions random deviations. models use linear model coneection CAD_Weight Actual_Weight. denote CAD weight observations ii x_i, corresponding actual weight yiy_i. two models defined Model : yi∼𝖭𝗈𝗋𝗆𝖺𝗅[θ1+θ2xi,exp(θ3+θ4xi)]y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3 + \\theta_4 x_i)] Model B: yi∼𝖭𝗈𝗋𝗆𝖺𝗅[θ1+θ2xi,exp(θ3)+exp(θ4)xi2)]y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3) + \\exp(\\theta_4) x_i^2)] printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, leads model B approximation . basic physics assumption error CAD software calculation weight proportional weight . Model hand slightly mathematically convenient, motivation physics. Start loading data plot . Solution:","code":"data(\"filament1\", package = \"StatCompLab\") suppressPackageStartupMessages(library(tidyverse)) ggplot(filament1, aes(CAD_Weight, Actual_Weight, colour = Material)) +   geom_point()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"estimate-and-predict","dir":"Articles","previous_headings":"","what":"Estimate and predict","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Next week, assess model predictions using cross validation, data split separate parts parameter estimation prediction assessment, order avoid reduce bias prediction assessments. simplicity week, start using entire data set parameter estimation prediction assessment.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"estimate","dir":"Articles","previous_headings":"Estimate and predict","what":"Estimate","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"First, use filament1_estimate() StatCompLab package estimate two models B using filament1 data. See help text information. Solution:","code":"fit_A <- filament1_estimate(filament1, \"A\") fit_B <- filament1_estimate(filament1, \"B\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"prediction","dir":"Articles","previous_headings":"Estimate and predict","what":"Prediction","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Next, use filament1_predict() compute probabilistic predictions Actual_Weight using two estimated models. Solution: Inspect predictions drawing figures, e.g. geom_ribbon(aes(CAD_Weight, ymin = lwr, ymax = upr), alpha = 0.25) (alpha transparency), together observed data. can useful join predictions data new data.frame object get access prediction information data plotting. Solution:  , geom_point call gets data input ensure data point plotted .","code":"pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) ggplot(rbind(cbind(pred_A, filament1, Model = \"A\"),              cbind(pred_B, filament1, Model = \"B\")),        mapping = aes(CAD_Weight)) +   geom_line(aes(y = mean, col = Model)) +   geom_ribbon(aes(ymin = lwr, ymax = upr, fill = Model), alpha = 0.25) +   geom_point(aes(y = Actual_Weight), data = filament1)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"prediction-scores","dir":"Articles","previous_headings":"","what":"Prediction Scores","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Now, use score calculator function proper_score() StatCompLab package compute squared error, Dawid-Sebastiani, Interval scores (target coverage probability 90%). ’s useful joint prediction information data set cbind, e.g. mutate() can access needed information. Solution: See next section compact alternative (first combining prediction information models, computing scores one go). basic summary results, compute average score S¯({Fi,yi})\\overline{S}(\\{F_i,y_i\\}) model type score, present result knitr::kable(). Solution: scores indicate one models better worse ? three score types agree ? Solution: squared error score doesn’t really care difference two models, since doesn’t directly involve variance model (paramter estimates mean different, much). two scores indicate model B better model .","code":"score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_AB <-   rbind(cbind(score_A, model = \"A\"),         cbind(score_B, model = \"B\")) %>%   group_by(model) %>%           summarise(se = mean(se),                     ds = mean(ds),                     interval = mean(interval)) knitr::kable(score_AB)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"data-splitting","dir":"Articles","previous_headings":"","what":"Data splitting","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Now, use sample() function generate random selection rows data set, split two parts, ca 50% used parameter estimation, 50% used prediction assessment. Solution: Redo previous analysis problem using division data. Solution: score results agree previous results? Solution: results random variability due smaller size estimation prediction sets, likely agree previous results.","code":"idx_est <- sample(filament1$Index,                   size = round(nrow(filament1) * 0.5),                   replace = FALSE) filament1_est <- filament1 %>% filter(Index %in% idx_est) filament1_pred <- filament1 %>% filter(!(Index %in% idx_est)) fit_A <- filament1_estimate(filament1_est, \"A\") fit_B <- filament1_estimate(filament1_est, \"B\") pred_A <- filament1_predict(fit_A, newdata = filament1_pred) pred_B <- filament1_predict(fit_B, newdata = filament1_pred) scores <-   rbind(cbind(pred_A, filament1_pred, model = \"A\"),         cbind(pred_B, filament1_pred, model = \"B\")) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   )  score_summary <- scores %>%   group_by(model) %>%   summarise(se = mean(se),             ds = mean(ds),             interval = mean(interval)) knitr::kable(score_summary)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 07: Data wrangling and cross validation","text":"lab session explore Data wrangling; restructuring data frames Graphical data exploration Cross validation lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 23.7.0 higher. Solution: accompanying Tutorial07Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. Throughout tutorial, ’ll make use data wrangling plotting tools dplyr, magrittr, ggplot2 tidyverse packages, start code library(tidyverse) library(StatCompLab). Note exercises tutorial build , often one solution similar previous solutions, just additions /modifications. data wrangling functions used tutorial mutate add alter variables filter keep rows fulfilling given criteria group_by perform analyses within subgroups defined one variables summarise compute data summaries, usually subgroups select keep variables left_join join two data frames together, based common identifier variables pivot_wider split value variable new named variables based category variable pivot_longer gather several variables new single value variable, names stored new category variable %>%, “pipe” operator used glue sequence operations together feeding result operation first argument following function call head view first rows data frame. can useful debugging sequence “piped” expressions, placing last pipe sequence pull extract contents single variable Functions plotting, ggplot initialising plots geom_point drawing points facet_wrap splitting plot “facet” plots, based one category variables","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"Tutorial 07: Data wrangling and cross validation","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (google maps online map systems can usually interpret latitude longitude searches). ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm) values data object 502901 rows, don’t want try view whole object directly. Instead, can start summarising . Start counting many observations station , type measurement. shortest approach use count() function. generalisable approach use group_by(), summarise(), n() functions. See description ?count count() connected others. avoid create temporary named variables, end pipe operations call knitr::kable(), especially ’re working RMarkdown document.","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"exploratory-plotting","dir":"Articles","previous_headings":"","what":"Exploratory plotting","title":"Tutorial 07: Data wrangling and cross validation","text":", looked station data weather measurements separately. plotting, least like access station names instead identifying codes, give humanly readable presentation. can accomplished left_join() function, can add copies rows one data frame another, one columns match. Create new variable, ghcnd observation contains measurements station data: Now plot daily minimum maximum temperature measurements connected lines function time (DecYear), different colour element, separate subplot station (facet_wrap(~variablename)), labeled station names. , avoid creating temporary named variable. Instead, feed initial data wrangling result ggplot() directly. Due amount data, ’s difficult see clear patterns . Produce two figures, one showing yearly averages TMIN TMAX points, one showing monthly seasonal averages (months 1 12) TMIN TMAX, separately station. , avoid creating temporary named variable. previous code, insert calls group_by() summarise(), modify x-values aesthetics. common patterns yearly values, monthly seasonal values?","code":"ghcnd <- left_join(ghcnd_values, ghcnd_stations, by = \"ID\") head(ghcnd)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"scatter-plots","dir":"Articles","previous_headings":"Exploratory plotting","what":"Scatter plots","title":"Tutorial 07: Data wrangling and cross validation","text":"want scatter plot TMIN TMAX, need rearrange data bit. can use pivot_wider function, can turn name variable values variable several named variable. Note measurement elements present given day, NA’s produced default. Optionally, filter rows calling ggplot(). Draw scatterplot daily TMIN vs TMAX station, colour determined month.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation","text":"Choose one stations, create new data variable data ghcnd yearly averages TMIN column (previous pivot_wider output), missing values removed filter().","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"within-sample-assessment","dir":"Articles","previous_headings":"Cross validation","what":"Within-sample assessment","title":"Tutorial 07: Data wrangling and cross validation","text":"Now, using whole data estimate linear model TMIN, lm() formula TMIN ~ 1 + Year, compute average 80% Interval score (use proper_score() used lab 6) prediction intervals TMIN observations data. See ?predict.lm documentation predict() method models estimated lm().","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"cross-validation-1","dir":"Articles","previous_headings":"Cross validation","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation","text":"now want compute 5 average 80% Interval scores 5-fold cross validation based random partition data 5 approximately equal parts. First add new column Group data defining partitioning, using mutate(). One approach compute random permutation index vector, use modulus operator %% reduce 5 values, ceiling() scaled indices. loop partition groups, estimating model leaving group , predicting scoring predictions group. Compare resulting scores one based whole data set. average cross validation score larger smaller?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"lab session explore Data wrangling; restructuring data frames Graphical data exploration Cross validation Open github repository clone project Lab 2 4 lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.7.0 higher. Throughout tutorial, ’ll make use data wrangling plotting tools dplyr, magrittr, ggplot2 tidyverse packages, start code library(tidyverse) library(StatCompLab). Note exercises tutorial build , often one solution similar previous solutions, just additions /modifications. data wrangling functions used tutorial mutate add alter variables filter keep rows fulfilling given criteria group_by perform analyses within subgroups defined one variables summarise compute data summaries, usually subgroups select keep variables left_join join two data frames together, based common identifier variables pivot_wider split value variable new named variables based category variable pivot_longer gather several variables new single value variable, names stored new category variable %>%, “pipe” operator used glue sequence operations together feeding result operation first argument following function call head view first rows data frame. can useful debugging sequence “piped” expressions, placing last pipe sequence pull extract contents single variable Functions plotting, ggplot initialising plots geom_point drawing points facet_wrap splitting plot “facet” plots, based one category variables","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (google maps online map systems can usually interpret latitude longitude searches). Solution: ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm) values data object 502901 rows, don’t want try view whole object directly. Instead, can start summarising . Start counting many observations station , type measurement. shortest approach use count() function. generalisable approach use group_by(), summarise(), n() functions. See description ?count count() connected others. avoid create temporary named variables, end pipe operations call knitr::kable(), especially ’re working RMarkdown document. Solution:","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\") knitr::kable(ghcnd_stations) ghcnd_values %>%   count(ID, Element) %>%   knitr::kable()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"exploratory-plotting","dir":"Articles","previous_headings":"","what":"Exploratory plotting","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":", looked station data weather measurements separately. plotting, least like access station names instead identifying codes, give humanly readable presentation. can accomplished left_join() function, can add copies rows one data frame another, one columns match. Create new variable, ghcnd observation contains measurements station data: Now plot daily minimum maximum temperature measurements connected lines function time (DecYear), different colour element, separate subplot station (facet_wrap(~variablename)), labeled station names. , avoid creating temporary named variable. Instead, feed initial data wrangling result ggplot() directly. Solution:  Due amount data, ’s difficult see clear patterns . Produce two figures, one showing yearly averages TMIN TMAX points, one showing monthly seasonal averages (months 1 12) TMIN TMAX, separately station. , avoid creating temporary named variable. previous code, insert calls group_by() summarise(), modify x-values aesthetics. common patterns yearly values, monthly seasonal values? Solution: yearly averages seem slight increasing trend:  monthly seasonal averages clearly show seasonal pattern. shapes similar stations, average amplitude varies bit:","code":"ghcnd <- left_join(ghcnd_values, ghcnd_stations, by = \"ID\") head(ghcnd) ## # A tibble: 6 × 11 ##   ID           Year Month   Day DecYear Element Value Name    Latitude Longitude ##   <chr>       <int> <int> <int>   <dbl> <chr>   <dbl> <chr>      <dbl>     <dbl> ## 1 UKE00105874  1960     1     1   1960  TMAX      3.9 BRAEMAR     57.0     -3.40 ## 2 UKE00105874  1960     1     1   1960  TMIN      3.3 BRAEMAR     57.0     -3.40 ## 3 UKE00105874  1960     1     2   1960. TMAX      7.2 BRAEMAR     57.0     -3.40 ## 4 UKE00105874  1960     1     2   1960. TMIN     -8.3 BRAEMAR     57.0     -3.40 ## 5 UKE00105874  1960     1     3   1960. TMAX      6.7 BRAEMAR     57.0     -3.40 ## 6 UKE00105874  1960     1     3   1960. TMIN     -6.7 BRAEMAR     57.0     -3.40 ## # ℹ 1 more variable: Elevation <dbl> ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   ggplot(aes(DecYear, Value, colour = Element)) +   geom_line() +   facet_wrap(~ Name) ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   group_by(ID, Name, Element, Year) %>%   summarise(Value = mean(Value), .groups = \"drop\") %>%   ggplot(aes(Year, Value, colour = Element)) +   geom_point() +   facet_wrap(~ Name) ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   group_by(ID, Name, Element, Month) %>%   summarise(Value = mean(Value), .groups = \"drop\") %>%   ggplot(aes(Month, Value, colour = Element)) +   geom_point() +   facet_wrap(~ Name)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"scatter-plots","dir":"Articles","previous_headings":"Exploratory plotting","what":"Scatter plots","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"want scatter plot TMIN TMAX, need rearrange data bit. can use pivot_wider function, can turn name variable values variable several named variable. Note measurement elements present given day, NA’s produced default. Optionally, filter rows calling ggplot(). Draw scatterplot daily TMIN vs TMAX station, colour determined month. Solution:","code":"ghcnd %>%   pivot_wider(names_from = Element, values_from = Value) %>%   filter(!is.na(TMIN) & !is.na(TMAX)) %>%   ggplot(aes(TMIN, TMAX, colour = factor(Month))) +   geom_point() +   facet_wrap(~ Name)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Choose one stations, create new data variable data ghcnd yearly averages TMIN column (previous pivot_wider output), missing values removed filter(). Solution:","code":"data <- ghcnd %>%   filter(ID == \"UKE00105875\") %>%   pivot_wider(names_from = Element, values_from = Value) %>%   filter(!is.na(TMIN)) %>%   group_by(ID, Name, Year) %>%   summarise(TMIN = mean(TMIN), .groups = \"drop\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"within-sample-assessment","dir":"Articles","previous_headings":"Cross validation","what":"Within-sample assessment","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Now, using whole data estimate linear model TMIN, lm() formula TMIN ~ 1 + Year, compute average 80% Interval score (use proper_score() used lab 6) prediction intervals TMIN observations data. See ?predict.lm documentation predict() method models estimated lm(). Solution:","code":"fit0 <- lm(TMIN ~ 1 + Year, data = data) pred0 <- predict(fit0, newdata = data,                  interval = \"prediction\", level = 0.8) score0 <- mean(proper_score(   \"interval\", data$TMIN,   lwr = pred0[, \"lwr\"], upr = pred0[,\"upr\"], alpha = 0.8))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"cross-validation-1","dir":"Articles","previous_headings":"Cross validation","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"now want compute 5 average 80% Interval scores 5-fold cross validation based random partition data 5 approximately equal parts. First add new column Group data defining partitioning, using mutate(). One approach compute random permutation index vector, use modulus operator %% reduce 5 values, ceiling() scaled indices. Solution: loop partition groups, estimating model leaving group , predicting scoring predictions group. Compare resulting scores one based whole data set. average cross validation score larger smaller? Solution: average cross validation score problem usually larger one whole data set; ’s intended reduce risk underestimating prediction error, expect. Repeat random group allocation see much influences cross validation score average.","code":"data <-    data %>%   mutate(Group = sample(seq_len(nrow(data)), size = nrow(data), replace = FALSE),          Group = (Group %% 5) + 1) # Alternative: # data <-  #  data %>% #  mutate(Group = sample(seq_len(nrow(data)), size = nrow(data), replace = FALSE), #         Group = ceiling(Group / nrow(data) * 5)) scores <- numeric(5) for (grp in seq_len(5)) {   fit <- lm(TMIN ~ 1 + Year, data = data %>% filter(Group != grp))   pred <- predict(fit, newdata = data %>% filter(Group == grp),                      interval = \"prediction\", level = 0.8)   scores[grp] <- mean(proper_score(     \"interval\",     (data %>% filter(Group == grp)) %>% pull(\"TMIN\"),     lwr = pred[, \"lwr\"], upr = pred[,\"upr\"], alpha = 0.8)) } knitr::kable(data.frame(\"Whole data score\" = score0,                         \"Cross validation score\" = mean(scores)))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 08: Floating point computations and least squares","text":"lab session explore Speed matrix computations Floating point accuracy least squares estimation statistical linear models Open github repository clone project Lab 2 4 upgrade StatCompLab package lab, can work .R file, working code chunks .Rmd highly recommended. measuring relative running times different methods, ’ll use bench package. won’t need load package library. Instead, check bench::mark() runs without errors. doesn’t, ’s package installed, install install.packages(\"bench\") R Console. Suggested code setup code chunk: Solution: accompanying Tutorial08Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":"suppressPackageStartupMessages(library(tidyverse)) library(StatCompLab)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"numerical-speed","dir":"Articles","previous_headings":"","what":"Numerical speed","title":"Tutorial 08: Floating point computations and least squares","text":"Define following function computing vector norm: Try simple vector can verify result correct, e.g. now take look computational speed matrix operations. First construct vector 𝐛\\boldsymbol{b} matrix 𝐀\\boldsymbol{}: Let’s check speed difference 𝐀⊤𝐛\\boldsymbol{}^\\top\\boldsymbol{b} (𝐛⊤𝐀)⊤(\\boldsymbol{b}^\\top\\boldsymbol{})^\\top (convince two expressions equivalent exact arithmetic, possibly also floating point arithmetic). function t() construct transpose matrix. Note b vector (opposed single-column matrix) interpreted either column vector row vector, depending operation. b vector, .matrix(b) always single-column matrix, t(b) single-row matrix, b %*% interpreted t(b) %*% . method computing 𝐀⊤𝐛\\boldsymbol{}^\\top \\boldsymbol{b} faster? Use bench::mark() compare different ways evaluating expression. extracting variables expression, median, itr/sec bench::mark() output using select() function, can summarise results table. save typing, define function bench_table kable printing: 𝐀𝐁𝐛\\boldsymbol{}\\boldsymbol{B}\\boldsymbol{b}, order matrix&vector multiplication done important. Define new variables: Compare running times %*% B %*% b, (%*% B) %*% b, %*% (B %*% b)). three versions produce similar numerical results (bench::mark complain don’t) method fastest? ? computed results ? square matrix 𝐀\\boldsymbol{} size 1000×10001000\\times 1000, compare cost solve() %*% b (Get matrix inverse 𝐀−1\\boldsymbol{}^{-1}, multiply 𝐛\\boldsymbol{b}) cost solve(, b) (Solve linear system 𝐀𝐮=𝐛\\boldsymbol{}\\boldsymbol{u}=\\boldsymbol{b}).","code":"vec_norm <- function(x) {   sum(x^2)^0.5 } c(vec_norm(c(1, 2, 3)), sqrt(1 + 4 + 9)) ## [1] 3.741657 3.741657 n <- 100000 m <- 100 b <- rnorm(n) A <- matrix(rnorm(n * m), n, m) bench_table <- function(bm) {   knitr::kable(     bm %>%       select(expression, median, `itr/sec`)   ) } m <- 1000 n <- 1000 p <- 1000 A <- matrix(rnorm(m * n), m, n) B <- matrix(rnorm(n * p), n, p) b <- rnorm(p)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"numerical-least-squares-estimation","dir":"Articles","previous_headings":"","what":"Numerical Least Squares estimation","title":"Tutorial 08: Floating point computations and least squares","text":"’ll now investigate numerical issues numerical least squares estimation. Run following code generates synthetic observations linear model yi=xi−100.55+(xi−100.5)2+ei, \\begin{aligned} y_i &= \\frac{x_i-100.5}{5} + (x_i-100.5)^2 + e_i, \\end{aligned}  ei∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σe=0.1)e_i \\sim \\mathsf{Normal}(0, \\sigma_e=0.1), independent, =1,…,n=100i=1,\\dots,n=100. Let’s plot data stored data.frame: true values β0\\beta_0, β1\\beta_1, β2\\beta_2 standardised model formulation, yi=μi+ei=β0+β1xi+β2xi2+ei? \\begin{aligned} y_i &= \\mu_i + e_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i ? \\end{aligned}  Hint: Identify values matching terms expanding initial model definition . Create beta_true vector containing (β0,β1,β2)(\\beta_0, \\beta_1, \\beta_2): Use β\\beta-values plot quadratic true model predictor function function x∈[100,101]x\\[100,101], together observations, estimate provided lm(). Since observed xx values relatively dense, don’t necessarily need special plot sequence xx values, can reuse observed values, model predictions value can obtained fitted() method. lm() formula specification, use + (x^2) quadratic term. tells lm() first compute x^2, use result new covariate. () method can interpreted “use computed value instead possible special meanings expression”. Use model.matrix(mod1) function extract 𝐗\\boldsymbol{X} matrix vector formulation model, 𝐲=𝐗𝛃+𝐞\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{e}, store X1. (See ?model.matrix information). use direct normal equations solve shown Lecture 8, (𝐗⊤𝐗)−1𝐗⊤𝐲(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}, estimate β\\beta parameters. work?","code":"## Set the \"seed\" for the random number sequences, so we can ## reproduce exactly the same random numbers every time we run the code set.seed(1) ## Simulate the data # x: 100 random numbers between 100 and 101 data <- data.frame(x = 100 + sort(runif(100))) # y: random variation around a quadratic function: data <- data %>%   mutate(y = (x - 100.5) / 5 + (x - 100.5)^2 + rnorm(n(), sd = 0.1)) ggplot(data) + geom_point(aes(x, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"shifted-covariate","dir":"Articles","previous_headings":"","what":"Shifted covariate","title":"Tutorial 08: Floating point computations and least squares","text":"whole model shifted right, given x+1000x+1000 instead original xx-values? model expression still able represent quadratic expression different β\\beta values. Create new data set: Estimate model using lm() new data set. work? default lm() allow singular models, “fix” problem removing co-linear columns model matrix. can ask disallow singular models singular.ok argument. Define function cond_nr() taking matrix X input returning condition number, computed help svd (see Lecture 8 code example condition number). help model.matrix, examine condition numbers model matrices two cases previous tasks. lm() computes fit using QR decomposition approach, direct solution normal equations. second lm() fit bad? Plot second third columns model matrix X2 , use cor examine correlation (see ?cor). explain large condition number? Since linear model says simply expected value vector 𝖤(𝐲)\\mathsf{E}(\\boldsymbol{y}) lies space spanned columns 𝐗\\boldsymbol{X}, one possibility attempt arrive better conditioned 𝐗\\boldsymbol{X} linear rescaling /recombination columns. always equivalent linear re-parameterization. Try model matrix model causes lm() fail. particular, column (except intercept column) subtract column mean (e.g., try sweep() colMeans() functions, see example code ). divide column (except intercept column) standard deviation. Find condition number new model matrix. Fit model model matrix using something like mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) Produce plot confirms resulting fit sensible now. Note: data split estimation test sets, transformation must applied test data, using scaling parameters derived observation data. Therefore scale() function isn’t useful . Instead, explicitly computing storing transformation information necessary. Compute condition numbers X2 X3. Also compute correlation column 2 3 X3 (subtract 11 see close 11 ). subtracting rescaling help? alternative fix subtract mean x value original x vector defining quadratic model. Try see happens condition number column correlations model matrix. First, construct modified data: Different methods modifying inputs model definitions require different calculations compensating model parameters, figured case separately. common effect change interpretation β0\\beta_0 parameter.","code":"data2 <- data %>%   mutate(x = x + 1000) cor(X2[, 2:3]) ggplot() +   geom_point(aes(X2[, 2], X2[, 3])) +   ggtitle(paste(\"Correlation =\", cor(X2[,2], X2[,3]))) ## Very close to co-linear ## Data setup: data4 <- data2 %>%   mutate(x_shifted = x - mean(x))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"lab session explore Speed matrix computations Floating point accuracy least squares estimation statistical linear models Open github repository clone project Lab 2 4 upgrade StatCompLab package lab, can work .R file, working code chunks .Rmd highly recommended. measuring relative running times different methods, ’ll use bench package. won’t need load package library. Instead, check bench::mark() runs without errors. doesn’t, ’s package installed, install install.packages(\"bench\") R Console. Suggested code setup code chunk:","code":"suppressPackageStartupMessages(library(tidyverse)) library(StatCompLab)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"numerical-speed","dir":"Articles","previous_headings":"","what":"Numerical speed","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"Define following function computing vector norm: Try simple vector can verify result correct, e.g. now take look computational speed matrix operations. First construct vector 𝐛\\boldsymbol{b} matrix 𝐀\\boldsymbol{}: Let’s check speed difference 𝐀⊤𝐛\\boldsymbol{}^\\top\\boldsymbol{b} (𝐛⊤𝐀)⊤(\\boldsymbol{b}^\\top\\boldsymbol{})^\\top (convince two expressions equivalent exact arithmetic, possibly also floating point arithmetic). function t() construct transpose matrix. Note b vector (opposed single-column matrix) interpreted either column vector row vector, depending operation. b vector, .matrix(b) always single-column matrix, t(b) single-row matrix, b %*% interpreted t(b) %*% . method computing 𝐀⊤𝐛\\boldsymbol{}^\\top \\boldsymbol{b} faster? Use bench::mark() compare different ways evaluating expression. extracting variables expression, median, itr/sec bench::mark() output using select() function, can summarise results table. Solution: save typing, define function bench_table kable printing: Solution: t() %*% b method slowest implementation. Transposing matrix requires copying values new locations. Transposing vector doesn’t require copying. 𝐀𝐁𝐛\\boldsymbol{}\\boldsymbol{B}\\boldsymbol{b}, order matrix&vector multiplication done important. Define new variables: Compare running times %*% B %*% b, (%*% B) %*% b, %*% (B %*% b)). Solution: three versions produce similar numerical results (bench::mark complain don’t) method fastest? ? computed results ? Solution: One matrix-matrix multiplication expensive compared matrix-vector multiplication. results numerically close, one can likely express bounds terms .Machine$double.eps m,n,pm,n,p. square matrix 𝐀\\boldsymbol{} size 1000×10001000\\times 1000, compare cost solve() %*% b (Get matrix inverse 𝐀−1\\boldsymbol{}^{-1}, multiply 𝐛\\boldsymbol{b}) cost solve(, b) (Solve linear system 𝐀𝐮=𝐛\\boldsymbol{}\\boldsymbol{u}=\\boldsymbol{b}). Solution:","code":"vec_norm <- function(x) {   sum(x^2)^0.5 } c(vec_norm(c(1, 2, 3)), sqrt(1 + 4 + 9)) ## [1] 3.741657 3.741657 n <- 100000 m <- 100 b <- rnorm(n) A <- matrix(rnorm(n * m), n, m) bm <- bench::mark(   t(A) %*% b,   t(t(b) %*% A),   t(b %*% A) ) knitr::kable(bm %>% select(expression, median, `itr/sec`)) bench_table <- function(bm) {   knitr::kable(     bm %>%       select(expression, median, `itr/sec`)   ) } m <- 1000 n <- 1000 p <- 1000 A <- matrix(rnorm(m * n), m, n) B <- matrix(rnorm(n * p), n, p) b <- rnorm(p) bm <- bench::mark(   A %*% B %*% b,   (A %*% B) %*% b,   A %*% (B %*% b) ) bench_table(bm) bm <- bench::mark(   solve(A) %*% b,   solve(A, b),   check = FALSE # The results will differ by approximately 8e-13 ) bench_table(bm)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"numerical-least-squares-estimation","dir":"Articles","previous_headings":"","what":"Numerical Least Squares estimation","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"’ll now investigate numerical issues numerical least squares estimation. Run following code generates synthetic observations linear model yi=xi−100.55+(xi−100.5)2+ei, \\begin{aligned} y_i &= \\frac{x_i-100.5}{5} + (x_i-100.5)^2 + e_i, \\end{aligned}  ei∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σe=0.1)e_i \\sim \\mathsf{Normal}(0, \\sigma_e=0.1), independent, =1,…,n=100i=1,\\dots,n=100. Let’s plot data stored data.frame:  true values β0\\beta_0, β1\\beta_1, β2\\beta_2 standardised model formulation, yi=μi+ei=β0+β1xi+β2xi2+ei? \\begin{aligned} y_i &= \\mu_i + e_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i ? \\end{aligned}  Hint: Identify values matching terms expanding initial model definition . Solution: β0=−100.55+100.52=10080.15β1=15−201=−200.8β2=1 \\begin{aligned} \\beta_0 &= -\\frac{100.5}{5} + 100.5^2 = 10080.15 \\\\ \\beta_1 &= \\frac{1}{5} - 201 = -200.8 \\\\ \\beta_2 &= 1 \\end{aligned} Create beta_true vector containing (β0,β1,β2)(\\beta_0, \\beta_1, \\beta_2): Solution: Use β\\beta-values plot quadratic true model predictor function function x∈[100,101]x\\[100,101], together observations, estimate provided lm(). Since observed xx values relatively dense, don’t necessarily need special plot sequence xx values, can reuse observed values, model predictions value can obtained fitted() method. lm() formula specification, use + (x^2) quadratic term. tells lm() first compute x^2, use result new covariate. () method can interpreted “use computed value instead possible special meanings expression”. Solution:  Use model.matrix(mod1) function extract 𝐗\\boldsymbol{X} matrix vector formulation model, 𝐲=𝐗𝛃+𝐞\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{e}, store X1. (See ?model.matrix information). use direct normal equations solve shown Lecture 8, (𝐗⊤𝐗)−1𝐗⊤𝐲(\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}, estimate β\\beta parameters. work? Solution:","code":"## Set the \"seed\" for the random number sequences, so we can ## reproduce exactly the same random numbers every time we run the code set.seed(1) ## Simulate the data # x: 100 random numbers between 100 and 101 data <- data.frame(x = 100 + sort(runif(100))) # y: random variation around a quadratic function: data <- data %>%   mutate(y = (x - 100.5) / 5 + (x - 100.5)^2 + rnorm(n(), sd = 0.1)) ggplot(data) + geom_point(aes(x, y)) beta_true <- c(10080.15, -200.8, 1) # beta_0 = -100.5/5 + 100.5^2 = 10080.15 # beta_1 = 1/5 - 201 = -200.8 # beta_2 = 1 data <- data %>%   mutate(mu_true = beta_true[1] + beta_true[2] * x + beta_true[3] * x^2) pl <- ggplot(data) +   geom_point(aes(x, y)) +   geom_line(aes(x, mu_true, col =\"True\"))  ## lm manages to estimate the regression: mod1 <- lm(y ~ x + I(x^2), data = data) ## Add the fitted curve: pl +   geom_line(aes(x, fitted(mod1), col = \"Estimated\")) X1 <- model.matrix(mod1) # To allow the tutorial document to keep running even in case of errors, wrap in try() try(   beta.hat <- solve(t(X1) %*% X1, t(X1) %*% data$y) ) ## Error in solve.default(t(X1) %*% X1, t(X1) %*% data$y) :  ##   system is computationally singular: reciprocal condition number = 3.98647e-19"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"shifted-covariate","dir":"Articles","previous_headings":"","what":"Shifted covariate","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"whole model shifted right, given x+1000x+1000 instead original xx-values? model expression still able represent quadratic expression different β\\beta values. Create new data set: Estimate model using lm() new data set. work? Solution:  lm() fails fit correct curve, without warning! default lm() allow singular models, “fix” problem removing co-linear columns model matrix. can ask disallow singular models singular.ok argument. Solution: Define function cond_nr() taking matrix X input returning condition number, computed help svd (see Lecture 8 code example condition number). Solution: help model.matrix, examine condition numbers model matrices two cases previous tasks. lm() computes fit using QR decomposition approach, direct solution normal equations. second lm() fit bad? Solution: Plot second third columns model matrix X2 , use cor examine correlation (see ?cor). explain large condition number?  Solution: cases columns virtually co-linear, leads small d3d_3 value, therefore large condition number. Since linear model says simply expected value vector 𝖤(𝐲)\\mathsf{E}(\\boldsymbol{y}) lies space spanned columns 𝐗\\boldsymbol{X}, one possibility attempt arrive better conditioned 𝐗\\boldsymbol{X} linear rescaling /recombination columns. always equivalent linear re-parameterization. Try model matrix model causes lm() fail. particular, column (except intercept column) subtract column mean (e.g., try sweep() colMeans() functions, see example code ). divide column (except intercept column) standard deviation. Find condition number new model matrix. Fit model model matrix using something like mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) Produce plot confirms resulting fit sensible now. Solution:  Note: data split estimation test sets, transformation must applied test data, using scaling parameters derived observation data. Therefore scale() function isn’t useful . Instead, explicitly computing storing transformation information necessary. Compute condition numbers X2 X3. Also compute correlation column 2 3 X3 (subtract 11 see close 11 ). Solution: subtracting rescaling help? Solution: Yes, condition number significantly reduced. correlation columns however still close 1, meaning matrix still close singular. alternative fix subtract mean x value original x vector defining quadratic model. Try see happens condition number column correlations model matrix. First, construct modified data: Solution:  Different methods modifying inputs model definitions require different calculations compensating model parameters, figured case separately. common effect change interpretation β0\\beta_0 parameter.","code":"data2 <- data %>%   mutate(x = x + 1000) mod2 <- lm(y ~ x + I(x^2), data = data2) ggplot(data2) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod1), col = \"mod1\")) +   geom_line(aes(x, fitted(mod2), col = \"mod2\")) # lm() fails to fit the correct curve, without warning! try(   mod2b <- lm(y ~ x + I(x^2), data = data2, singular.ok = FALSE) ) ## Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :  ##   singular fit encountered # Now lm refuses to compute an estimate. cond_nr <- function(X) {   d <- svd(X)$d   max(d) / min(d) } cond_nr(X1) ## [1] 1560769713 # large, but somewhat manageable condition number  X2 <- model.matrix(mod2) cond_nr(X2) ## [1] 2.242481e+13 # very large condition number, # but not large enough to trigger a warning! cor(X2[, 2:3]) ##        x I(x^2) ## x      1      1 ## I(x^2) 1      1 ggplot() +   geom_point(aes(X2[, 2], X2[, 3])) +   ggtitle(paste(\"Correlation =\", cor(X2[,2], X2[,3]))) ## Very close to co-linear ## Solution code: mean_vec <- colMeans(X2[, 2:3]) sd_vec <- c(sd(X2[, 2]), sd(X2[, 3])) X3 <- cbind(   X2[, 1],   sweep(X2[, 2:3], 2, mean_vec, FUN = \"-\") ) X3[, 2:3] <-   sweep(X3[, 2:3], 2, sd_vec, FUN = \"/\") data3 <- data2 %>%   mutate(x1 = X3[, 1], x2 = X3[, 2], x3 = X3[, 3])  mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) ggplot(data3) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod3))) c(cond_nr(X2), cond_nr(X3)) ## [1] 2.242481e+13 1.791760e+04 c(cor(X2[, 2], X2[, 3]) - 1,   cor(X3[, 2], X3[, 3]) - 1) ## [1] -6.229745e-09 -6.229745e-09 ## Data setup: data4 <- data2 %>%   mutate(x_shifted = x - mean(x)) mod4 <- lm(y ~ x_shifted + I(x_shifted^2), data = data4) ggplot(data4) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod4))) X4 <- model.matrix(mod4) cond_nr(X4) ## [1] 15.36915 cor(X4[, 2], X4[, 3]) ## [1] -0.09094755"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"lab session explore Exchangeability test prediction scores Pareto smoothed importance sampling Open one github lab repository clone projects lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.9.0 higher. Solution: accompanying Tutorial09Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"leave-one-out-prediction","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Leave-one-out prediction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"Revisit Tutorial 6 load 3D printer filament data: tutorial 6, two models B estimated Predictions scores done well 50% estimation/prediction data split. Implement function leave1out(data, model) performs leave-one-cross validation selected model. two models; =1,…,Ni=1,\\dots,N, estimate model parameters using {(xj,yj),j≠}\\{(x_j,y_j), j\\neq \\}, compute prediction information based xix_i prediction model FiF_i, compute required scores S(Fi,yi)S(F_i,y_i). output data.frame NN rows extends original data frame four additional columns mean, sd, se ds leave-one-prediction means, standard deviations, prediction scores Squared Error Dawid-Sebastiani scores. following code work code correct:","code":"data(\"filament1\", package = \"StatCompLab\") pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_A <- leave1out(filament1, model = \"A\") score_B <- leave1out(filament1, model = \"B\") ggplot() +   geom_point(aes(CAD_Weight, se, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, se, colour = \"B\"), data = score_B) ggplot() +   geom_point(aes(CAD_Weight, ds, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, ds, colour = \"B\"), data = score_B)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"exchangeability-test","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Exchangeability test","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"want investigate whether one model better predicting . equivalent, matter, average, randomly swap B prediction scores within leave-one-score pair (SiA,SiB)(S^A_i, S^B_i). Use test statistic 1N∑=1N(SiA−SiB)\\frac{1}{N}\\sum_{=1}^N (S^A_i - S^B_i), make Monte Carlo estimate p-value test exchangeability model predictions B alternative hypothesis B better . First compute test statistic two prediction scores. Hints: particular test statistic, one possible approach first compute pairwise score differences generate randomisation samples sampling random sign changes. Compute test statistic randomly altered set values compare original test statistic. See lecture exchangeability information. Start J=1000J=1000 iterations testing code. Increase 10000 precise results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"pareto-smoothed-importance-sampling","dir":"Articles","previous_headings":"","what":"Pareto smoothed importance sampling","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"Explore Pareto smoothed importance sampling (PSIS) method. Use t-distribution 10 degrees freedom, importance sampling distribution x∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2)x\\sim \\mathsf{Normal}(0, \\sigma^2) different values σ\\sigma. Use N=100N=100 samples. Use loo::psis() function compute Pareto smoothed importance log-weights diagnostics. effect different values σ\\sigma kk parameter estimate? (k̂\\hat{k} value $diagnostics$pareto_k field loo::psis() output.) Plot original smoothed log-weights function xx, stat_ecdf, different values σ\\sigma, including σ=1\\sigma=1 σ=2\\sigma=2.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"lab session explore Exchangeability test prediction scores Pareto smoothed importance sampling Open one github lab repository clone projects lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.9.0 higher.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"leave-one-out-prediction","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Leave-one-out prediction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"Revisit Tutorial 6 load 3D printer filament data: tutorial 6, two models B estimated Predictions scores done well 50% estimation/prediction data split. Implement function leave1out(data, model) performs leave-one-cross validation selected model. two models; =1,…,Ni=1,\\dots,N, estimate model parameters using {(xj,yj),j≠}\\{(x_j,y_j), j\\neq \\}, compute prediction information based xix_i prediction model FiF_i, compute required scores S(Fi,yi)S(F_i,y_i). output data.frame NN rows extends original data frame four additional columns mean, sd, se ds leave-one-prediction means, standard deviations, prediction scores Squared Error Dawid-Sebastiani scores. Solution: following code work code correct:","code":"data(\"filament1\", package = \"StatCompLab\") pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) leave1out <- function(data, model) {   data <- data %>% mutate(mean = NA_real_, sd = NA_real_)   for (i in seq_len(nrow(data))) {     fit <- filament1_estimate(data[-i, , drop = FALSE], model)     pred <- filament1_predict(fit, newdata = data[i, , drop = FALSE])     data[i, \"mean\"] <- pred$mean     data[i, \"sd\"] <- pred$sd   }   data <- data %>%     mutate(       se = proper_score(\"se\", Actual_Weight, mean = mean),       ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd)     )   data } score_A <- leave1out(filament1, model = \"A\") score_B <- leave1out(filament1, model = \"B\") ggplot() +   geom_point(aes(CAD_Weight, se, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, se, colour = \"B\"), data = score_B) ggplot() +   geom_point(aes(CAD_Weight, ds, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, ds, colour = \"B\"), data = score_B)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"exchangeability-test","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Exchangeability test","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"want investigate whether one model better predicting . equivalent, matter, average, randomly swap B prediction scores within leave-one-score pair (SiA,SiB)(S^A_i, S^B_i). Use test statistic 1N∑=1N(SiA−SiB)\\frac{1}{N}\\sum_{=1}^N (S^A_i - S^B_i), make Monte Carlo estimate p-value test exchangeability model predictions B alternative hypothesis B better . First compute test statistic two prediction scores. Hints: particular test statistic, one possible approach first compute pairwise score differences generate randomisation samples sampling random sign changes. Compute test statistic randomly altered set values compare original test statistic. See lecture exchangeability information. Start J=1000J=1000 iterations testing code. Increase 10000 precise results. Solution: see Square Error scores, two model predictions appear exchangeable, prediction uncertainty taken account Dawid-Sebastiani score, B appears better , average.","code":"score_diff <- data.frame(se = score_A$se - score_B$se,                          ds = score_A$ds - score_B$ds) statistic0 <- score_diff %>% summarise(se = mean(se), ds = mean(ds)) J <- 10000 statistic <- data.frame(se = numeric(J),                         ds = numeric(J)) for (loop in seq_len(J)) {   random_sign <- sample(c(-1, 1), size = nrow(score_diff), replace = TRUE)   statistic[loop, ] <- score_diff %>% summarise(se = mean(random_sign * se),                                                 ds = mean(random_sign * ds)) } p_values <-   statistic %>%   summarise(se = mean(se > statistic0$se),             ds = mean(ds > statistic0$ds)) # Estimates: p_values ##       se     ds ## 1 0.5011 0.0442 # Monte Carlo std.error:: sqrt(p_values * (1 - p_values) / J) ##            se          ds ## 1 0.004999988 0.002055392"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"pareto-smoothed-importance-sampling","dir":"Articles","previous_headings":"","what":"Pareto smoothed importance sampling","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"Explore Pareto smoothed importance sampling (PSIS) method. Use t-distribution 10 degrees freedom, importance sampling distribution x∼𝖭𝗈𝗋𝗆𝖺𝗅(0,σ2)x\\sim \\mathsf{Normal}(0, \\sigma^2) different values σ\\sigma. Use N=100N=100 samples. Use loo::psis() function compute Pareto smoothed importance log-weights diagnostics. effect different values σ\\sigma kk parameter estimate? (k̂\\hat{k} value $diagnostics$pareto_k field loo::psis() output.) Plot original smoothed log-weights function xx, stat_ecdf, different values σ\\sigma, including σ=1\\sigma=1 σ=2\\sigma=2. Solution:","code":"sigma <- 1 x <- rnorm(100, sd = sigma) log_ratio <- dt(x, df = 10, log = TRUE) - dnorm(x, sd = sigma, log = TRUE) result <- loo::psis(log_ratio, r_eff = 1) ## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. result$diagnostics$pareto_k ## [1] 2.692393 result$diagnostics$n_eff ## [1] 98.14698 df <- data.frame(x = x,                  log_ratio = log_ratio,                  log_weight = result$log_weights) ggplot(df) +   geom_point(aes(x, log_ratio, color = \"Original\")) +   geom_point(aes(x, log_weight, color = \"Smoothed\")) ggplot(df) +   stat_ecdf(aes(log_ratio, color = \"Original\")) +   stat_ecdf(aes(log_weight, color = \"Smoothed\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Finn Lindgren. Author, maintainer.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lindgren F (2025). StatCompLab: Workshop Material UoE Statistical Computing. R package version 25.4.9, https://github.com/finnlindgren/StatCompLab, https://finnlindgren.github.io/StatCompLab.","code":"@Manual{,   title = {StatCompLab: Workshop Material For UoE Statistical Computing},   author = {Finn Lindgren},   year = {2025},   note = {R package version 25.4.9,     https://github.com/finnlindgren/StatCompLab},   url = {https://finnlindgren.github.io/StatCompLab}, }"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"statcomplab","dir":"","previous_headings":"","what":"Workshop Material For UoE Statistical Computing","title":"Workshop Material For UoE Statistical Computing","text":"package collects workshop materials Statistical Computing (MATH10093) University Edinburgh 2021/22. tutorial documents browsable online https://finnlindgren.github.io/StatCompLab/ Contact Finn Lindgren, finn.lindgren@ed.ac.uk information. package version number system YearOfStudy.StudyWeek.MinorUpdate","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Workshop Material For UoE Statistical Computing","text":"can install (later upgrade) package GitHub : want view vignette versions tutorials within RStudio, need add build_vignettes argument:","code":"# If devtools isn't installed, first run install.packages(\"remotes\") remotes::install_github(\"finnlindgren/StatCompLab\") remotes::install_github(\"finnlindgren/StatCompLab\", build_vignettes = TRUE)"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Workshop Material For UoE Statistical Computing","text":"convenient way access tutorial documents run Tutorial tab upper right part RStudio. allows see tutorial text code hints code files, running code Console viewing plots. alternative methods also allows viewing documents, less convenient; vignettes show space plots, run_tutorial blocks Console window used run code.","code":"library(StatCompLab) # To install the vignette versions, add build_vignettes=TRUE to the install_github() call above. vignette(package = \"StatCompLab\") # List available vignettes vignette(\"Tutorial01\", \"StatCompLab\") # View a specific vignette # The following method blocks the Console from running other code; better to use the Tutorials pane instead library(learnr) available_tutorials(\"StatCompLab\") # List available tutorials run_tutorial(\"Tutorial01\", \"StatCompLab\") # Run a specific tutorial"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/StatEwcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"StatEwcdf ggproto object — StatEwcdf","title":"StatEwcdf ggproto object — StatEwcdf","text":"StatEwcdf ggproto object","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple binomial model likelihood — arch_loglike","title":"Multiple binomial model likelihood — arch_loglike","text":"Compute log-likelihood multiple Binom(N,phi) observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(param, y)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple binomial model likelihood — arch_loglike","text":"param Either vector two elements, N phi, data.frame two columns, named N phi y data vector","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple binomial model likelihood — arch_loglike","text":"log-likelihood N,phi combination. implementation internally uses log-gamma function provide differentiable function N, allows optim() treat N continuous variable. N < max(y), log-likelihood defined -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(   param = data.frame(N = c(5, 10, 20, 40),                      phi = c(0.1, 0.2, 0.3, 0.4)),   y = c(2, 4)) #> [1] -10.324930  -3.626867  -5.618058 -25.216655"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate filament models — filament1_estimate","title":"Estimate filament models — filament1_estimate","text":"Estimate filament models different variance structure","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate filament models — filament1_estimate","text":"","code":"filament1_estimate(data, model)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate filament models — filament1_estimate","text":"data data.frame variables filament1 data set. Must columns CAD_Weight Actual_Weight model Either \"\" log-linear variance model, \"B\" proportional scaling error model","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate filament models — filament1_estimate","text":"estimation object suitable use filament1_predict()","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate filament models — filament1_estimate","text":"","code":"if (FALSE) { # \\dontrun{ if(interactive()){  #EXAMPLE1  } } # }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict values for a filament model — filament1_predict","title":"Predict values for a filament model — filament1_predict","text":"Uses estimated filament model predict new observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict values for a filament model — filament1_predict","text":"","code":"filament1_predict(object, newdata, alpha = 0.05, df = Inf, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict values for a filament model — filament1_predict","text":"object estimation object, like output filament1_estimate() newdata data use prediction. Must column CAD_Weight alpha target coverage error prediction intervals, Default: 0.05 df degrees freedom t-quantiles prediction interval construction, Default: Inf ... Additional arguments, currently ignored","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict values for a filament model — filament1_predict","text":"data.frame variables mean, sd, lwr, upr, summarising prediction distribution row newdata.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict values for a filament model — filament1_predict","text":"","code":"if (FALSE) { # \\dontrun{ if(interactive()){  #EXAMPLE1  } } # }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive numerical optimisation explorer — optimisation","title":"Interactive numerical optimisation explorer — optimisation","text":"Shiny app exploring several numerical optimisation methods work","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive numerical optimisation explorer — optimisation","text":"","code":"optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate proper scores — proper_score","title":"Calculate proper scores — proper_score","text":"Calculates proper scores probabilistic predictions","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate proper scores — proper_score","text":"","code":"proper_score(type, obs, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate proper scores — proper_score","text":"type string indicating type score calculate. One \"se\", \"ae\", \"ds\", \"interval\". See Details . obs vector observations ... Additional named arguments needed type score, see Details .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate proper scores — proper_score","text":"vector calculated scores, one observation","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate proper scores — proper_score","text":"score type takes additional arguments define prediction model information used score calculations. Unless specified otherwise, extra arguments either scalars (applying predictions) vectors length obs vector. se Squared error score. Requires mean, defining prediction mean ae Absolute error score. Requires median, defining prediction median ds Dawid-Sebastiani score. Requires mean sd, defining mean standard deviation predictions interval Squared error score. Requires lwr upr, defining lower upper prediction interval endpoints, alpha, defining prediction error probability targeted prediction intervals scores \\(S_{type}(F_i,y_i)\\) defined lecture notes, obs[] equal \\(y_i\\) prediction distribution \\(F_i\\) defined additional named arguments.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate proper scores — proper_score","text":"","code":"# Five realisations of N(3, 3^2) obs <- rnorm(5, mean = 3, sd = 3)  # One prediction for each observation. Only one of the predictions describes # the true model F_mean <- 1:5 F_median <- 1:5 F_sd <- 1:5 F_lwr <- F_mean - F_sd * qnorm(0.9) F_upr <- F_mean - F_sd * qnorm(0.1)  # Compute the scores data.frame(   se = proper_score(\"se\", obs, mean = F_mean),   ae = proper_score(\"ae\", obs, median = F_median),   ds = proper_score(\"ds\", obs, mean = F_mean, sd = F_sd),   interval = proper_score(\"interval\", obs,                           lwr = F_lwr, upr = F_upr, alpha = 0.2) ) #>            se        ae       ds  interval #> 1  4.84057444 2.2001306 4.840574 11.748893 #> 2  3.11858352 1.7659512 2.165940  5.126206 #> 3 53.46228520 7.3117908 8.137478 42.360671 #> 4  1.03370707 1.0167139 2.837195 10.252413 #> 5  0.01831741 0.1353418 3.219609 12.815516"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute empirical weighted cumulative distribution — stat_ewcdf","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"Version ggplot2::stat_ecdf adds weights property observation, produce empirical weighted cumulative distribution function. empirical cumulative distribution function (ECDF) provides alternative visualisation distribution. Compared visualisations rely density (like geom_histogram()), ECDF require tuning parameters handles continuous discrete variables. downside requires training accurately interpret, underlying visual tasks somewhat challenging.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"stat_ewcdf(   mapping = NULL,   data = NULL,   geom = \"step\",   position = \"identity\",   ...,   n = NULL,   pad = TRUE,   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"n NULL, interpolate. NULL, number points interpolate . pad TRUE, pad ecdf additional points (-Inf, 0) (Inf, 1) na.rm FALSE (default), removes missing values warning.  TRUE silently removes missing values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"computed-variables","dir":"Reference","previous_headings":"","what":"Computed variables","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"x x data y cumulative density corresponding x","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"library(ggplot2)  n <- 100 df <- data.frame(   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),   g = gl(2, n),   w = c(rep(1/n, n), sort(runif(n))^sqrt(n)) ) ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\")   # Don't go to positive/negative infinity ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE)   # Multiple ECDFs ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf()  ggplot(df, aes(x, colour = g, weights = w)) +   stat_ewcdf() +   facet_wrap(vars(g), ncol = 1)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted sample quantiles — wquantile","title":"Weighted sample quantiles — wquantile","text":"Calculates empirical sample quantiles optional weights, given probabilities. Like quantile(), smallest observation corresponds probability 0 largest probability 1. Interpolation discrete values done type=7, quantile(). Use type=1 generate quantile values raw input samples.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted sample quantiles — wquantile","text":"","code":"wquantile(   x,   probs = seq(0, 1, 0.25),   na.rm = FALSE,   type = 7,   weights = NULL,   ... )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted sample quantiles — wquantile","text":"x numeric vector whose sample quantiles wanted. NA NaN values allowed numeric vectors unless na.rm TRUE. probs numeric vector probabilities values \\([0,1]\\). na.rm logical; true, NA NaN's removed x quantiles computed. type numeric, 1 interpolation, 7, interpolated quantiles. Default 7. weights numeric vector non-negative weights, length x, NULL. weights normalised sum 1. NULL, wquantile(x) behaves quantile(x), equal weight sample value. ... Additional arguments, currently ignored","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted sample quantiles — wquantile","text":"","code":"# Some random numbers x <- rnorm(100)  # Plain quantiles: quantile(x) #>         0%        25%        50%        75%       100%  #> -2.1775760 -0.4950209  0.1309871  0.8588734  2.1587566   # Larger values given larger weight, on average shifting the quantiles upward: wquantile(x, weights = sort(runif(length(x)))) #> [1] -2.1775760 -0.4175936  0.2000932  1.0427150  2.1587566"}]
