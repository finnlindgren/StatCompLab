[{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-problem","dir":"Articles","previous_headings":"Avoiding long running times","what":"The problem","title":"StatComp Project 1 (2021/22): Hints","text":"Collecting values data.frame inside loop can slow. common issue combining results computed loops using rbind step loop add one rows large vector data.frame can slow; instead linear computational cost size output, cost can become quadratic; needs reallocate memory copy previous results, new version result object.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-solution","dir":"Articles","previous_headings":"Avoiding long running times","what":"The solution","title":"StatComp Project 1 (2021/22): Hints","text":"key avoid reallocating memory instead pre-allocating whole result object loop. However subtleties , examples blow show differences approaches. common solution work simpler intermediate data structures within loop, collect results end. Indexing simple vectors much cheaper reallocating copying memory. Slow: Better, even data.frame indexing assignments causes memory reallocation: Fast; several orders magnitude faster (see timings ) avoiding memory reallocation inside loop: fast, bit compact; using matrix indexing doesn’t need memory reallocation, bit slower vector indexing. potential drawback method works columns output data.frame type (numeric), cases works code simple, readable, generally quick run: Benchmark timing comparisons:","code":"slow_fun <- function(N) {   result <- data.frame()   for (loop in seq_len(N)) {     result <- rbind(       result,       data.frame(A = cos(loop), B = sin(loop))     )   }   result } better_fun <- function(N) {   result <- data.frame(A = numeric(N), B = numeric(N))   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   result } fast_fun <- function(N) {   result_A <- numeric(N)   result_B <- numeric(N)   for (loop in seq_len(N)) {     result_A[loop] <- cos(loop)     result_B[loop] <- sin(loop)   }   data.frame(     A = result_A,     B = result_B   ) } ok_fun <- function(N) {   result <- matrix(0, N, 2)   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   colnames(result) <- c(\"A\", \"B\")   as.data.frame(result) } N <- 10000 bench::mark(   slow = slow_fun(N),   better = better_fun(N),   fast = fast_fun(N),   ok = ok_fun(N) ) #> Warning: Some expressions had a GC in every iteration; so filtering is #> disabled. #> # A tibble: 4 × 6 #>   expression      min   median `itr/sec` mem_alloc `gc/sec` #>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #> 1 slow          6.01s    6.01s     0.166    2.28GB     9.15 #> 2 better        1.12s    1.12s     0.893    1.49GB    30.4  #> 3 fast         2.02ms   2.21ms   435.     229.67KB     2.00 #> 4 ok           6.29ms   7.46ms   125.     458.21KB     9.91"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"dynamically-generated-column-names","dir":"Articles","previous_headings":"","what":"Dynamically generated column names","title":"StatComp Project 1 (2021/22): Hints","text":"size matrix data.frame depends parameter, multiple columns similar names, can convenient dynamically generate column names. paste() function can used . example, want data.frame size \\(n\\times m\\), column names Name1, Name2, etc, can use resulting object looks like :","code":"df <- matrix(runif(15), 5, 3) # Create a matrix with some values in it colnames(df) <- paste(\"Name\", seq_len(ncol(df)), sep = \"\") df <- as.data.frame(df) df #>       Name1     Name2     Name3 #> 1 0.5456319 0.1015897 0.1310044 #> 2 0.1270139 0.8729465 0.9193526 #> 3 0.9775196 0.5058848 0.8161976 #> 4 0.3839627 0.2795140 0.7087566 #> 5 0.4731387 0.2145316 0.7542970"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"multi-column-summarise-constructions","dir":"Articles","previous_headings":"","what":"Multi-column summarise constructions","title":"StatComp Project 1 (2021/22): Hints","text":"summarise() method dplyr package usually used construct simple summaries function computes mu sigma? make return data.frame, can used summarise:","code":"suppressPackageStartupMessages(library(tidyverse)) df <- data.frame(x = rnorm(100)) df %>%   summarise(     mu = mean(x),     sigma = sd(x)   ) #>          mu   sigma #> 1 0.0248793 1.04352 my_fun <- function(x) {   data.frame(     mu = mean(x),     sigma = sd(x)   ) } df %>%   summarise(     my_fun(x)   ) #>          mu   sigma #> 1 0.0248793 1.04352"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"avoiding-unnecessary-for-loops","dir":"Articles","previous_headings":"","what":"Avoiding unnecessary for-loops","title":"StatComp Project 1 (2021/22): Hints","text":"Since operations functions R vectorised, code can often written clear way avoiding unnecessary -loops. simple example, say want compute \\(\\sum_{k=1}^{100} \\cos(k)\\). -loop solution style good solution high performance low-level language like C might look like : taking advantage vectorised calling cos(), function sum(), can reformulate shorter clear vectorised solution: commonly used functions involving vectors TRUE FALSE () (). example, sum, , also operate across elements matrix. row-wise column-wise sums, see rowSums()colSums().","code":"result <- 0 for (k in seq_len(100)) {   result <- result + cos(k) } result <- sum(cos(seq_len(100))) vec <- c(TRUE, TRUE, FALSE, TRUE) all(vec) #> [1] FALSE any(vec) #> [1] TRUE sum(vec) # FALSE == 0, TRUE == 1 #> [1] 3"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"aligning-equations-in-multi-step-derivations","dir":"Articles","previous_headings":"LaTeX equations","what":"Aligning equations in multi-step derivations","title":"StatComp Project 1 (2021/22): Hints","text":"typesetting multi-step derivations, one align equations. LaTeX, can done align align* environments. case, might work RMarkdown, one can use aligned environment instead.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-1","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 1","title":"StatComp Project 1 (2021/22): Hints","text":"Result: \\[\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}\\]","code":"\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-2","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 2","title":"StatComp Project 1 (2021/22): Hints","text":"Result: \\[ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} \\]","code":"$$ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} $$"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation.","code":"newdata <- data.frame(z = -10:20)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models","text":"can now use something like following code plot results: Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models (solutions)","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models (solutions)","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models (solutions)","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models (solutions)","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models (solutions)","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models (solutions)","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation. Solution:","code":"newdata <- data.frame(z = -10:20) data_pred <- cbind(newdata,                    data.frame(fit = predict(mod, newdata))) ggplot(data) +   geom_point(aes(z, y)) +   geom_line(aes(z, fit), data = data_pred) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models (solutions)","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Solution: Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"pred <- cbind(   newdata,   predict(mod, newdata, interval = \"prediction\") ) plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: Solution: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions <- function(x, newdata, xname = \"x\") {   pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   ggplot(pred) +     geom_line(aes_string(xname, \"fit\")) +     geom_ribbon(aes_string(x = xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25) } plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models (solutions)","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot. Solution:","code":"# Extended version of the solution: plot_prediction <- function(x, newdata, xname = \"x\",                             xlab = NULL, ylab = NULL, ...) {   if (is.null(xlab)) {     xlab <- xname   }   if (is.null(ylab)) {     ylab <- \"Response\"   }    pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   pl <- ggplot() +     geom_line(data = pred,               aes_string(xname, \"fit\")) +     geom_ribbon(data = pred,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Also add the confidence intervals for the predictor curve   conf <- cbind(newdata,                 predict(x, newdata, interval = \"confidence\"))   pl <- pl +     geom_ribbon(data = conf,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Add lables:   pl + xlab(xlab) + ylab(ylab) }"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models (solutions)","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data) plot_prediction(mod2, newdata, xname = \"z\", ylab = \"y\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") +   ggtitle(\"Confidence and prediction intervals\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods. Solution:","code":"formulas <- c(y ~ 1,               y ~ z,               y ~ z + I(z^2),               y ~ z + I(z^2) + I(z^3)) mods <- lapply(formulas, function(x) lm(x, data))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models (solutions)","text":"can now use something like following code plot results:  Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization","text":"example, ’ll look model connection values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Check ?optim help text information result object contains. optimisation converge? Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals.","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1)"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization (solutions)","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization (solutions)","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization (solutions)","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization (solutions)","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization (solutions)","text":"example, ’ll look model connection values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. Solution: aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Solution: Check ?optim help text information result object contains. optimisation converge? Solution: Answer: optimisation converged opt$convergence 0. least optim() thinks coverged, since triggered ’s convergence tolerances. Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) neg_log_lik <- function(theta, y, X) {   -sum(dnorm(x = y,              mean = X %*% theta[1:2],              sd = exp(X %*% theta[3:4]),              log = TRUE)) } opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik,              y = y, X = X,              method = \"BFGS\") data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization (solutions)","text":"help ggplot produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals. Solution:","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1) opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik, y = y, X = X,              method = \"BFGS\",              hessian = TRUE) covar <- solve(opt$hessian) covar ##               [,1]         [,2]          [,3]         [,4] ## [1,]  0.0017940969 -0.005724814 -0.0005605767  0.001121155 ## [2,] -0.0057248136  0.036111726  0.0035360846 -0.007072178 ## [3,] -0.0005605767  0.003536085  0.0190674071 -0.028134823 ## [4,]  0.0011211550 -0.007072178 -0.0281348225  0.056269649"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks. Solution: accompanying Tutorial03Solutions tutorial document contains solutions explicitly, make easier review material workshops. can also run document Tutorials pane RStudio.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms ","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~} + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Solution: \\[ \\begin{aligned} \\mathsf{Var}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{Var}(Y|X)] + \\mathsf{Var}[\\mathsf{E}(Y|X)] \\\\&= \\mathsf{E}[\\lambda(\\mu,X)] + \\mathsf{Var}[\\lambda(\\mu,X)] \\\\&= \\mathsf{E}[\\exp(\\mu+X)] +\\mathsf{Var}[\\exp(\\mu+X)] \\\\&= \\exp(\\mu + \\sigma^2/2) + (\\exp(\\sigma^2)-1) \\exp(2\\mu + \\sigma^2) \\\\&= \\exp(\\mu + \\sigma^2/2) \\left[ 1 + (\\exp(\\sigma^2)-1) \\exp(\\mu + \\sigma^2/2) \\right] \\end{aligned} \\] Since second factor always \\(\\geq 1\\), shows variance never smaller expectation. Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Solution:  plot, overdispersed Poisson probability function (just computed) compared plain Poisson probability function expectation value. Lines values included clarity. Note: case, second approach faster, situations first approach required, since doesn’t require storing random numbers time, thus allowing much larger \\(K=n_{\\text{mc}}\\) value used. Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Solution: Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms Solution:","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) # Vectorised over m mu <- log(2) - 1/2 K <- 10000 m <- 0:15 P_Y <- numeric(length(m)) for (loop in seq_len(K)) {   x <- rnorm(1, sd = 1)   P_Y <- P_Y + dpois(m, lambda = exp(mu + x)) } P_Y <- P_Y / K  # Plot the results suppressPackageStartupMessages(library(ggplot2)) ggplot(data.frame(m = m,                   P_Y = P_Y,                   P_Poisson = dpois(m, lambda = 2))) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\")) doverpois <- function(m, mu, sigma, K) {   P_Y <- numeric(length(m))   for (loop in seq_len(K)) {     x <- rnorm(1, sd = sigma)     P_Y <- P_Y + dpois(m, lambda = exp(mu + x))   }   P_Y <- P_Y / K    data.frame(m = m,              P_Y = P_Y,              P_Poisson = dpois(m, lambda = exp(mu + sigma^2/2))) } ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000)) suppressPackageStartupMessages(library(ggplot2)) ggplot(doverpois(m = 0:30, mu = log(8)-0.125, sigma = 0.5, K = 10000)) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~} + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Solution: Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":"estimate <- function(y, xi, a, b, K) {   samples <- data.frame(     N = rgeom(K, prob = xi),     phi = rbeta(K, shape1 = a, shape2 = b)   )   loglike <- arch_loglike(param = samples, y = y)   p_y <- mean(exp(loglike))   c(p_y = p_y,     E_N = mean(samples$N * exp(loglike) / p_y),     E_phi = mean(samples$phi * exp(loglike) / p_y)) } estimate(y = c(237, 256), xi = 1/1001, a = 0.5, b = 0.5, K = 10000) ##          p_y          E_N        E_phi  ## 6.360064e-06 9.884083e+02 3.694532e-01"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk. Solution: accompanying Tutorial04Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. separate T4sol.Rmd document https://github.com/finnlindgren/StatCompLab/blob/main/vignettes/articles/T4sol.Rmd source document standalone solution shown T4sol StatCompLab website.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration","text":"Consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] week 4 lecture, two parameterisations considered. now add third option: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) week 4 lecture, know inverse expected Fisher information \\(\\lambda/n\\) case 1 \\(1/(4n)\\) case 2. case 3, show inverse expected Fisher information \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration","text":"Use approximation method large \\(n\\) lecture construct approximate confidence intervals \\(\\lambda\\) using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values \\(\\overline{y}\\). Experiment different values n lambda simulation y. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; \\(\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha\\) \\(\\theta\\) (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration","text":"Assume true value \\(\\lambda=10\\), simulate sample \\(\\boldsymbol{y}\\) size \\(n=5\\). Now consider Bayesian version Poisson model, prior model \\[ \\lambda \\sim \\mathsf{Exp}() \\] probability density function \\(p(\\lambda) = \\exp(-\\lambda)\\). One can show exact posterior distribution \\(\\lambda\\) given \\(\\boldsymbol{y}\\) \\(\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n)\\) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let \\(\\theta=\\log(\\lambda)\\), \\(\\lambda=\\exp(\\theta)\\). Show prior probability density \\(\\theta\\) \\(p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration","text":"posterior density function \\(\\theta\\) \\[ p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})} \\] log-density \\[ \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) , \\] taking derivatives find mode \\(\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right)\\), negated Hessian \\(1+n\\overline{y}\\) mode. information can construct Gaussian approximation posterior distribution, \\(\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}})\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration","text":"Simulate sample \\(\\boldsymbol{x}=\\{x_1,\\dots,x_m\\}\\) Gaussian approximation posterior distribution, large \\(m > 10000\\), hyperparameter \\(=1/5\\). need calculate unnormalised importance weights \\(w_k\\), \\(k=1,\\dots,m\\), \\[ w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} . \\] Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, \\(\\log(w_k)\\), new, equivalent unnormalised weights \\(\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]\\). Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval \\(\\theta\\) using \\(\\boldsymbol{x}\\) sample associate weights, transform credible interval \\(\\lambda\\)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function \\(\\lambda\\) (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample \\(\\lambda_k=\\exp(x_k)\\), (unnormalised) weights \\(w_k\\). Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Finn Lindgren. Author, maintainer.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lindgren F (2023). StatCompLab: Workshop Material UoE Statistical Computing. https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab.","code":"@Manual{,   title = {StatCompLab: Workshop Material For UoE Statistical Computing},   author = {Finn Lindgren},   year = {2023},   note = {https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab}, }"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"statcomplab","dir":"","previous_headings":"","what":"Workshop Material For UoE Statistical Computing","title":"Workshop Material For UoE Statistical Computing","text":"package collects workshop materials Statistical Computing (MATH10093) University Edinburgh 2021/22. tutorial documents browsable online https://finnlindgren.github.io/StatCompLab/ Contact Finn Lindgren, finn.lindgren@ed.ac.uk information. package version number system YearOfStudy.StudyWeek.MinorUpdate","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Workshop Material For UoE Statistical Computing","text":"can install (later upgrade) package GitHub : want view vignette versions tutorials within RStudio, need add build_vignettes argument:","code":"# If devtools isn't installed, first run install.packages(\"remotes\") remotes::install_github(\"finnlindgren/StatCompLab\") remotes::install_github(\"finnlindgren/StatCompLab\", build_vignettes = TRUE)"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Workshop Material For UoE Statistical Computing","text":"convenient way access tutorial documents run Tutorial tab upper right part RStudio. allows see tutorial text code hints code files, running code Console viewing plots. alternative methods also allows viewing documents, less convenient; vignettes show space plots, run_tutorial blocks Console window used run code.","code":"library(StatCompLab) # To install the vignette versions, add build_vignettes=TRUE to the install_github() call above. vignette(package = \"StatCompLab\") # List available vignettes vignette(\"Tutorial01\", \"StatCompLab\") # View a specific vignette # The following method blocks the Console from running other code; better to use the Tutorials pane instead library(learnr) available_tutorials(\"StatCompLab\") # List available tutorials run_tutorial(\"Tutorial01\", \"StatCompLab\") # Run a specific tutorial"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/StatEwcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"StatEwcdf ggproto object — StatEwcdf","title":"StatEwcdf ggproto object — StatEwcdf","text":"StatEwcdf ggproto object","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple binomial model likelihood — arch_loglike","title":"Multiple binomial model likelihood — arch_loglike","text":"Compute log-likelihood multiple Binom(N,phi) observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(param, y)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple binomial model likelihood — arch_loglike","text":"param Either vector two elements, N phi, data.frame two columns, named N phi y data vector","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple binomial model likelihood — arch_loglike","text":"log-likelihood N,phi combination. implementation internally uses log-gamma function provide differentiable function N, allows optim() treat N continuous variable. N < max(y), log-likelihood defined -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(   param = data.frame(N = c(5, 10, 20, 40),                      phi = c(0.1, 0.2, 0.3, 0.4)),   y = c(2, 4)) #> [1] -10.324930  -3.626867  -5.618058 -25.216655"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate filament models — filament1_estimate","title":"Estimate filament models — filament1_estimate","text":"Estimate filament models different variance structure","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate filament models — filament1_estimate","text":"","code":"filament1_estimate(data, model)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate filament models — filament1_estimate","text":"data data.frame variables filament1 data set. Must columns CAD_Weight Actual_Weight model Either \"\" log-linear variance model, \"B\" proportional scaling error model","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate filament models — filament1_estimate","text":"estimation object suitable use filament1_predict()","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate filament models — filament1_estimate","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict values for a filament model — filament1_predict","title":"Predict values for a filament model — filament1_predict","text":"Uses estimated filament model predict new observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict values for a filament model — filament1_predict","text":"","code":"filament1_predict(object, newdata, alpha = 0.05, df = Inf, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict values for a filament model — filament1_predict","text":"object estimation object, like output filament1_estimate() newdata data use prediction. Must column CAD_Weight alpha target coverage error prediction intervals, Default: 0.05 df degrees freedom t-quantiles prediction interval construction, Default: Inf ... Additional arguments, currently ignored","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict values for a filament model — filament1_predict","text":"data.frame variables mean, sd, lwr, upr, summarising prediction distribution row newdata.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict values for a filament model — filament1_predict","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive numerical optimisation explorer — optimisation","title":"Interactive numerical optimisation explorer — optimisation","text":"Shiny app exploring several numerical optimisation methods work","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive numerical optimisation explorer — optimisation","text":"","code":"optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate proper scores — proper_score","title":"Calculate proper scores — proper_score","text":"Calculates proper scores probabilistic predictions","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate proper scores — proper_score","text":"","code":"proper_score(type, obs, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate proper scores — proper_score","text":"type string indicating type score calculate. One \"se\", \"ae\", \"ds\", \"interval\". See Details . obs vector observations ... Additional named arguments needed type score, see Details .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate proper scores — proper_score","text":"vector calculated scores, one observation","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate proper scores — proper_score","text":"score type takes additional arguments define prediction model information used score calculations. Unless specified otherwise, extra arguments either scalars (applying predictions) vectors length obs vector. se Squared error score. Requires mean, defining prediction mean ae Absolute error score. Requires median, defining prediction median ds Dawid-Sebastiani score. Requires mean sd, defining mean standard deviation predictions interval Squared error score. Requires lwr upr, defining lower upper prediction interval endpoints, alpha, defining prediction error probability targeted prediction intervals scores \\(S_{type}(F_i,y_i)\\) defined lecture notes, obs[] equal \\(y_i\\) prediction distribution \\(F_i\\) defined additional named arguments.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate proper scores — proper_score","text":"","code":"# Five realisations of N(3, 3^2) obs <- rnorm(5, mean = 3, sd = 3)  # One prediction for each observation. Only one of the predictions describes # the true model F_mean <- 1:5 F_median <- 1:5 F_sd <- 1:5 F_lwr <- F_mean - F_sd * qnorm(0.9) F_upr <- F_mean - F_sd * qnorm(0.1)  # Compute the scores data.frame(   se = proper_score(\"se\", obs, mean = F_mean),   ae = proper_score(\"ae\", obs, median = F_median),   ds = proper_score(\"ds\", obs, mean = F_mean, sd = F_sd),   interval = proper_score(\"interval\", obs,                           lwr = F_lwr, upr = F_upr, alpha = 0.2) ) #>           se        ae        ds  interval #> 1 40.7008802 6.3797242 40.700880 53.544829 #> 2  3.9573135 1.9892998  2.375623  5.126206 #> 3  8.5421301 2.9226923  3.146350  7.689309 #> 4  0.9274896 0.9630626  2.830557 10.252413 #> 5  6.4348068 2.5366921  3.476268 12.815516"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute empirical weighted cumulative distribution — stat_ewcdf","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"Version ggplot2::stat_ecdf adds weights property observation, produce empirical weighted cumulative distribution function. empirical cumulative distribution function (ECDF) provides alternative visualisation distribution. Compared visualisations rely density (like geom_histogram()), ECDF require tuning parameters handles continuous discrete variables. downside requires training accurately interpret, underlying visual tasks somewhat challenging.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"stat_ewcdf(   mapping = NULL,   data = NULL,   geom = \"step\",   position = \"identity\",   ...,   n = NULL,   pad = TRUE,   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"n NULL, interpolate. NULL, number points interpolate . pad TRUE, pad ecdf additional points (-Inf, 0) (Inf, 1) na.rm FALSE (default), removes missing values warning.  TRUE silently removes missing values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"computed-variables","dir":"Reference","previous_headings":"","what":"Computed variables","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"x x data y cumulative density corresponding x","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"library(ggplot2)  n <- 100 df <- data.frame(   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),   g = gl(2, n),   w = c(rep(1/n, n), sort(runif(n))^sqrt(n)) ) ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\") #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Don't go to positive/negative infinity ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Multiple ECDFs ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf() #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?  ggplot(df, aes(x, colour = g, weights = w)) +   stat_ewcdf() +   facet_wrap(vars(g), ncol = 1) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted sample quantiles — wquantile","title":"Weighted sample quantiles — wquantile","text":"Calculates empirical sample quantiles optional weights, given probabilities. Like quantile(), smallest observation corresponds probability 0 largest probability 1. Interpolation discrete values done type=7, quantile(). Use type=1 generate quantile values raw input samples.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted sample quantiles — wquantile","text":"","code":"wquantile(   x,   probs = seq(0, 1, 0.25),   na.rm = FALSE,   type = 7,   weights = NULL,   ... )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted sample quantiles — wquantile","text":"x numeric vector whose sample quantiles wanted. NA NaN values allowed numeric vectors unless na.rm TRUE. probs numeric vector probabilities values \\([0,1]\\). na.rm logical; true, NA NaN's removed x quantiles computed. type numeric, 1 interpolation, 7, interpolated quantiles. Default 7. weights numeric vector non-negative weights, length x, NULL. weights normalised sum 1. NULL, wquantile(x) behaves quantile(x), equal weight sample value. ... Additional arguments, currently ignored","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted sample quantiles — wquantile","text":"","code":"# Some random numbers x <- rnorm(100)  # Plain quantiles: quantile(x) #>           0%          25%          50%          75%         100%  #> -2.243909518 -0.855850395 -0.001731204  0.753959995  2.693502723   # Larger values given larger weight, on average shifting the quantiles upward: wquantile(x, weights = sort(runif(length(x)))) #> [1] -2.24390952 -0.78741055  0.03426173  0.85167172  2.69350272"}]
