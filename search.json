[{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Lab 4, consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] lab, one considered parameterisation alternatives \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) confidence interval construction motivated ratio \\(\\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\lambda/n}}\\), approximated \\(\\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\widehat{\\lambda}/n}}\\). now wish see achieve better construction using non-approximated version. Solve inequalities \\[ = z_{\\alpha/2} < \\frac{\\widehat{\\lambda}-\\lambda}{\\sqrt{\\lambda/n}} < z_{1-\\alpha/2} = b \\] respect \\(\\lambda\\), implement resulting confidence interval construction function (code.R; also document function) taking arguments y alpha. Also include another function method used lab, call syntax. Hint: may help mathematical derivation several steps; e.g. introducing \\(x=\\sqrt{\\lambda}\\), solve two separate quadratic equations (\\(\\) one \\(b\\)) work intervals two inequalities fulfilled. , likely need fact \\(\\alpha<0.5\\), \\(z_{\\alpha/2}<0\\) \\(z_{1-\\alpha/2}>0\\). function estimate_coverage, defined code.R, can used estimate coverage construction confidence intervals samples \\(\\mathsf{Poisson}(\\lambda)\\), given interval construction R function. Use discuss following questions: two alternatives coverage closest nominal coverage 90% case \\(n=2\\), \\(\\lambda=3\\)? happens larger values \\(n\\)?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"d-printer-materials-prediction","dir":"Articles","previous_headings":"","what":"3D printer materials prediction","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"aim estimate parameters Bayesian statistical model material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator made simple physics analysis, settled model connection CAD_Weight Actual_Weight follows linear model, variance increases square CAD_Weight. denote CAD weight observations \\(\\) x_i, corresponding actual weight \\(y_i\\), model can defined \\[ y_i \\sim \\mathsf{Normal}[\\beta_1 + \\beta_2 x_i, \\beta_3 + \\beta_4 x_i^2)] . \\] ensure positivity variance, parameterisation \\(\\boldsymbol{\\theta}=[\\theta_1,\\theta_2,\\theta_3,\\theta_4]=[\\beta_1,\\beta_2,\\log(\\beta_3),\\log(\\beta_4)]\\) introduced, printer operator assigns independent prior distributions follows: \\[ \\begin{aligned} \\theta_1 &\\sim \\mathsf{Normal}(0,\\gamma_1), \\\\ \\theta_2 &\\sim \\mathsf{Normal}(1,\\gamma_2), \\\\ \\theta_3 &\\sim \\mathsf{LogExp}(\\gamma_3), \\\\ \\theta_4 &\\sim \\mathsf{LogExp}(\\gamma_4), \\end{aligned} \\] \\(\\mathsf{LogExp}()\\) denotes logarithm exponentially distributed random variable rate parameter \\(\\), seen Tutorial 4. \\(\\boldsymbol{\\gamma}=(\\gamma_1,\\gamma_2,\\gamma_3,\\gamma_4)\\) values positive parameters. printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, lead model, approximation . basic physics assumption error CAD software calculation weight proportional weight . Start loading data plot .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"prior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Prior density","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"help dnorm dlogexp function (see code.R file documentation), define document (code.R) function log_prior_density arguments theta params, theta \\(\\boldsymbol{\\theta}\\) parameter vector, params vector \\(\\boldsymbol{\\gamma}\\) parameters. function evaluate logarithm joint prior density \\(p(\\boldsymbol{\\theta})\\) four \\(\\theta_i\\) parameters.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"observation-likelihood","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Observation likelihood","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"help dnorm, define document function log_like, taking arguments theta, x, y, evaluates observation log-likelihood \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) model defined .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-density","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior density","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Define document function log_posterior_density arguments theta, x, y, params, evaluates logarithm posterior density \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\), apart unevaluated normalisation constant.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"posterior-mode-and-gaussian-approximation","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Posterior mode and Gaussian approximation","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Let \\(\\gamma_i=1\\), \\(=1,2,3,4\\), use optim() together log_posterior_density filament data find posterior mode \\(\\boldsymbol{\\theta}\\). Also evaluate inverse negated Hessian mode, order obtain multivariate Normal approximation \\(\\mathsf{Normal}(\\boldsymbol{\\mu},\\boldsymbol{S})\\) posterior distribution \\(\\boldsymbol{\\theta}\\). See documentation optim maximisation instead minimisation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling function","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"aim construct 90% Bayesian credible interval \\(\\beta_j\\) using importance sampling, similarly method used lab 4. , one dimensional Gaussian approximation posterior parameter used. , instead use multivariate Normal approximation importance sampling distribution. functions rmvnorm dmvnorm mvtnorm package can used sample evaluate densities. Define document function do_importance taking arguments N (number samples generate), mu (mean vector importance distribution), S (covariance matrix), additional parameters needed function code. function output data.frame five columns, beta1, beta2, beta3, beta4, log_weights, containing \\(\\beta_i\\) samples normalised \\(\\log\\)-importance-weights, sum(exp(log_weights)) \\(1\\). Use log_sum_exp function compute needed normalisation information.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01.html","id":"importance-sampling","dir":"Articles","previous_headings":"3D printer materials prediction","what":"Importance sampling","title":"StatComp Project 1 (2021/22): Simulation and sampling","text":"Use defined functions compute importance sample size \\(N=10000\\). Plot empirical weighted CDFs together un-weighted CDFs parameter, help stat_ewcdf, discuss results. achieve simpler ggplot code, may find pivot_longer(???, starts_with(\"beta\")) facet_wrap(vars(name)) useful. Construct 90% credible intervals four model parameters, based importance sample. addition wquantile pivot_longer, methods group_by summarise helpful. may wish define function make_CI taking arguments x, weights, prob (control intended coverage probability), generating 1-row, 2-column data.frame help structure code. Discuss results sampling method point view 3D printer application point view (may also involve e.g. plotting prediction intervals based point estimates parameters, plotting importance log-weights see depend sampled \\(\\beta\\)-values, like T4sol document).","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-problem","dir":"Articles","previous_headings":"Avoiding long running times","what":"The problem","title":"Project 01 Hints","text":"Collecting values data.frame inside loop can slow. common issue combining results computed loops using rbind step loop add one rows large vector data.frame can slow; instead linear computational cost size output, cost can become quadratic; needs reallocate memory copy previous results, new version result object.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"the-solution","dir":"Articles","previous_headings":"Avoiding long running times","what":"The solution","title":"Project 01 Hints","text":"key avoid reallocating memory instead pre-allocating whole result object loop. However subtleties , examples blow show differences approaches. common solution work simpler intermediate data structures within loop, collect results end. Indexing simple vectors much cheaper reallocating copying memory. Slow: Better, even data.frame indexing assignments causes memory reallocation: Fast; several orders magnitude faster (see timings ) avoiding memory reallocation inside loop: fast, bit compact; using matrix indexing doesn’t need memory reallocation, bit slower vector indexing. potential drawback method works columns output data.frame type (numeric), cases works code simple, readable, generally quick run: Benchmark timing comparisons:","code":"slow_fun <- function(N) {   result <- data.frame()   for (loop in seq_len(N)) {     result <- rbind(       result,       data.frame(A = cos(loop), B = sin(loop))     )   }   result } better_fun <- function(N) {   result <- data.frame(A = numeric(N), B = numeric(N))   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   result } fast_fun <- function(N) {   result_A <- numeric(N)   result_B <- numeric(N)   for (loop in seq_len(N)) {     result_A[loop] <- cos(loop)     result_B[loop] <- sin(loop)   }   data.frame(     A = result_A,     B = result_B   ) } ok_fun <- function(N) {   result <- matrix(0, N, 2)   for (loop in seq_len(N)) {     result[loop, ] <- c(cos(loop), sin(loop))   }   colnames(result) <- c(\"A\", \"B\")   as.data.frame(result) } N <- 10000 bench::mark(   slow = slow_fun(N),   better = better_fun(N),   fast = fast_fun(N),   ok = ok_fun(N) ) #> Warning: Some expressions had a GC in every iteration; so filtering is disabled. #> # A tibble: 4 × 6 #>   expression      min   median `itr/sec` mem_alloc `gc/sec` #>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> #> 1 slow          4.42s    4.42s     0.226    2.28GB     12.0 #> 2 better     758.04ms 758.04ms     1.32     1.49GB     44.9 #> 3 fast         1.75ms   2.16ms   454.     229.67KB      0   #> 4 ok           5.72ms   6.77ms   144.     457.98KB     14.0"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"dynamically-generated-column-names","dir":"Articles","previous_headings":"","what":"Dynamically generated column names","title":"Project 01 Hints","text":"size matrix data.frame depends parameter, multiple columns similar names, can convenient dynamically generate column names. paste() function can used . example, want data.frame size \\(n\\times m\\), column names Name1, Name2, etc, can use resulting object looks like :","code":"df <- matrix(runif(15), 5, 3) # Create a matrix with some values in it colnames(df) <- paste(\"Name\", seq_len(ncol(df)), sep = \"\") df <- as.data.frame(df) df #>       Name1       Name2     Name3 #> 1 0.8616293 0.008831673 0.7636030 #> 2 0.4959491 0.154100553 0.3035060 #> 3 0.7710326 0.918935379 0.3733938 #> 4 0.4815360 0.404045780 0.2798814 #> 5 0.1300462 0.447135166 0.9350371"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"multi-column-summarise-constructions","dir":"Articles","previous_headings":"","what":"Multi-column summarise constructions","title":"Project 01 Hints","text":"summarise() method dplyr package usually used construct simple summaries function computes mu sigma? make return data.frame, can used summarise:","code":"suppressPackageStartupMessages(library(tidyverse)) df <- data.frame(x = rnorm(100)) df %>%   summarise(     mu = mean(x),     sigma = sd(x)   ) #>           mu     sigma #> 1 0.03728718 0.9352652 my_fun <- function(x) {   data.frame(     mu = mean(x),     sigma = sd(x)   ) } df %>%   summarise(     my_fun(x)   ) #>           mu     sigma #> 1 0.03728718 0.9352652"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"avoiding-unnecessary-for-loops","dir":"Articles","previous_headings":"","what":"Avoiding unnecessary for-loops","title":"Project 01 Hints","text":"Since operations functions R vectorised, code can often written clear way avoiding unnecessary -loops. simple example, say want compute \\(\\sum_{k=1}^{100} \\cos(k)\\). -loop solution style good solution high performance low-level language like C might look like : taking advantage vectorised calling cos(), function sum(), can reformulate shorter clear vectorised solution: commonly used functions involving vectors TRUE FALSE () (). example, sum, , also operate across elements matrix. row-wise column-wise sums, see rowSums()colSums().","code":"result <- 0 for (k in seq_len(100)) {   result <- result + cos(k) } result <- sum(cos(seq_len(100))) vec <- c(TRUE, TRUE, FALSE, TRUE) all(vec) #> [1] FALSE any(vec) #> [1] TRUE sum(vec) # FALSE == 0, TRUE == 1 #> [1] 3"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"aligning-equations-in-multi-step-derivations","dir":"Articles","previous_headings":"LaTeX equations","what":"Aligning equations in multi-step derivations","title":"Project 01 Hints","text":"typesetting multi-step derivations, one align equations. LaTeX, can done align align* environments. case, might work RMarkdown, one can use aligned environment instead.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-1","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 1","title":"Project 01 Hints","text":"Result: \\[\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}\\]","code":"\\begin{align*} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{align*}"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project01Hints.html","id":"example-2","dir":"Articles","previous_headings":"LaTeX equations > Aligning equations in multi-step derivations","what":"Example 2","title":"Project 01 Hints","text":"Result: \\[ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} \\]","code":"$$ \\begin{aligned} \\frac{\\partial}{\\partial x}f(x, y) &= \\frac{\\partial}{\\partial x}\\exp(2x+3y) \\\\ &= 2\\exp(2x+3y) \\end{aligned} $$"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"StatComp Project 2 (2021/22): Scottish weather","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. See Tutorial07 exploratory introduction data, techniques wrangling data. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (Google maps online map systems can usually interpret latitude longitude searches). ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm)","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"seasonal-variability","dir":"Articles","previous_headings":"","what":"Seasonal variability","title":"StatComp Project 2 (2021/22): Scottish weather","text":"Note: first half assignment, can essentially ignore data missing; defining averages, just take average whatever observations available relevant data subset. construct averages group_by() summarise() & mean(), essentially happen .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"is-precipitation-seasonally-varying","dir":"Articles","previous_headings":"Seasonal variability","what":"Is precipitation seasonally varying?","title":"StatComp Project 2 (2021/22): Scottish weather","text":"daily precipitation data challenging model temperature, mix zeros positive values. Plot data show behaviour, e.g. one years, stations. temperature, seasonal effects clear; ’s generally colder winter summer. precipitation, seasonal variability less clear. Let winter \\(\\{\\text{Jan, Feb, Mar, Oct, Nov Dec}\\}\\), let summer \\(\\{\\text{Apr, May, Jun, Jul, Aug, Sep}\\}\\). easier code structure, add season information weather data object. Construct Monte Carlo permutation test hypotheses \\[ \\begin{aligned} H_0:&\\text{rainfall distribution winter summer} \\\\ H_1:&\\text{winter summer distributions different expeced values} \\end{aligned} \\] Use \\(T=|\\text{winter average} - \\text{summer average}|\\) test statistic, compute separate p-values weather station, respective Monte Carlo standard deviations. Collect results suitable data structure, present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"how-often-does-it-rain","dir":"Articles","previous_headings":"Seasonal variability","what":"How often does it rain?","title":"StatComp Project 2 (2021/22): Scottish weather","text":"Using technique rainfall amounts, test hypotheses \\[ \\begin{aligned} H_0:&\\text{daily probability rainfall winter summer} \\\\ H_1:&\\text{daily probability rainfall different winter summer} \\end{aligned} \\] Use \\(T=|\\text{winter empirical nonzero proportion} - \\text{summer empirical nonzero proportion}|\\) test statistic. Collect results suitable data structure, present discuss results. structured code reduced computation time, consider adapting previous code handle tests time; don’t need keep code two tests separate, can discuss results jointly.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"spatial-weather-prediction","dir":"Articles","previous_headings":"","what":"Spatial weather prediction","title":"StatComp Project 2 (2021/22): Scottish weather","text":"second half project, first construct version data set monthly averaged precipitation values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"estimation-and-prediction","dir":"Articles","previous_headings":"Spatial weather prediction","what":"Estimation and prediction","title":"StatComp Project 2 (2021/22): Scottish weather","text":"Define estimate monthly precipitation model Scotland. Use covariates spatial coordinates, elevation, suitable \\(\\cos\\) \\(\\sin\\) functions capture seasonal variability. long term climate trend covariate may also useful. Organise code can easily change input data without change estimation code predict new locations times. ’ll need able run estimation different subsets data, well manually constructed covariate values, e.g. illustrate prediction new location present available weather data. Present discuss modelling choices, results model estimation.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project02.html","id":"assessment","dir":"Articles","previous_headings":"Spatial weather prediction","what":"Assessment","title":"StatComp Project 2 (2021/22): Scottish weather","text":"interested well model able predict precipitation new locations. Construct stratified cross-validation groups data weather station, computes prediction scores station, well overall cross-validated average scores, aggregated 12 months year. Present discuss results score assessments, Squared Error Dawid-Sebastiani scores. model equally good predicting different stations? prediction accuracy across whole year?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example: Numerical statistics","text":"Lab 4, consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] lab, considered three parameterisation alternatives: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) know inverse expected Fisher information \\(\\lambda/n\\) case 1, \\(1/(4n)\\) case 2, \\(1/(n\\lambda)\\) case 3. lab, three functions, CI1, CI2, CI3, defined constructing confidence intervals based asymptotic Normal approximation Maximum Likelihood estimators parameterisations. project, use function pois_CI defined code.R file project repository. code file imported method shown RMdemo.Rmd file, displayed report. Look file documentation functions. task simulation study assess accuracy three interval construction methods. my_code.R file, define function multi_pois_CI(m, n, lambda, alpha,type) takes inputs m, indicating number simulations perform, n, number observations used sample, lambda, true parameter value, well alpha type interpretation pois_CI. function must return data frame m rows two columns, named Lower Upper containing confidence intervals, one row. Show mathematically widths \\(\\lambda\\)-intervals parameterisation type 1 2 always identical , given observation vector \\(\\boldsymbol{y}\\) confidence level \\(1-\\alpha\\). Note: ignore cases \\(\\theta\\)- \\(\\lambda\\)-intervals extend \\(0\\) (case 1, normal quantile \\(z\\) \\(\\overline{y} < z^2/n\\), case 2, \\(4\\overline{y} < z^2/n\\)). my_code.R file, define function tidy_multi_pois_CI(m, n, lambda, alpha) takes inputs multi_pois_CI except type argument. function must return data frame 3*m rows three columns, named Lower, Upper, Type containing confidence intervals three calls multi_pois_CI, Type column containing values 1, 2, 3 indicate method calculated interval. help tidy_multi_pois_CI, estimate coverage probability nominal confidence level \\(1-\\alpha\\)=90%, three interval types, \\(n=2\\), \\(\\lambda=3\\), using \\(m\\) least \\(100000\\). well coverage probabilities match nominal confidence level method? tidyverse package loaded, group_by summarise functions can used avoid direct indexing loops. Summarise discuss results. Plot empirical CDFs interval widths three methods, common figure. Use factor(Type) group information colour. Also compute median interval width method. Summarise discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic Sea","title":"StatComp Project Example: Numerical statistics","text":"Lab 3, investigated data archaeological excavation Gotland Baltic sea. 1920s, battle gravesite 1361 subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. wanted figure many persons likely buried gravesite. must reasonably least \\(256\\), many ? assignment, use importance sampling estimate credible intervals problem. lab, seemed difficult extract much information sample, since two observations two parameters. investigate including data excavations type might decrease uncertainty number persons buried, improved information average detection probability, \\(\\phi\\). function arch_data code.R returns data frame data four different excavations (note: one real; three synthetic data generated assignment). assume model lab excavation, unknown \\(N_j\\), \\(j=1,\\dots,J\\), \\(J\\) excavations, common detection probability \\(\\phi\\). assume model , conditionally \\(\\{N_j\\}\\) \\(\\phi\\), \\((Y_{j,}|N_j,\\phi)\\sim\\mathsf{Bin}(N_j,\\phi)\\), \\(=1,2\\), independent, prior distributions \\(N_j\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\), \\[ \\begin{aligned} p_{Y_{j,}|N_j=n,\\phi}(y) = \\mathsf{P}(Y_{j,}=y|N_j=n,\\phi) &= {n \\choose y} \\phi^y(1-\\phi)^{n-y}, \\quad y=0,\\dots,n, \\\\ p_{N_j}(n) = \\mathsf{P}(N_j=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\(0,1) , \\end{aligned} \\] \\(B(,b)\\) Beta function, see ?beta R.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"joint-probability-function-for-boldsymbolnboldsymboly","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Joint probability function for \\((\\boldsymbol{N},\\boldsymbol{Y})\\)","title":"StatComp Project Example: Numerical statistics","text":"Conditionally \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), observations \\(\\boldsymbol{Y}\\), conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\) \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) \\(\\widetilde{}=+\\sum_{j,} Y_{j,}\\) \\(\\widetilde{b}=b+2\\sum_j N_j -\\sum_{j,} Y_{j,}\\), sums go \\(j=1,\\dots,J\\) \\(=1,2\\). Show joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\), \\(N_j\\geq\\max(Y_{j,1},Y_{j,2})\\) \\(Y_{j,}=0,1,2,\\dots\\), given by1 \\[ p(\\boldsymbol{N},\\boldsymbol{Y}) = \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)} \\prod_{j=1}^J \\left[ p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\right], \\] \\(p(N_j)\\) probability mass function Geometric distribution prior \\(N_j\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"probability-function-implementation","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Probability function implementation","title":"StatComp Project Example: Numerical statistics","text":"help functions lbeta, lchoose, dgeom, define (my_code.R) function log_prob_NY(N,Y,xi,,b) arguments N (vector length \\(J\\)) \\(Y\\) (data frame size \\(J\\times 2\\), formatted like output arch_data(J)). arguments xi, , b model hyperparameters \\(\\xi\\), \\(\\), \\(b\\). function return logarithm \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\) combination \\(\\boldsymbol{N}\\) \\(\\boldsymbol{Y}\\) valid, otherwise return -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Importance sampling function","title":"StatComp Project Example: Numerical statistics","text":"aim construct Bayesian credible interval \\(N_j\\) using importance sampling, similarly method used lab 4. , Gaussian approximation posterior parameter used. , instead use prior distribution \\(\\boldsymbol{N}\\) sampling distribution, samples \\(N_j^{[k]}\\sim\\mathsf{Geom}(\\xi)\\). Since observations \\(\\boldsymbol{Y}\\) fixed, posterior probability function \\(p(\\boldsymbol{N}|\\boldsymbol{Y})\\) \\(\\boldsymbol{N}\\) proportional \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\). define logarithm unnormalised importance weights, \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[p(\\boldsymbol{N}^{[k]})]\\), \\(p(\\boldsymbol{N}^{[k]})\\) product \\(J\\) geometric distribution probabilities. Define (my_code.R) function arch_importance(K,Y,xi,,b) arguments Y, xi, , b , first argument K defining number samples generate. function return data frame \\(K\\) rows \\(J+1\\) columns, named N1, N2, etc, \\(j=1,\\dots,J\\), containing samples prior distributions \\(N_j\\), final column called Log_Weights, containing re-normalised log-importance-weights, constructed shifting \\(\\log[w^{[k]}]\\) subtracting largest \\(\\log[w^{[k]}]\\) value, lab 4.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"more-efficient-alternative","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea > Importance sampling function","what":"More efficient alternative","title":"StatComp Project Example: Numerical statistics","text":"Using \\(\\mathsf{Geom}(\\xi)\\) prior distribution \\(N_j\\) leads rather inefficient importance sampler, especially \\(J=4\\); weights become zero (\\(N_j^{[k]}<Y_{j,}\\)) near zeros, samples influence practical estimates credible intervals. alternative, can use sampling distributions better adapted posterior distributions. Let \\(N^\\text{min}_j=\\max(Y_{j,1}, Y_{j,2})\\) smallest allowed value sample, \\(j=1,\\dots,J\\). eliminate impossible combinations, weights (theory numerically) positive. improve method, can choose different values probability parameter samples, also adapt differently \\(j\\). Without motivation, can use values \\(\\xi_j=1/(1 + 4 N^\\text{min}_j)\\) sampling, \\(N_j^{[k]}=N^\\text{min}_j + \\mathsf{Geom}(\\xi_j)\\). Note: Due nature geometric distribution, also equivalent ordinary \\(\\mathsf{Geom}(\\xi_j)\\) conditioned least \\(N^\\text{min}_j\\). take different sampling method account, log-weights need defined \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[\\widetilde{p}(\\boldsymbol{N}^{[k]})]\\), \\(\\widetilde{p}(\\boldsymbol{N}^{[k]})=\\prod_{j=1}^J p_{\\mathsf{Geom}(\\xi_j)}(N_j^{[k]}-N^\\text{min}_j)\\), product \\(J\\) different geometric distribution probabilities. may adapt example code (details depend vectorisation method use): Using method, \\(K\\) doesn’t need larger \\(100000\\), small \\(K=1000\\) starts give reasonable values code testing purposes, \\(K=10000\\) take seconds run code fully vectorised (vapply function can used, basic -loop also fine case).","code":"# matrix of minimum allowed N for each j, repeated K times N_min <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J) xi_sample <- 1 / (1 + 4 * N_min) # Sample values and call log_prob_NY for each sample, # store N-values in a K-by-J matrix, and log(p_NY)-values in a vector log_PY N <- matrix(rgeom(..., prob = xi_sample), K, J) + N_min log_PY <- ... # Subtract the sampling log-probabilities log_PY - rowSums(dgeom(N - N_min, prob = xi_sample, log = TRUE))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-n_j","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(N_j\\)","title":"StatComp Project Example: Numerical statistics","text":"lab, let \\(\\xi=1/1001\\), \\(=b=1/2\\). Use arch_importance wquantile function type=1 (see help text information) compute credible intervals \\(N_j\\). Start including data single excavation, observe interval \\(N_1\\) changes data excavations added. Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(\\phi\\)","title":"StatComp Project Example: Numerical statistics","text":"Update2 arch_importance function add column Phi samples conditional distribution \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) row importance sample. Use updated function construct credible intervals \\(\\phi\\), one \\(J=1\\), one \\(J=4\\). Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"investigate behaviour three approximate methods confidence interval construction expectation parameter \\(\\lambda\\) model observations \\(y_i\\sim\\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\). denote three methods given assignment \\(\\textrm{CI}_1\\), \\(\\textrm{CI}_2\\), \\(\\textrm{CI}_3\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"theoretical-interval-width-equivalence","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Theoretical interval width equivalence","title":"StatComp Project Example Solution: Numerical Statistics","text":"Let \\(z=z_{1-\\alpha/2}=-z_{\\alpha/2}\\), \\(z_p\\) (lower) \\(p\\)-quantile standard \\(\\textrm{N}(0,1)\\) distribution. , \\(\\overline{y} > z^2/n\\), Methods 1 2 pois_CI constructing confidence interval \\(\\lambda\\) observations \\(y_i\\sim \\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\), given \\[\\begin{align*} \\textrm{CI}_1 &= (\\overline{y} - z\\sqrt{\\overline{y}/n}, \\overline{y} + z\\sqrt{\\overline{y}/n}), \\\\ \\textrm{CI}_2 &= ([\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2, [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2). \\end{align*}\\] interval width \\(\\textrm{CI}_1\\) \\(2z\\sqrt{\\overline{y}/n}\\). \\(\\textrm{CI}_2\\), width \\[\\begin{align*} [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2 - [\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2 &=  [\\overline{y} + 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] - [\\overline{y} - 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] \\\\ &= 4z\\sqrt{\\overline{y}}/\\sqrt{4n} \\end{align*}\\] since remaining terms cancel . Finally \\(4/\\sqrt{4}=2\\), see width \\(\\textrm{CI}_2\\) interval \\(\\textrm{CI}_1\\), observed value \\(\\overline{y}>z^2/n\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-coverage-and-width-assessment","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Confidence interval coverage and width assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"interested behaviour three confidence interval methods far away asymptotic limit. Instead assuming \\(n\\rightarrow\\infty\\) \\(\\lambda\\rightarrow\\infty\\) instead look case \\(n=2\\), \\(\\lambda=3\\), discrete nature sampling distribution maximum likelihood estimator \\(\\widehat{\\lambda}=\\overline{y}\\) still apparent. my_code.R file, ’ve defined two helper functions, multi_pos_CI tidy_multi_pois_CI, order simulate \\(m\\) realisations model, compute three types approximate confidence intervals desired confidence level (also called target nominal confidence level) 90%, corresponding nominal error probability \\(\\alpha=0.1\\). get reasonable precision results, use \\(m=100000\\). See code appendix short analysis code. assess quality methods, compute estimate coverage probability proportion \\(m\\) simulations intervals cover true \\(\\lambda\\) value, Coverage table . also compute median width intervals, denoted MedWidth. extra details, estimate probability lower endpoint large, OutsideLower, upper endpoint low, OutsideUpper. Finally, compute median lower upper endpoints, denoted MedLower MedUpper table. expected theoretical calculations intervals don’t reach \\(0\\) left, median width methods 1 2 identical, whereas method 3 widths slightly larger. see methods 1 3 low high coverage, respectively, compared nominal overage \\(90\\%\\), method 2 coverage close nominal, \\(89.58\\%\\). ’s clear method 1 make ’s mistakes upper end, much \\(15\\%\\) probability low upper endpoint. contrast, method 3 overly cautious upper end, just \\(1.5\\%\\) error probability upper tail, expense wider intervals, sometimes stretching way \\(+\\infty\\). Method 2 appears strike good balance coverage probability interval widths, lower tail error probability almost method 3 (ca \\(4\\%\\)), moving upper endpoint left, thereby reducing interval widths increasing upper tail error probability around \\(6\\%\\). result, method 2 nearly equal tail error probabilities (perfect equality 5%) test case \\(n=2\\), \\(\\lambda=3\\), appears best three methods capturing behaviour sampling distribution \\(\\lambda\\)-estimator. figure , ’ve plotted empirical cumulative distribution functions interval widths three methods, clearly shows discrete nature interval contructions, since based \\(\\overline{y}\\), clear discrete set possible outcomes. equal width property methods 1 2 breaks small values \\(\\overline{y}\\), due handle intervals gone zero simplistic implementation. summary, method 2 appears preferable, despite three methods based asympototic maximum likelihood theory. small sample sizes, asymptotic theory doesn’t tell whole story, instead one look carefully problem structure order get reliable estimates.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"StatComp Project Example Solution: Numerical Statistics","text":"Pairs counts \\((Y_{j,1},Y_{j,2})\\) left right femurs archaeological excavations modelled conditionally independent realisations \\(\\textrm{Binom}(N_j,\\phi)\\), \\(N_j\\) original number people buried excavtion site, \\(\\phi\\) probability finding buried femur. prior distribution \\(N_j\\), \\(j=1,\\dots,J\\), \\(\\textrm{Geom}(\\xi)\\), \\(\\xi=1/(1+1000)\\) (\\(\\textrm{E}(N_j)=1000\\) priori), detection probability, \\(\\phi\\sim\\textrm{Beta}(\\frac{1}{2},\\frac{1}{2})\\). denote \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\) \\(\\boldsymbol{Y}=\\{(Y_{1,1},Y_{1,2}),\\dots,(Y_{J,1},Y_{J,2})\\}\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"joint-probability-function","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Joint probability function","title":"StatComp Project Example Solution: Numerical Statistics","text":"Taking product prior density \\(\\phi\\), prior probabilities \\(N_j\\), conditional observation probabilities \\(N_{j,}\\) integrating \\(\\phi\\), get joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\) \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\int_0^1 p_\\phi(\\phi) \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 p(Y_{j,}|N_j,\\phi)\\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\phi^{Y_{j,}}(1-\\phi)^{N_j-Y_{j,}}   \\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(,b)} \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} , \\end{aligned} \\] \\(\\widetilde{} = +\\sum_{j=1}^J\\sum_{=1}^2 Y_{j,}\\) \\(\\widetilde{b} = b+\\sum_{j=1}^J\\sum_{=1}^2 (N_j - Y_{j,})\\). integral can renormalised integral distribution probability density function \\(\\textrm{Beta}(\\widetilde{},\\widetilde{b})\\) distribution: \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(\\widetilde{},\\widetilde{b})}   \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\\\ &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} . \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"credible-intervals-for-n_j-and-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Credible intervals for \\(N_j\\) and \\(\\phi\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"order obtain posterior credible intervals unknown quantities \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), \\(\\phi\\), implement importance sampling method sample \\(N_j\\) shifted Geometric distribution smallest value equal smallest valid \\(N_j\\), given observed counts, expectation corresponding \\(\\phi=1/4\\). sampling \\(\\boldsymbol{N}\\) values, corresponding importance weights \\(w_k\\), stored Log_Weights equal \\(\\log(w_k)\\) normalised subtracting larges \\(\\log(w_k)\\) value. Finally, \\(\\phi\\) values sampled directly conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\sim\\textrm{Beta}\\left(\\frac{1}{2}+\\sum_{j,} Y_{,j},\\frac{1}{2}+\\sum_{j,} (N_j-Y_{,j})\\right)\\). implementation arch_importance() function my_code.R shown code appendix, produces importance sample size \\(K\\). achieve relatively reliable results, use \\(K=100000\\), \\(J=1\\), \\(2\\), \\(3\\), \\(4\\), takes ca 4 seconds computer. First, use stat_ewcdf plot emprirical cumulative distribution functions importance weighted posterior samples \\(N_1\\), \\(J=1,\\dots,4\\). can see random variability increases larger \\(J\\), indicating lower efficiency importance sampler. Also included CDF prior distribution \\(N_1\\). Except lower end distribution, ’s immediately clear effect observations distribution.  Next corresponding plot \\(\\phi\\). contrast \\(N_1\\), ’s clear observation pairs get, concentrated distribution \\(\\phi\\) gets. new observation pair also influences particular lower end distribution, lower quantiles clearly increasing \\(J=4\\) even though upper quantiles remain relatively stable \\(J\\geq 2\\).  Using wquantile extract lower upper \\(5\\%\\) quantiles obtain approximate \\(90\\%\\) posterior credible intervals \\(N_1\\) \\(\\phi\\) shown following table. can see credible intervals \\(N_1\\) strongly linked information posterior distribution \\(\\phi\\). Interestingly, due lowering lower endpoint \\(\\phi\\) \\(J=2\\) \\(3\\), width \\(N_1\\) interval actually increases add information second third. stark contrast common intuition expect uncertainty decrease collect data. behaviour partly due random variability inherent small sample, also due strong non-linearity estimation problem, products \\(\\phi N_j\\) much easier estimate individual \\(\\phi\\) \\(N_j\\) parameters. \\(J=4\\), interval width \\(N_1\\) finally decreases, well width \\(\\phi\\) interval. Looking values observations, can see \\(J=4\\) case much larger values cases, leads providing relatively information \\(\\phi\\) parameter. practical point view, ’s clear estimation problem easy solution. Even multiple excavations, assuming model appropriate value \\(\\phi\\) \\(\\phi\\) every excavation, gain modest amount additional information. positive side, model good job approximating reality, borrowing strength across multiple excavations improve estimates uncertainty information excavation. improvements, one likely need closely study excavation precedures, gain better understanding data generating process, might lead improved statistical methods archaeologically relevant outcomes.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"effective-sample-size","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Effective sample size","title":"StatComp Project Example Solution: Numerical Statistics","text":"Using method computing effective importance sample size later course can see even efficient method, sampling inefficient \\(J=4\\), indicating improved choice importance distribution \\(N_j\\) improve precision accuracy results: , EffSampleSize approximate effective sample size \\((\\sum_k w_k)^2/\\sum w_k^2\\), RelEff relative efficiency, defined ratio EffSampleSize \\(K=100000\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"profile-likelihood-method-for-frequentist-confidence-interval-for-textrmbinomnp","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Profile likelihood method for frequentist confidence interval for \\(\\textrm{Binom}(n,p)\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"(actually range(y) depends n may mean l0 justified example)","code":"## [1] 274"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"function-definitions","dir":"Articles","previous_headings":"Code appendix","what":"Function definitions","title":"StatComp Project Example Solution: Numerical Statistics","text":"","code":"# Finn Lindgren (s0000000, finnlindgren)  # Part 1 ####  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using one of the three methods implemented in pois_CI # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals #   type: 1, 2, 3, or 4, indicating which CI method to use # # Output: #   A data frame with `m` rows and columns `Lower` and `Upper`, with one #   confidence interval per row  multi_pois_CI <- function(m, n, lambda, alpha, type) {   Lower = numeric(m)   Upper = numeric(m)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     CI <- pois_CI(y = y, alpha = alpha, type = type)     Lower[m_idx] <- CI[1]     Upper[m_idx] <- CI[2]   }   result <- data.frame(Lower = Lower, Upper = Upper) }  tidy_multi_pois_CI_alt <- function(m, n, lambda, alpha) {   Lower <- matrix(0, m, 4)   Upper <- matrix(0, m, 4)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     for (type in 1:4) {       CI <- pois_CI(y = y, alpha = alpha, type = type)       Lower[m_idx, type] <- CI[1]       Upper[m_idx, type] <- CI[2]     }   }   data.frame(     Index = rep(1:m, times = 4),     Type = rep(1:4, each = m),     Lower = as.vector(Lower),     Upper = as.vector(Upper)   ) }  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using all three methods implemented in pois_CI. # This implementation uses independent simulations for the three method, by # calling multi_pois_CI for each interval type. An alternative implementation # would be to use the same samples for all three methods. # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals # # Output: #   A data frame with `3m` rows and columns `Lower`, `Upper`, and `Type`, #   with one confidence interval per row, calculated with the method indicated #   in the `Type` column  tidy_multi_pois_CI <- function(m, n, lambda, alpha) {   result <-     rbind(       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 1         ),         Type = 1       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 2         ),         Type = 2       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 3         ),         Type = 3       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 4         ),         Type = 4       )     )   result }   # Part 2 ####  # Joint log-probability # # Usage: #   log_prob_NY(N, Y, xi, a, b) # # Calculates the joint probability for N and Y, when #   N_j ~ Geom(xi) #   phi ~ Beta(a, b) #   Y_{ji} ~ Binom(N_j, phi) #   For j = 1,...,J # # Inputs: #   N: vector of length J #   Y: data.frame or matrix of dimension Jx2 #   xi: The probability parameter for the N_j priors #   a, b: the parameters for the phi-prior # # Output: #   The log of the joint probability for N and Y #   For impossible N,Y combinations, returns -Inf  log_prob_NY <- function(N, Y, xi, a, b) {   # Convert Y to a matrix to allow more vectorised calculations:   Y <- as.matrix(Y)   if (any(N < Y) || any(Y < 0)) {     return(-Inf)   }   atilde <- a + sum(Y)   btilde <- b + 2 * sum(N) - sum(Y) # Note: b + sum(N - Y) would be more general   lbeta(atilde, btilde) - lbeta(a, b) +     sum(dgeom(N, xi, log = TRUE)) +     sum(lchoose(N, Y)) }  # Importance sampling for N,phi conditionally on Y # # Usage: #   arch_importance(K,Y,xi,a,b) # # Inputs: #   Y,xi,a,b have the same meaning and syntax as for log_prob_NY(N,Y,xi,a,b) #   The argument K defines the number of samples to generate. # # Output: #  A data frame with K rows and J+1 columns, named N1, N2, etc, #  for each j=1,…,J, containing samples from an importance sampling #  distribution for Nj, #  a column Phi with sampled phi-values, #  and a final column called Log_Weights, containing re-normalised #  log-importance-weights, constructed by shifting log(w[k]) by subtracting the #  largest log(w[k]) value, as in lab 4.  arch_importance <- function(K, Y, xi, a, b) {   Y <- as.matrix(Y)   J <- nrow(Y)   N_sample <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J)   xi_sample <- 1 / (1 + 4 * N_sample)    N <- matrix(rgeom(K * J, prob = xi_sample), K, J) + N_sample   colnames(N) <- paste0(\"N\", seq_len(J))    Log_Weights <- vapply(     seq_len(K),     function(k) log_prob_NY(N[k, , drop = TRUE], Y, xi, a, b),     0.0) -     rowSums(dgeom(N - N_sample, prob = xi_sample, log = TRUE))    phi <- rbeta(K, a + sum(Y), b + 2 * rowSums(N) - sum(Y))    cbind(as.data.frame(N),         Phi = phi,         Log_Weights = Log_Weights - max(Log_Weights)) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-intervals","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Confidence intervals","title":"StatComp Project Example Solution: Numerical Statistics","text":"Poisson parameter interval simulations: Summarise coverage probabilities interval widths: Plot CDFs CI width distributions:","code":"n <- 2 lambda_true <- 3 result <- tidy_multi_pois_CI_alt(100000, n, lambda_true, alpha = 0.1) result %>%   group_by(Type) %>%   summarise(     Coverage = mean((Lower <= lambda_true) & (lambda_true <= Upper)),     MedWidth = median(Upper - Lower),     OutsideLower = mean(lambda_true < Lower),     OutsideUpper = mean(lambda_true > Upper),     MedLower = median(Lower),     MedUpper = median(Upper)   ) %>%   knitr::kable()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Archaeology","title":"StatComp Project Example Solution: Numerical Statistics","text":"Compute credible intervals \\(N_j\\) different amounts data, \\(J\\): completeness, compute credible intervals \\(N_j\\), \\(1\\leq J\\), \\(J=1,\\dots,4\\), analysis focuses \\(N_1\\) \\(\\phi\\): Restructure interval data.frame focus \\(N_1\\) \\(\\phi\\) :","code":"xi <- 1 / 1001 a <- 0.5 b <- 0.5  K <- 1000000 NW1 <- arch_importance(K, arch_data(1), xi, a, b) NW2 <- arch_importance(K, arch_data(2), xi, a, b) NW3 <- arch_importance(K, arch_data(3), xi, a, b) NW4 <- arch_importance(K, arch_data(4), xi, a, b) CI <- rbind(   NW1 %>%     summarise(       J = 1,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = NA,       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW2 %>%     summarise(       J = 2,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW3 %>%     summarise(       J = 3,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW4 %>%     summarise(       J = 4,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = wquantile(N4,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ) ) CI_ <- CI %>%   pivot_longer(     cols = c(N1, N2, N3, N4, Phi),     names_to = \"Variable\",     values_to = \"value\"   ) %>%   pivot_wider(     names_from = End,     values_from = value   ) %>%   filter(!is.na(Lower)) %>%   mutate(Width = Upper - Lower) knitr::kable(CI_ %>% filter(Variable %in% c(\"N1\", \"Phi\")) %>%   arrange(Variable, J))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"deriving-the-hessian","dir":"Articles","previous_headings":"","what":"Deriving the Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Define individual terms \\(f_k=y_k\\log(1+e^{-\\eta_k}) + (N-y_k)\\log(1+e^{\\eta_k})\\), \\(f=\\sum_{k=1}^n f_k\\). know \\(\\partial\\eta_k/\\partial\\theta_1\\equiv 1\\) \\(\\partial\\eta_k/\\partial\\theta_2=k\\). derivatives respect \\(\\theta_1\\) \\(\\theta_2\\) can obtain chain rule: \\[\\begin{align*} \\frac{df_k}{d\\theta_i} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\frac{\\partial f_k}{\\partial\\eta_k} \\\\   &= \\frac{\\partial \\eta_k}{\\partial \\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k}}{1+e^{-\\eta_k}} +   (N-y_k) \\frac{e^{\\eta_k}}{1+e^{\\eta_k}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   (N-y_k) \\frac{e^{\\eta_k/2}}{e^{-\\eta_k/2}+e^{\\eta_k/2}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   - y_k \\frac{e^{-\\eta_k/2}+e^{\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   N \\frac{1}{e^{-\\eta_k}+1}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\ \\end{align*}\\] Since derivatives \\(\\eta_k\\) respect \\(\\theta_1\\) \\(\\theta_2\\) depend values \\(\\theta_1\\) \\(\\theta_2\\), second order derivatives \\[\\begin{align*} \\frac{d^2f_k}{d\\theta_i d\\theta_j} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{\\partial}{\\partial\\eta_k}\\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N e^{-\\eta_k}}{(e^{-\\eta_k}+1)^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{(e^{-\\eta_k/2}+e^{\\eta_k/2})^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{4\\cosh(\\eta_k/2)^2}   \\\\ \\end{align*}\\] Plugging \\(\\theta\\)-derivatives gives Hessain contribution term \\(f_k\\) \\[   \\frac{N}{4\\cosh(\\eta_k/2)^2} \\begin{bmatrix}1 & k \\\\ k & k^2\\end{bmatrix} . \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"positive-definite-hessian","dir":"Articles","previous_headings":"","what":"Positive definite Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"show total Hessian \\(f\\) positive definite \\(n \\geq 2\\), define vectors \\(\\boldsymbol{u}_k=\\begin{bmatrix}1 \\\\ k\\end{bmatrix}\\) \\(d_k=\\frac{N}{4\\cosh(\\eta_k/2)^2}\\). Define 2-\\(n\\) matrix \\(\\boldsymbol{U}=\\begin{bmatrix}\\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\cdots & \\boldsymbol{u}_n\\end{bmatrix}\\) diagonal matrix \\(\\boldsymbol{D}\\) \\(D_{ii}=d_i\\). product \\(\\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{U}^\\top\\) another way writing Hessian \\(f\\). Since vectors \\(\\boldsymbol{u}_k\\) non-parallel, \\(d_i\\) strictly positive combinations \\(\\theta_1\\) \\(\\theta_2\\), matrix full rank (rank 2) \\(n \\geq 2\\), positive definite.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"alternative-reasoning","dir":"Articles","previous_headings":"","what":"Alternative reasoning","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Hessian \\(f_k\\) positive semi-definite, since can written \\(d_k\\boldsymbol{u}_k\\boldsymbol{u}_k^\\top\\) \\(d_k > 0\\) vector \\(\\boldsymbol{u}_k\\). sum positive definite matrix positive semi-definite matrix positive definite, ’s sufficient prove sum first two terms positive definite. positive scaling constant \\(w\\), determinant \\[ \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}+w\\begin{bmatrix}1 & 2\\\\2 & 4\\end{bmatrix} \\] \\((1+w)(1+4w)-(1+2w)^2=1+5w+4w^2-1-4w-4w^2=w > 0\\). means (positively) weighted sum two matrices positive definite (since positive semi-definite, positive determinant rules ot sum positive semi-definite). proves total Hessian positive definite \\(n\\geq 2\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"remark","dir":"Articles","previous_headings":"","what":"Remark","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Note first proof positive definiteness, didn’t actually need know specific values \\(\\boldsymbol{u}_k\\) vectors, proportional gradients fo \\(\\eta_k\\) respect \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)\\); sufficient non-parallel, ensuring \\(\\boldsymbol{U}\\) full rank. means linear model \\(\\eta_k\\) set parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_p)\\) leads positive definite Hessian model, collection gradient vectors \\((\\eta_1,\\dots,\\eta_n)\\) respect \\(\\boldsymbol{\\theta}\\) collective rank least \\(p\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"case 3, negated log-likelihood \\[ \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant} \\] 1st order derivative \\[ \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i \\] shows \\(\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y})\\), 2nd order derivative \\[ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta \\] equal \\(n\\lambda\\) \\(y_i\\) values, inverse expected Hessian \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"can test interval construction methods following code. methods always produce valid intervals. \\(\\overline{y}=0\\), first method produces single point “interval”. third method fails \\(\\overline{y}=0\\), due log zero. can happen \\(n\\) \\(\\lambda\\) close zero.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"probability theory, know \\(p(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}\\), \\[ p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right) \\]","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"First, sample Gaussian approximation posterior distribution. avoid numerical problems, calculate log unnormalised weights, shift results get numerically sensible scale exponentiate. credible interval \\(\\theta\\) can now extracted quantiles weighted sample, \\(\\lambda\\)-interval obtained transformation \\(\\lambda=\\exp(\\theta)\\): code steps following:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043 a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval lambda_interval <- exp(theta_interval) lambda_interval"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"figure shows empirical distributions unweighted weighted samples \\(\\{x_k\\}\\) (weights \\(\\{w_k\\}\\)), together theoretical posterior empirical distribution function. weighted, importance sampling, version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution \\(\\log(\\lambda)\\) excellent approximation even add importance sampling step.  lower figure, importance weights normalised average, values 1 mean corresponding sample values occur often true, target, distribution raw samples, values 1 mean sample values occur less frequently target distribution. scaled weights shown logarithmic scale.","code":"# Plotting updated to include a plot of the importance weights p1 <-   ggplot(data.frame(lambda = exp(x), weights = weights)) +   ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\")) +   scale_x_log10(limits = c(3, 20)) +   labs(colour = \"Type\")  p2 <- ggplot(data.frame(lambda = exp(x), weights = weights),              mapping = aes(lambda, weights / mean(weights))) +   ylab(\"Importance weights\") +   geom_point() +   geom_line() +   geom_hline(yintercept = 1) +   scale_y_log10() +   scale_x_log10(limits = c(3, 20))  # The patchwork library provides a versatile framework # for combining multiple ggplot figures into one. library(patchwork) (p1 / p2)"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"regular-startup-on-rstudio-cloud","dir":"Articles","previous_headings":"Startup","what":"Regular startup on rstudio.cloud","title":"Tutorial 01: R and linear models","text":"haven’t already done , go Technical Setup page Learn either follow links sign rstudio.cloud, install R version 4.1.2 RStudio computer. rstudio.cloud, go “Statistical Computing” Workspace (left side menu) Create new project. Click project name top window rename descriptive name, example Lab01. project Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open  lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"alternative-local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Alternative local computer RStudio installation","title":"Tutorial 01: R and linear models","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open  lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work (rstudio.cloud regular RStudio, unsaved changes may still remembered system, much safer explicitly save files, also essential start git version control). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\")","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models","text":"Use lm() function estimate model save estimated model variable called .","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation.","code":"newdata <- data.frame(z = -10:20)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models","text":"can now use something like following code plot results: Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"regular-startup-on-rstudio-cloud","dir":"Articles","previous_headings":"Startup","what":"Regular startup on rstudio.cloud","title":"Tutorial 01: R and linear models (solutions)","text":"haven’t already done , go Technical Setup page Learn either follow links sign rstudio.cloud, install R version 4.1.2 RStudio computer. rstudio.cloud, go “Statistical Computing” Workspace (left side menu) Create new project. Click project name top window rename descriptive name, example Lab01. project Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open  lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"alternative-local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Alternative local computer RStudio installation","title":"Tutorial 01: R and linear models (solutions)","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open  lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models (solutions)","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models (solutions)","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work (rstudio.cloud regular RStudio, unsaved changes may still remembered system, much safer explicitly save files, also essential start git version control). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\")","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models (solutions)","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models (solutions)","text":"Use lm() function estimate model save estimated model variable called .","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models (solutions)","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation. Solution:","code":"newdata <- data.frame(z = -10:20) data_pred <- cbind(newdata,                    data.frame(fit = predict(mod, newdata))) ggplot(data) +   geom_point(aes(z, y)) +   geom_line(aes(z, fit), data = data_pred) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models (solutions)","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Solution: Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"pred <- cbind(   newdata,   predict(mod, newdata, interval = \"prediction\") ) plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: Solution: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions <- function(x, newdata, xname = \"x\") {   pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   ggplot(pred) +     geom_line(aes_string(xname, \"fit\")) +     geom_ribbon(aes_string(x = xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25) } plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models (solutions)","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot. Solution:","code":"# Extended version of the solution: plot_prediction <- function(x, newdata, xname = \"x\",                             xlab = NULL, ylab = NULL, ...) {   if (is.null(xlab)) {     xlab <- xname   }   if (is.null(ylab)) {     ylab <- \"Response\"   }    pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   pl <- ggplot() +     geom_line(data = pred,               aes_string(xname, \"fit\")) +     geom_ribbon(data = pred,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Also add the confidence intervals for the predictor curve   conf <- cbind(newdata,                 predict(x, newdata, interval = \"confidence\"))   pl <- pl +     geom_ribbon(data = conf,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Add lables:   pl + xlab(xlab) + ylab(ylab) }"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models (solutions)","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data) plot_prediction(mod2, newdata, xname = \"z\", ylab = \"y\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") +   ggtitle(\"Confidence and prediction intervals\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods. Solution:","code":"formulas <- c(y ~ 1,               y ~ z,               y ~ z + I(z^2),               y ~ z + I(z^2) + I(z^3)) mods <- lapply(formulas, function(x) lm(x, data))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models (solutions)","text":"can now use something like following code plot results:  Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization","text":"example, ’ll look model connection  values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Check ?optim help text information result object contains. optimisation converge? Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization","text":"help  produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals.","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1)"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"initial-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Initial authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re working rstudio.cloud, first click name top right corner go Authentication. Activate github access, including “Private repo access also enabled”. redirected authenticate github. (Without step, creating new project github repository becomes complicated.) ’re working local computer, see sections Personal access token HTTPS (Personal Access Tokens, used week 1, see Technical Setup Learn) Set keys SSH (’re already using ssh keys, can used github) happygitwithr.com useful information setting system. simplest approach set GITHUB_PAT .Renviron file usethis::edit_r_environ()., methods secure. procedure similar setup rstudio.cloud Projects, local computer setup can done globally R projects.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"creating-a-new-rstudio-project-from-github","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Creating a new RStudio Project from github","title":"Tutorial 02: Optimization (solutions)","text":"Section Connect GitHub explains create new repositories github, access . tutorial, repository already created StatComp21 organisation, “clone” “Make local change, commit, push” steps needed. setting initial github authentication, follow steps: Go github.com/StatComp21 see Repository called lab02-username “username” github username. (, may connected course github time repostitory created; contact course organizer.) Click repository see contains. Use “Code” button copy HTTPS version URL can used create “Clone”.  rstudio.cloud: go Statistical Computing workspace. Create “New Project”-“Git Repository” based URL git repository local R/RStudio installation: opening RStudio, choosing “File -> New Project -> Version Control -> Git” Make sure place repository suitable location local disk, avoid blank spaces special characters path. project, open file lab02_code.R, use hold code lab. lab, remember save script file regularly, avoid losing work. also use git synchronise file contents github repository. Note: ’re rstudio.cloud, version StatCompLab package already installed, running remotes::install_github(\"finnlindgren/StatCompLab\") ensure latest version. local computer, also need run remotes::install_github(\"finnlindgren/StatCompLab\") install upgrade package. remotes::install_github syntax means R use function install_github package remotes, without load functions global environment, done library(\"remotes\"). cases, R/RStudio detect related packages newer versions, ask want first upgrade packages; generally safe accept upgrades CRAN repository, sources, including github, can decline updates unless know need development version package. addition lab tutorial documents, StatCompLab package includes graphical interactive tool exploring optimisation methods, based R interactive shiny system.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"further-authentication-setup","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Further authentication setup","title":"Tutorial 02: Optimization (solutions)","text":"’re using local installation, can skip step ’ve already setup github credentials. cloning repository rstudio.cloud, need setup authentication within project (method can used initially setting authentication computer, options, SSH public key authentication, see happygitwithr.com): github.com/StatComp21, go user (icon top right corner) “Settings”->“Developer settings”->“Personal access tokens” Create new token (“PAT”), “repo” access option enabled. Copy generated string safe place RStudio project Console pane, run credentials::set_github_pat() add new token Terminal pane, run sets cache timeout bit 16 weeks need authenticate project . need go procedure rstudio.cloud git project create future. (store PAT safe place can reuse projects, ’s safer use , delete copy running set_github_pat()) Use credentials::credential_helper_get(), ..._list() ..._set() control method used store authentication information. computer, can set method (usually called “store”) stores information permanently instead just caching limited time.","code":"git config --global credential.helper \"cache --timeout=10000000\""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"basic-git-usage","dir":"Articles","previous_headings":"Working with git/github/RStudio","what":"Basic git usage","title":"Tutorial 02: Optimization (solutions)","text":"basic git operations, clone, add, commit, push, pull process used copy git repository github either rstudio.cloud local machine create clone; Initially, cloned repository identical original, local changes can made. add commit: changes one several files “done”, need tell git collect changes store . git, one needs first tell file changes add (called staging git), commit changes. push: ’re happy local changes want make available either different computer, others access github copy repository, push commits. pull: get access changes made github repository, can pull commits made since last time either cloned pulled. Task: Open lab02_code.R file add line containing library(StatCompLab) Save file Git pane, press Commit select changed file (tick mark appear indicating file Staged, display also shows changed; can also done pressing Commit) Enter informative “Commit message”, e.g. “Load StatCompLab” Press “Commit”, soon indicate success Press “Push” push changes github, , asked, enter github login credentials Go repository github (https://github.com/StatComp21/) click lab02_code.R see change included","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"optimisation-exploration","dir":"Articles","previous_headings":"","what":"Optimisation exploration","title":"Tutorial 02: Optimization (solutions)","text":"optimisation methods discussed lecture (Gradient Descent, Newton Quasi-Newton method BFGS, see Lecture 2) based starting current best estimate minimum, finding search direction, performing line search find new, better, estimate. Nelder-Mead Simplex method works similar way, doesn’t use derivatives target function; uses function evaluations, keeps track set local points (\\(m+1\\) points, \\(m\\) dimension problem). 2D problems method updates triangle points. iteration, attempts move away “worst” point, performing simple line search. addition basis line search, reduce expand triangle. Expansion happens similarly adaptive step length Gradient Descent method. Start optimisation shiny app code Console: “Simple (1D)” “Simple (2D)” functions, familiarise “Step”, “Converge”, “Reset” buttons. Choose different optimisation starting points clicking figure. Explore different optimisation methods display figure optimisation step. simplex/triangle shapes shown “Simplex” method step blue. “best” points simplex connected (magenta). Newton methods display true quadratic Taylor approximations (contours red) well approximations used find proposed steps (contours blue). Also observe diagnostic output box number function, gradient, Hessian evaluations differ methods. “Rosenbrock (2D)” function, observe differences convergence behaviour four different optimisation methods. \\(m\\)-dimensional problems derivatives approximated finite differences, gradient calculation costs least \\(m\\) extra function evaluations, Hessian costs least \\(2m^2\\) extra function evaluations. 2D Rosenbrock function, count total number function evaluations required optimisation method (assumption finite differences used gradient Hessian) convergence reached. Hint: find “Evaluations:” information app. may need use “Converge” button multiple times, since 500 iterations time. Also note total number iterations needed method. compare? Answer: \\(m=2\\), number function evaluations given #f + 2 * #gradient + 8 * #hessian (2 extra function evaluations needed first order differences gradient, 8 extra points needed second order differences needed second order derivatives; ’ll revisit week 9) Nelder-Mead: \\(202\\) (103 iterations) Gradient Descent: \\(14132+2\\cdot 9405 = 3.2942\\times 10^{4}\\) (9404 iterations) Newton: \\(29+2\\cdot 23 +8\\cdot 22 = 251\\) (22 iterations) BFGS: \\(54+2\\cdot 41 +8\\cdot 1 = 144\\) (40 iterations) BFGS uses smallest number function evaluations, makes faster “exact” Newton method, despite needing almost twice many iterations. Thus, normally consider using full Newton method efficient way obtaining Hessian finite differences. Gradient Descent extremely badly test problem; clearly, using approximate information second order derivatives can provide much better search directions step lengths using higher order information. Simplex method doesn’t use higher order information, due local “memory” can still competetive; test case, outperforms Newton method terms cost, BFGS method. “Multimodal” functions, explore optimisation methods behave different starting points. far can optimisation start “Spiral” function? E.g., try “Newton” method, starting top right corner figure.","code":"StatCompLab::optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Tutorial 02: Optimization (solutions)","text":"function R can name, arguments (parameters), return value. function defined expressions one lines code. result last evaluated expression function value returned caller. input parameter can default value, caller doesn’t specify unless want different value. Sometimes, NULL default value used, parameter checked internally .null(parameter) determine user supplied something. good practice refer parameters name calling function (instead relying order parameters; first parameter common exception practice), especially parameters default values. Example: main optimisation function R optim(), following call syntax: , method parameter appears whole vector default value. However, merely way show user permitted values. user supply anything specific, first value, \"Nelder-Mead\" used. Read documentation ?optim see different arguments mean can used. may briefly encountered special ... parameter syntax earlier lab. means may additional parameters specified caller, passed another function called internally. optim(), extra parameters passed target function user supplies. call optim() minimise function defined myTargetFunction() might take form: optim() uses function myTargetFunction(), called like :","code":"thename <- function(arguments) {   expressions } my_function <- function(param1, param2 = NULL, param3 = 4) {   # 'if': run a block of code if a logical statement is true   if (is.null(param2)) {     param1 + param3   } else {     # 'else', companion to 'if':     #    run this other code if the logical statement wasn't true     param1 + param2 / param3   } } my_function(1) ## [1] 5 my_function(1, param3 = 2) ## [1] 3 my_function(1, param2 = 8, param3 = 2) ## [1] 5 my_function(1, param2 = 8) ## [1] 3 optim(par, fn, gr = NULL, ...,       method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\",                  \"Brent\"),       lower = -Inf, upper = Inf,       control = list(), hessian = FALSE) opt <- optim(par = start_par,              fn = myTargetFunction,              extra1 = value1,              extra2 = value2) myTargetFunction(current_par, extra1 = value1, extra2 = value2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"estimating-a-complicated-model","dir":"Articles","previous_headings":"Functions","what":"Estimating a complicated model","title":"Tutorial 02: Optimization (solutions)","text":"example, ’ll look model connection  values \\((x_1,\\dots,x_n)\\) observations \\(y_i\\) (see Lecture 2) can written \\[ \\begin{aligned} &\\\\{1, 2, \\dots, n\\} \\\\ x_i &= \\frac{-1}{n-1} \\\\ \\boldsymbol{X} &= \\begin{bmatrix}\\boldsymbol{1} & \\boldsymbol{x}\\end{bmatrix} \\\\ \\boldsymbol{\\mu} &= \\boldsymbol{X} \\begin{bmatrix}\\theta_1 \\\\ \\theta_2\\end{bmatrix} \\\\ \\log(\\sigma_i) &= \\theta_3 + x_i \\theta_4 \\\\ y_i &= \\mathsf{N}(\\mu_i, \\sigma_i^2), \\quad\\text{(independent)} . \\end{aligned} \\] Use following code simulate synthetic random data model, mean standard deviation observation depends two model parameters via common set covariates. Plot data.  Write function neg_log_lik <- function(theta, y, X) {(Code goes )} evaluates negative log-likelihood model. See ?dnorm. Solution: aid help text optim(), find maximum likelihood parameter estimates statistical model using BFGS method numerical derivatives. Use \\((0,0,0,0)\\) starting point optimisation. Solution: Check ?optim help text information result object contains. optimisation converge? Solution: Answer: optimisation converged opt$convergence 0. least optim() thinks coverged, since triggered ’s convergence tolerances. Compute store estimated expectations \\(\\sigma\\) values like : estimates \\(\\sigma_i\\) function \\(x_i\\) look similar :  expression(sigma) y-axis label way making R try interpret R expression format like mathematical expression, greek letters, etc. example, ylab(expression(theta[3] + x[] * theta[4])) formatted \\(\\theta_3+x_i\\theta_4\\).","code":"n <- 100 theta_true <- c(2, -2, -2, 3) X <- cbind(1, seq(0, 1, length.out = n)) y <- rnorm(n = n,            mean = X %*% theta_true[1:2],            sd = exp(X %*% theta_true[3:4])) library(ggplot2) theme_set(theme_bw()) ggplot(data.frame(x = X[, 2], y = y)) +   geom_point(aes(x, y)) neg_log_lik <- function(theta, y, X) {   -sum(dnorm(x = y,              mean = X %*% theta[1:2],              sd = exp(X %*% theta[3:4]),              log = TRUE)) } opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik,              y = y, X = X,              method = \"BFGS\") data <- data.frame(x = X[, 2],                    y = y,                    expectation_true = X %*% theta_true[1:2],                    sigma_true = exp(X %*% theta_true[3:4]),                    expectation_est = X %*% opt$par[1:2],                    sigma_est = exp(X %*% opt$par[3:4])) ggplot(data) +   geom_line(aes(x, sigma_true)) +   geom_line(aes(x, sigma_est), col = \"red\") +   xlab(\"x\") +   ylab(expression(sigma))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial02Solutions.html","id":"data-wrangling","dir":"Articles","previous_headings":"Functions","what":"Data wrangling","title":"Tutorial 02: Optimization (solutions)","text":"help  produce proper figure labels, can use data wrangling methods set R packages commonly referred tidyverse. idea convert data frame type output separate data columns format values true estimated expectations standard deviations stored single column, , new columns encode row contains. , collected true estimated expectations standard deviations single column value, introduced new columns property type, containing strings (characters) indicating “expectation”/“sigma” “est”/“true”. Take look original new data data_long objects see differ. can now plot results single ggplot call:  function facet_wrap splits plot two parts; one based data data_long rows property “expectation”, one based data data_long rows property “sigma”. colour chosed based type, shown common legend whole plot. Rerun optimisation extra parameter hessian = TRUE, obtain numeric approximation Hessian target function optimum, compute inverse (see ?solve), estimate covariance matrix error parameter estimates. future Computer Lab use evaluate approximate confidence prediction intervals. Solution:","code":"suppressPackageStartupMessages(library(tidyverse)) data_long <-   data %>%   pivot_longer(cols = -c(x, y),                values_to = \"value\",                names_to = c(\"property\", \"type\"),                names_pattern = \"(.*)_(.*)\") data_long ## # A tibble: 400 × 5 ##         x     y property    type  value ##     <dbl> <dbl> <chr>       <chr> <dbl> ##  1 0       1.92 expectation true  2     ##  2 0       1.92 sigma       true  0.135 ##  3 0       1.92 expectation est   2.01  ##  4 0       1.92 sigma       est   0.122 ##  5 0.0101  1.95 expectation true  1.98  ##  6 0.0101  1.95 sigma       true  0.139 ##  7 0.0101  1.95 expectation est   1.99  ##  8 0.0101  1.95 sigma       est   0.126 ##  9 0.0202  2.18 expectation true  1.96  ## 10 0.0202  2.18 sigma       true  0.144 ## # … with 390 more rows ggplot(data_long, aes(x, value)) +   geom_line(aes(col = type)) +   facet_wrap(vars(property), ncol=1) opt <- optim(par = c(0, 0, 0, 0),              fn = neg_log_lik, y = y, X = X,              method = \"BFGS\",              hessian = TRUE) covar <- solve(opt$hessian) covar ##               [,1]         [,2]          [,3]         [,4] ## [1,]  0.0017940969 -0.005724814 -0.0005605767  0.001121155 ## [2,] -0.0057248136  0.036111726  0.0035360846 -0.007072178 ## [3,] -0.0005605767  0.003536085  0.0190674071 -0.028134823 ## [4,]  0.0011211550 -0.007072178 -0.0281348225  0.056269649"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks. Solution: accompanying Tutorial03Solutions tutorial document contains solutions explicitly, make easier review material workshops. can also run document Tutorials pane RStudio.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms ","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined  \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~}  + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"lab session explore Monte Carlo integration importance sampling. Open github repository clone project Lab 2 (either https://rstudio.cloud computer upgrade StatCompLab package (see https://finnlindgren.github.io/StatCompLab/ details) Save work lab one several new files project, commit push changes github (see Lab 2 information) code script, start library(StatCompLab) get access arch_loglike function needed one tasks.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"overdispersed-poisson-distribution","dir":"Articles","previous_headings":"","what":"Overdispersed Poisson distribution","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"’ll use overdispersed Poisson distribution first example. ordinary Poisson distribution \\(Y\\sim\\mathsf{Poisson}(\\lambda)\\) expectation \\(\\lambda\\), variance also \\(\\lambda\\). coefficient variation defined ratio standard deviation expectation, gives \\(1/\\sqrt{\\lambda}\\). real applications, one often observes data coefficient variation larger Poisson model give. One way dealing add latent random effect; hidden layer random values “nudges” expectation observation, increases observed coefficient variation. model can written two steps: \\[ \\begin{aligned} X & \\sim \\mathsf{Normal}(0,\\sigma^2), \\\\ (Y|X=x) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x)], \\end{aligned} \\] conditional expectation \\(Y\\) given \\(X=x\\) \\(\\mu\\) \\(\\lambda(\\mu,x)=\\exp(\\mu+x)\\), \\(\\mu\\) (fixed) parameter. marginal expectation (still conditionally \\(\\mu\\) \\(\\sigma\\)) can obtained via tower property (law total expectation), \\[ \\begin{aligned} \\mathsf{E}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{E}(Y|X)] = \\mathsf{E}[\\lambda(\\mu,X)] = \\mathsf{E}[\\exp(\\mu+X)] = \\exp(\\mu + \\sigma^2/2) \\end{aligned} \\] last step comes expectation log-Normal distribution. multiple observations, can write model \\[ \\begin{aligned} x_i & \\sim \\mathsf{Normal}(0,\\sigma^2), \\text{ independent $=1,\\dots,n$,}\\\\ (y_i|x_i) & \\sim \\mathsf{Poisson}[\\exp(\\mu + x_i)], \\text{ conditionally independent $=1,\\dots,n$}. \\end{aligned} \\] Consider following simulation code: expectation \\(Y\\)? Theory exercise: Using tower property (law total variance), theoretically derive expression \\(\\mathsf{Var}(Y)\\). can get needed log-Normal variance expression https://en.wikipedia.org/wiki/Log-normal_distribution Solution: \\[ \\begin{aligned} \\mathsf{Var}(Y|\\mu,\\sigma) &= \\mathsf{E}[\\mathsf{Var}(Y|X)] + \\mathsf{Var}[\\mathsf{E}(Y|X)] \\\\&= \\mathsf{E}[\\lambda(\\mu,X)] + \\mathsf{Var}[\\lambda(\\mu,X)] \\\\&= \\mathsf{E}[\\exp(\\mu+X)] +\\mathsf{Var}[\\exp(\\mu+X)] \\\\&= \\exp(\\mu + \\sigma^2/2) + (\\exp(\\sigma^2)-1) \\exp(2\\mu + \\sigma^2) \\\\&= \\exp(\\mu + \\sigma^2/2) \\left[ 1 + (\\exp(\\sigma^2)-1) \\exp(\\mu + \\sigma^2/2) \\right] \\end{aligned} \\] Since second factor always \\(\\geq 1\\), shows variance never smaller expectation. Monte Carlo integration: Use Monte Carlo integration approximate probability mass function \\[ \\begin{aligned} p_Y(m|\\mu,\\sigma)=\\mathsf{P}(Y=m|\\mu,\\sigma) &= \\int_{-\\infty}^\\infty \\mathsf{P}(Y=m|\\mu,\\sigma,X=x) p_X(x|\\sigma) \\,\\mathrm{d}x \\end{aligned} \\] \\(m=0,1,2,3,\\dots,15\\), \\(\\mu=\\log(2)-1/2\\) \\(\\sigma=1\\). Check formulas resulting theoretical expectation \\(Y\\) parameter combination. efficient, vectorise calculations evaluating conditional probability function \\(Y\\) \\(m\\) values , simulated value \\(x^{[k]}\\sim\\mathsf{Normal}(0,\\sigma^2)\\) value, \\(k=1,2,\\dots,K\\). Use \\(K=10000\\) samples. Plot resulting probability mass function \\(p_Y(m|\\mu,\\sigma)\\) together ordinary Poisson probability mass function expectation value, \\(\\lambda = 2\\). (Use theory convince two models \\(Y\\) expectation.) Solution:  plot, overdispersed Poisson probability function (just computed) compared plain Poisson probability function expectation value. Lines values included clarity. Note: case, second approach faster, situations first approach required, since doesn’t require storing random numbers time, thus allowing much larger \\(K=n_{\\text{mc}}\\) value used. Reusable function: Generalise code function doverpois (“d”ensity -dispersed Poisson) takes m, mu, sigma, K input returns data.frame suitable use ggplot. Solution: Use function plot results \\(\\mu=\\log(8)-1/8\\), \\(\\sigma = 1/2\\), \\(m=0,1,\\dots,30\\) adding P_Y P_Poisson geoms Solution:","code":"Y <- rpois(30, lambda = 3 * exp(-2 + rnorm(30, sd = 2))) # Vectorised over m mu <- log(2) - 1/2 K <- 10000 m <- 0:15 P_Y <- numeric(length(m)) for (loop in seq_len(K)) {   x <- rnorm(1, sd = 1)   P_Y <- P_Y + dpois(m, lambda = exp(mu + x)) } P_Y <- P_Y / K  # Plot the results suppressPackageStartupMessages(library(ggplot2)) ggplot(data.frame(m = m,                   P_Y = P_Y,                   P_Poisson = dpois(m, lambda = 2))) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\")) doverpois <- function(m, mu, sigma, K) {   P_Y <- numeric(length(m))   for (loop in seq_len(K)) {     x <- rnorm(1, sd = sigma)     P_Y <- P_Y + dpois(m, lambda = exp(mu + x))   }   P_Y <- P_Y / K    data.frame(m = m,              P_Y = P_Y,              P_Poisson = dpois(m, lambda = exp(mu + sigma^2/2))) } ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000)) suppressPackageStartupMessages(library(ggplot2)) ggplot(doverpois(m = 0:30, mu = log(8)-0.125, sigma = 0.5, K = 10000)) +   geom_point(aes(m, P_Y, col = \"MC\")) +   geom_point(aes(m, P_Poisson, col = \"Poisson\")) +   geom_line(aes(m, P_Y, col = \"MC\")) +   geom_line(aes(m, P_Poisson, col = \"Poisson\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"Waldemar cross, source: Wikipedia ``Anno Domini MCCCLXI feria III post Jacobi ante portas Visby manibus Danorum ceciderunt Gutenses, hic sepulti, orate pro eis!’’  ``year Lord 1361, third day St. Jacob, Goth fell outside gates Visby hands Danish. buried . Pray !’’ Strategically located middle Baltic sea, island Gotland shifting periods partly self-governed, partial control Hanseatic trading alliance, Sweden, Denmark, Denmark-Norway-Sweden union, settling part Sweden 1645. Gotland abundance archaeological treasures, coins dating back Viking era trade routes via Russia Arab Caliphates. captured rich Hanseatic town Visby. 1361 Danish king Valdemar Atterdag conquered Gotland. conquest followed plunder Visby. defenders (primarily local farmers take shelter inside city walls) killed attack buried field, Korsbetningen (Literal translation: grazing field marked cross, shown picture), outside walls Visby. 1920s gravesite subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. want figure many persons likely buried gravesite. must reasonably least \\(256\\), many ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"statistical-model","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Statistical model","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"build simple model problem, assume number left (\\(y_1=256\\)) right (\\(y_2=237\\)) femurs two independent observations \\(\\mathsf{Bin}(N,\\phi)\\) distribution. \\(N\\) total number people buried \\(\\phi\\) probability finding femur, left right, \\(N\\) \\(\\phi\\) unknown parameters. probability function single observation \\(y\\sim\\mathsf{Bin}(N,\\phi)\\) \\[\\begin{align*} p(y|N,\\phi) &= {N \\choose y} \\phi^y (1-\\phi)^{N-y} . \\end{align*}\\] function arch_loglike() StatCompLab package evaluates combined log-likelihood \\(\\log[p(\\boldsymbol{y}|N,\\phi)]\\) collection \\(\\boldsymbol{y}\\) \\(y\\)-observations. data.frame columns N phi provided, log-likelihood row-pair \\((N,\\phi)\\) returned. combined  \\(l(y_1,y_2|N,\\theta)=\\log p(y_1,y_2|N,\\phi)\\) data set \\(\\{y_1,y_2\\}\\) given \\[ \\begin{aligned} l(y_1,y_2|N,\\theta) &= -\\log\\Gamma(y_1+1) - \\log\\Gamma(y_2+1) \\\\&\\phantom{=~} - \\log\\Gamma(N-y_1+1) - \\log\\Gamma(N-y_2+1) + 2\\log\\Gamma(N+1) \\\\&\\phantom{=~}  + (y_1+y_2) \\log(\\phi) + (2 N - y_1 - y_2)\\log(1-\\phi) \\end{aligned} \\] task use Monte Carlo integration estimate posterior expectations \\(N\\) \\(\\phi\\), prior distributions \\(N\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\). mathematical definitions : Let \\(N\\) \\(\\mathsf{Geom}(\\xi)\\), \\(\\xi>0\\), prior distribution, let \\(\\phi\\) \\(\\mathsf{Beta}(,b)\\), \\(,b>0\\), prior distribution: \\[ \\begin{aligned} p_N(n) = \\mathsf{P}(N=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\[0,1] . \\end{aligned} \\] probability mass function \\(p_N(n)\\) can evaluated dgeom() R, density \\(p_\\phi(\\phi)\\) can evaluated dbeta.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"bayesian-estimation","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Bayesian estimation","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"excavation took place, archaeologist believed around \\(1000\\) individuals buried, find around half femurs. encode belief Bayesian analysis, set \\(\\xi=1/(1+1000)\\), corresponds expected total count \\(1000\\), \\(=b=2\\), makes \\(\\phi\\) likely close \\(1/2\\) \\(0\\) \\(1\\). posterior density/probability function \\((N,\\phi|\\boldsymbol{y})\\) \\[ \\begin{aligned} p_{N,\\phi|\\boldsymbol{y}}(n,\\phi|\\boldsymbol{y}) &= \\frac{p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y})}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} = \\frac{p_N(n)p_\\phi(\\phi)p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} \\end{aligned} \\] can estimate \\(p_{\\boldsymbol{y}}(\\boldsymbol{y})\\) Monte Carlo integration, time using importance sampling estimate posterior expectations. theoretical expressions \\[ \\begin{aligned} p_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p_{N,\\phi,\\boldsymbol{y}}(n,\\phi,\\boldsymbol{y}) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 p(\\boldsymbol{y}|n,\\phi) p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(N|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 n\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi\\\\ \\mathsf{E}(\\phi|\\boldsymbol{y}) &= \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,p_{N,\\phi|\\boldsymbol{y}}(n,\\phi) \\,\\mathrm{d}\\phi = \\sum_{n=\\max(y_1,y_2)}^\\infty \\int_0^1 \\phi\\,\\frac{p(\\boldsymbol{y}|n,\\phi)}{p_{\\boldsymbol{y}}(\\boldsymbol{y})} p_N(n) p_\\phi(\\phi) \\,\\mathrm{d}\\phi \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial03Solutions.html","id":"monte-carlo-integration","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Monte Carlo integration","title":"Tutorial 03: Monte Carlo integration (solutions)","text":"might good choice sampling distribution ? first approach problem, let’s use prior distributions sampling distributions Monte Carlo integration. means need sample \\(n^{[k]}\\sim\\mathsf{Geom}(\\xi)\\) \\(\\phi^{[k]}\\sim\\mathsf{Beta}(,b)\\), compute Monte Carlo estimates \\[ \\begin{aligned} \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y}) &= \\frac{1}{K}   \\sum_{k=1}^K p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(N|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K n^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\widehat{\\mathsf{E}}(\\phi|\\boldsymbol{y}) &=   \\frac{1}{K \\widehat{p}_{\\boldsymbol{y}}(\\boldsymbol{y})}   \\sum_{k=1}^K \\phi^{[k]} p(\\boldsymbol{y}|n^{[k]},\\phi^{[k]})\\\\ \\end{aligned} \\] Write function estimate takes inputs y, xi, , b, K implements Monte Carlo integration method. Test function running estimate(y=c(237,256), xi=1/1001, =0.5, b=0.5, K=10000) Solution: Note: shown lecture 3, conditional posterior distribution \\(\\phi\\) fixed \\(N=n\\) \\((\\phi|n,\\boldsymbol{y})\\sim \\mathsf{Beta}(+y_1+y_2,b+2n-y_1-y_2)\\). can used construct potentially efficient method, \\(\\phi^{[k]}\\) sampled conditionally \\(n^{[k]}\\), integration importance weights adjusted appropriately.","code":"estimate <- function(y, xi, a, b, K) {   samples <- data.frame(     N = rgeom(K, prob = xi),     phi = rbeta(K, shape1 = a, shape2 = b)   )   loglike <- arch_loglike(param = samples, y = y)   p_y <- mean(exp(loglike))   c(p_y = p_y,     E_N = mean(samples$N * exp(loglike) / p_y),     E_phi = mean(samples$phi * exp(loglike) / p_y)) } estimate(y = c(237, 256), xi = 1/1001, a = 0.5, b = 0.5, K = 10000) ##          p_y          E_N        E_phi  ## 6.360064e-06 9.884083e+02 3.694532e-01"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk. Solution: accompanying Tutorial04Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. separate T4sol.Rmd document https://github.com/finnlindgren/StatCompLab/blob/main/vignettes/articles/T4sol.Rmd source document standalone solution shown T4sol StatCompLab website.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration","text":"Consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] week 4 lecture, two parameterisations considered. now add third option: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) week 4 lecture, know inverse expected Fisher information \\(\\lambda/n\\) case 1 \\(1/(4n)\\) case 2. case 3, show inverse expected Fisher information \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration","text":"Use approximation method large \\(n\\) lecture construct approximate confidence intervals \\(\\lambda\\) using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values \\(\\overline{y}\\). Experiment different values n lambda simulation y. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; \\(\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha\\) \\(\\theta\\) (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration","text":"Assume true value \\(\\lambda=10\\), simulate sample \\(\\boldsymbol{y}\\) size \\(n=5\\). Now consider Bayesian version Poisson model, prior model \\[ \\lambda \\sim \\mathsf{Exp}() \\] probability density function \\(p(\\lambda) = \\exp(-\\lambda)\\). One can show exact posterior distribution \\(\\lambda\\) given \\(\\boldsymbol{y}\\) \\(\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n)\\) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let \\(\\theta=\\log(\\lambda)\\), \\(\\lambda=\\exp(\\theta)\\). Show prior probability density \\(\\theta\\) \\(p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration","text":"posterior density function \\(\\theta\\) \\[ p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})} \\] log-density \\[ \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) , \\] taking derivatives find mode \\(\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right)\\), negated Hessian \\(1+n\\overline{y}\\) mode. information can construct Gaussian approximation posterior distribution, \\(\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}})\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration","text":"Simulate sample \\(\\boldsymbol{x}=\\{x_1,\\dots,x_m\\}\\) Gaussian approximation posterior distribution, large \\(m > 10000\\), hyperparameter \\(=1/5\\). need calculate unnormalised importance weights \\(w_k\\), \\(k=1,\\dots,m\\), \\[ w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} . \\] Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, \\(\\log(w_k)\\), new, equivalent unnormalised weights \\(\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]\\). Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval \\(\\theta\\) using \\(\\boldsymbol{x}\\) sample associate weights, transform credible interval \\(\\lambda\\)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function \\(\\lambda\\) (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample \\(\\lambda_k=\\exp(x_k)\\), (unnormalised) weights \\(w_k\\). Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"lab session explore using RMarkdown organise text code maximum likelihood estimator sampling distributions approximate confidence interval construction Laplace approximation importance sampling approximate Bayesian credible interval construction Clone lab04-* repository https://github.com/StatComp21/ either computer (new Project version control) https://rstudio.cloud rstudio.cloud, setup GITHUB_PAT credentials, like . Upgrade/install StatCompLab package, see https://finnlindgren.github.io/StatCompLab/ repository two files, RMDemo.Rmd my_code.R. Make copy RMDemo.Rmd, call Lab4.Rmd lab, modify Lab4.Rmd document add new code text commentary lab document. (can remove demonstration parts file don’t need anymore, /keep separate copy .) pressing “knit” button, RMarkdown file run R environment, need include needed library() calls code chnk file, normally initial “setup” chunk.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] week 4 lecture, two parameterisations considered. now add third option: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) week 4 lecture, know inverse expected Fisher information \\(\\lambda/n\\) case 1 \\(1/(4n)\\) case 2. case 3, show inverse expected Fisher information \\(1/(n\\lambda)\\). Solution: case 3, negated log-likelihood \\[ \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant} \\] 1st order derivative \\[ \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i \\] shows \\(\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y})\\), 2nd order derivative \\[ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta \\] equal \\(n\\lambda\\) \\(y_i\\) values, inverse expected Hessian \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Use approximation method large \\(n\\) lecture construct approximate confidence intervals \\(\\lambda\\) using three parameterisations. Define three functions, CI1, CI2, CI3, taking paramters y: vector observed values alpha: nominal error probability confidence intervals avoid specify alpha common case, can use alpha = 0.05 function argument definition set default value. function pmax may useful (see help text). Solution: can use following code test functions, storing interval row matrix rbind (“bind” “rows”, see also cbind combining columns): can print result table RMarkdown using separate codechunk, calling knitr::kable function: three methods always produce valid interval? Consider possible values \\(\\overline{y}\\). Experiment different values n lambda simulation y. Solution: \\(\\overline{y}=0\\), first method produces single point “interval”. third method fails \\(\\overline{y}=0\\), due log zero. can happen \\(n\\) \\(\\lambda\\) close zero. approximate confidence interval construction method, might ask question whether fulfils definition actual confidence interval construction method; \\(\\mathsf{P}_{\\boldsymbol{y}|\\theta}(\\theta\\\\text{CI}(\\boldsymbol{y})|\\theta)\\geq 1-\\alpha\\) \\(\\theta\\) (least relevant subset parameter space). coursework project 1, investigate accuracy approximate confidence interval construction methods.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) ## [1] 0 2 2 2 4 CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\") knitr::kable(CI)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Assume true value \\(\\lambda=10\\), simulate sample \\(\\boldsymbol{y}\\) size \\(n=5\\). Solution: Now consider Bayesian version Poisson model, prior model \\[ \\lambda \\sim \\mathsf{Exp}() \\] probability density function \\(p(\\lambda) = \\exp(-\\lambda)\\). One can show exact posterior distribution \\(\\lambda\\) given \\(\\boldsymbol{y}\\) \\(\\mathsf{Gamma}(1 + \\sum_{=1}^n y_i, + n)\\) distribution (using shape&rate parameterisation), credible intervals can constructed quantiles distribution. cases theoretical construction impractical, alternative instead construct samples posterior distribution, extract empirical quantiles sample. , use importance sampling achieve . Let \\(\\theta=\\log(\\lambda)\\), \\(\\lambda=\\exp(\\theta)\\). Show prior probability density \\(\\theta\\) \\(p(\\theta)=\\exp\\left( \\theta-ae^\\theta \\right)\\).","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"gaussian-approximation","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Gaussian approximation","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Solution: Alternative 1: probability theory, know \\(p(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}\\), \\[ p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right) \\] Alternative 2: First, derive CDF: \\(\\mathsf{P}(\\theta \\leq x)=\\mathsf{P}(\\log(\\lambda) \\leq x)=\\mathsf{P}(\\lambda \\leq e^x) = 1 - \\exp(-e^x)\\), CDF Exponential distribution. Taking derivative respect \\(x\\) gives density \\(p_\\theta(x)=-\\exp(-e^x) \\frac{d}{dx} (-e^x) = \\exp(-e^x) (e^x) = \\exp(x - e^x)\\), needed result. posterior density function \\(\\theta\\) \\[ p(\\theta|\\boldsymbol{y}) = \\frac{p(\\theta) p(\\boldsymbol{y}|\\theta)}{p(\\boldsymbol{y})} \\] log-density \\[ \\log p(\\theta|\\boldsymbol{y}) = \\text{const} + \\theta (1 + n\\overline{y}) - (+n)\\exp(\\theta) , \\] taking derivatives find mode \\(\\widetilde{\\theta}=\\log\\left(\\frac{1+n\\overline{y}}{+n}\\right)\\), negated Hessian \\(1+n\\overline{y}\\) mode. information can construct Gaussian approximation posterior distribution, \\(\\widetilde{p}(\\theta|\\boldsymbol{y})\\sim\\mathsf{Normal}(\\widetilde{\\theta},\\frac{1}{1+n\\overline{y}})\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"Simulate sample \\(\\boldsymbol{x}=\\{x_1,\\dots,x_m\\}\\) Gaussian approximation posterior distribution, large \\(m > 10000\\), hyperparameter \\(=1/5\\). Solution: need calculate unnormalised importance weights \\(w_k\\), \\(k=1,\\dots,m\\), \\[ w_k = \\left.\\frac{p(\\theta)p(\\boldsymbol{y}|\\theta)}{\\widetilde{p}(\\theta|\\boldsymbol{y})}\\right|_{\\theta=x_k} . \\] Due lack normalisation, “raw” weights represented accurately computer. get around issue, first compute logarithm weights, \\(\\log(w_k)\\), new, equivalent unnormalised weights \\(\\widetilde{w}_k=\\exp[\\log(w_k) - \\max_j \\log(w_j)]\\). Solution: Look help text function wquantile (StatCompLab package, version 0.4.0) computes quantiles weighted sample, construct 95% credible interval \\(\\theta\\) using \\(\\boldsymbol{x}\\) sample associate weights, transform credible interval \\(\\lambda\\) Solution:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial04Solutions.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (solutions)","text":"ggplot, use geom_function plot theoretical posterior cumulative distribution function \\(\\lambda\\) (CDF Gamma distribution given , see pgamma()) compare approximation given importance sampling. stat_ewcdf() function StatCompLab used plot cdf weighted sample \\(\\lambda_k=\\exp(x_k)\\), (unnormalised) weights \\(w_k\\). Also include unweighted sample, stat_ecwf(). close approximations come true posterior distribution? Solution: importance sampling version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution \\(\\log(\\lambda)\\) excellent approximation even add importance sampling step.","code":"ggplot(data.frame(lambda = exp(x), weights = weights)) +   xlim(0, 20) + ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 06: Prediction assessment with proper scores","text":"lab session explore Probabilistic prediction proper scores Open github repository clone project Tutorial 2 4 (either https://rstudio.cloud computer, upgrade StatCompLab package. lab, can either work .R file new .Rmd file. Solution: accompanying Tutorial06Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"d-printer","dir":"Articles","previous_headings":"","what":"3D printer","title":"Tutorial 06: Prediction assessment with proper scores","text":"aim build assess statistical models material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator wants know two models, named B, better capturing distributions random deviations. models use linear model coneection CAD_Weight Actual_Weight. denote CAD weight observations \\(\\) x_i, corresponding actual weight \\(y_i\\). two models defined Model : \\(y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3 + \\theta_4 x_i)]\\) Model B: \\(y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3) + \\exp(\\theta_4) x_i^2)]\\) printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, leads model B approximation . basic physics assumption error CAD software calculation weight proportional weight . Model hand slightly mathematically convenient, motivation physics. Start loading data plot .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"estimate-and-predict","dir":"Articles","previous_headings":"","what":"Estimate and predict","title":"Tutorial 06: Prediction assessment with proper scores","text":"Next week, assess model predictions using cross validation, data split separate parts parameter estimation prediction assessment, order avoid reduce bias prediction assessments. simplicity week, start using entire data set parameter estimation prediction assessment.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"estimate","dir":"Articles","previous_headings":"Estimate and predict","what":"Estimate","title":"Tutorial 06: Prediction assessment with proper scores","text":"First, use filament1_estimate() StatCompLab package estimate two models B using filament1 data. See help text information.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"prediction","dir":"Articles","previous_headings":"Estimate and predict","what":"Prediction","title":"Tutorial 06: Prediction assessment with proper scores","text":"Next, use filament1_predict() compute probabilistic predictions Actual_Weight using two estimated models. Inspect predictions drawing figures, e.g. geom_ribbon(aes(CAD_Weight, ymin = lwr, ymax = upr), alpha = 0.25) (alpha transparency), together observed data. can useful join predictions data new data.frame object get access prediction information data plotting.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"prediction-scores","dir":"Articles","previous_headings":"","what":"Prediction Scores","title":"Tutorial 06: Prediction assessment with proper scores","text":"Now, use score calculator function proper_score() StatCompLab package compute squared error, Dawid-Sebastiani, Interval scores (target coverage probability 90%). ’s useful joint prediction information data set cbind, e.g. mutate() can access needed information. basic summary results, compute average score \\(\\overline{S}(\\{F_i,y_i\\})\\) model type score, present result knitr::kable(). scores indicate one models better worse ? three score types agree ?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06.html","id":"data-splitting","dir":"Articles","previous_headings":"","what":"Data splitting","title":"Tutorial 06: Prediction assessment with proper scores","text":"Now, use sample() function generate random selection rows data set, split two parts, ca 50% used parameter estimation, 50% used prediction assessment. Redo previous analysis problem using division data. score results agree previous results?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"lab session explore Probabilistic prediction proper scores Open github repository clone project Tutorial 2 4 (either https://rstudio.cloud computer, upgrade StatCompLab package. lab, can either work .R file new .Rmd file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"d-printer","dir":"Articles","previous_headings":"","what":"3D printer","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"aim build assess statistical models material use 3D printer.1 printer uses rolls filament gets heated squeezed moving nozzle, gradually building objects. objects first designed CAD program (Computer Aided Design), also estimates much material required print object. data can loaded data(\"filament1\", package = \"StatCompLab\"), contains information one 3D-printed object per row. columns Index: observation index Date: printing dates Material: printing material, identified colour CAD_Weight: object weight (gram) CAD software calculated Actual_Weight: actual weight object (gram) printing CAD system printer perfect, CAD_Weight Actual_Weight values equal object. reality, random variation, example due varying humidity temperature, systematic deviations due CAD system perfect information properties printing materials. looking data (see ) ’s clear variability data larger larger values CAD_Weight. printer operator wants know two models, named B, better capturing distributions random deviations. models use linear model coneection CAD_Weight Actual_Weight. denote CAD weight observations \\(\\) x_i, corresponding actual weight \\(y_i\\). two models defined Model : \\(y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3 + \\theta_4 x_i)]\\) Model B: \\(y_i \\sim \\mathsf{Normal}[\\theta_1 + \\theta_2 x_i, \\exp(\\theta_3) + \\exp(\\theta_4) x_i^2)]\\) printer operator reasons due random fluctuations material properties (density) room temperature lead relative error instead additive error, leads model B approximation . basic physics assumption error CAD software calculation weight proportional weight . Model hand slightly mathematically convenient, motivation physics. Start loading data plot . Solution:","code":"data(\"filament1\", package = \"StatCompLab\") suppressPackageStartupMessages(library(tidyverse)) ggplot(filament1, aes(CAD_Weight, Actual_Weight, colour = Material)) +   geom_point()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"estimate-and-predict","dir":"Articles","previous_headings":"","what":"Estimate and predict","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Next week, assess model predictions using cross validation, data split separate parts parameter estimation prediction assessment, order avoid reduce bias prediction assessments. simplicity week, start using entire data set parameter estimation prediction assessment.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"estimate","dir":"Articles","previous_headings":"Estimate and predict","what":"Estimate","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"First, use filament1_estimate() StatCompLab package estimate two models B using filament1 data. See help text information. Solution:","code":"fit_A <- filament1_estimate(filament1, \"A\") fit_B <- filament1_estimate(filament1, \"B\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"prediction","dir":"Articles","previous_headings":"Estimate and predict","what":"Prediction","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Next, use filament1_predict() compute probabilistic predictions Actual_Weight using two estimated models. Solution: Inspect predictions drawing figures, e.g. geom_ribbon(aes(CAD_Weight, ymin = lwr, ymax = upr), alpha = 0.25) (alpha transparency), together observed data. can useful join predictions data new data.frame object get access prediction information data plotting. Solution:  , geom_point call gets data input ensure data point plotted .","code":"pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) ggplot(rbind(cbind(pred_A, filament1, Model = \"A\"),              cbind(pred_B, filament1, Model = \"B\")),        mapping = aes(CAD_Weight)) +   geom_line(aes(y = mean, col = Model)) +   geom_ribbon(aes(ymin = lwr, ymax = upr, fill = Model), alpha = 0.25) +   geom_point(aes(y = Actual_Weight), data = filament1)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"prediction-scores","dir":"Articles","previous_headings":"","what":"Prediction Scores","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Now, use score calculator function proper_score() StatCompLab package compute squared error, Dawid-Sebastiani, Interval scores (target coverage probability 90%). ’s useful joint prediction information data set cbind, e.g. mutate() can access needed information. Solution: See next section compact alternative (first combining prediction information models, computing scores one go). basic summary results, compute average score \\(\\overline{S}(\\{F_i,y_i\\})\\) model type score, present result knitr::kable(). Solution: scores indicate one models better worse ? three score types agree ? Solution: squared error score doesn’t really care difference two models, since doesn’t directly involve variance model (paramter estimates mean different, much). two scores indicate model B better model .","code":"score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_AB <-   rbind(cbind(score_A, model = \"A\"),         cbind(score_B, model = \"B\")) %>%   group_by(model) %>%           summarise(se = mean(se),                     ds = mean(ds),                     interval = mean(interval)) knitr::kable(score_AB)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial06Solutions.html","id":"data-splitting","dir":"Articles","previous_headings":"","what":"Data splitting","title":"Tutorial 06: Prediction assessment with proper scores (solutions)","text":"Now, use sample() function generate random selection rows data set, split two parts, ca 50% used parameter estimation, 50% used prediction assessment. Solution: Redo previous analysis problem using division data. Solution: score results agree previous results? Solution: results random variability due smaller size estimation prediction sets, likely agree previous results.","code":"idx_est <- sample(filament1$Index,                   size = round(nrow(filament1) * 0.5),                   replace = FALSE) filament1_est <- filament1 %>% filter(Index %in% idx_est) filament1_pred <- filament1 %>% filter(!(Index %in% idx_est)) fit_A <- filament1_estimate(filament1_est, \"A\") fit_B <- filament1_estimate(filament1_est, \"B\") pred_A <- filament1_predict(fit_A, newdata = filament1_pred) pred_B <- filament1_predict(fit_B, newdata = filament1_pred) scores <-   rbind(cbind(pred_A, filament1_pred, model = \"A\"),         cbind(pred_B, filament1_pred, model = \"B\")) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   )  score_summary <- scores %>%   group_by(model) %>%   summarise(se = mean(se),             ds = mean(ds),             interval = mean(interval)) knitr::kable(score_summary)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 07: Data wrangling and cross validation","text":"lab session explore Data wrangling; restructuring data frames Graphical data exploration Cross validation Open github repository clone project Lab 2 4 lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.7.0 higher. Solution: accompanying Tutorial07Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops. Throughout tutorial, ’ll make use data wrangling plotting tools dplyr, magrittr, ggplot2 tidyverse packages, start code library(tidyverse) library(StatCompLab). Note exercises tutorial build , often one solution similar previous solutions, just additions /modifications. data wrangling functions used tutorial mutate add alter variables filter keep rows fulfilling given criteria group_by perform analyses within subgroups defined one variables summarise compute data summaries, usually subgroups select keep variables left_join join two data frames together, based common identifier variables pivot_wider split value variable new named variables based category variable pivot_longer gather several variables new single value variable, names stored new category variable %>%, “pipe” operator used glue sequence operations together feeding result operation first argument following function call head view first rows data frame. can useful debugging sequence “piped” expressions, placing last pipe sequence pull extract contents single variable Functions plotting, ggplot initialising plots geom_point drawing points facet_wrap splitting plot “facet” plots, based one category variables","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"Tutorial 07: Data wrangling and cross validation","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (google maps online map systems can usually interpret latitude longitude searches). ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm) values data object 502901 rows, don’t want try view whole object directly. Instead, can start summarising . Start counting many observations station , type measurement. shortest approach use count() function. generalisable approach use group_by(), summarise(), n() functions. See description ?count count() connected others. avoid create temporary named variables, end pipe operations call knitr::kable(), especially ’re working RMarkdown document.","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"exploratory-plotting","dir":"Articles","previous_headings":"","what":"Exploratory plotting","title":"Tutorial 07: Data wrangling and cross validation","text":", looked station data weather measurements separately. plotting, least like access station names instead identifying codes, give humanly readable presentation. can accomplished left_join() function, can add copies rows one data frame another, one columns match. Create new variable, ghcnd observation contains measurements station data: Now plot daily minimum maximum temperature measurements connected lines function time (DecYear), different colour element, separate subplot station (facet_wrap(~variablename)), labeled station names. , avoid creating temporary named variable. Instead, feed initial data wrangling result ggplot() directly. Due amount data, ’s difficult see clear patterns . Produce two figures, one showing yearly averages TMIN TMAX points, one showing monthly seasonal averages (months 1 12) TMIN TMAX, separately station. , avoid creating temporary named variable. previous code, insert calls group_by() summarise(), modify x-values aesthetics. common patterns yearly values, monthly seasonal values?","code":"ghcnd <- left_join(ghcnd_values, ghcnd_stations, by = \"ID\") head(ghcnd)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"scatter-plots","dir":"Articles","previous_headings":"Exploratory plotting","what":"Scatter plots","title":"Tutorial 07: Data wrangling and cross validation","text":"want scatter plot TMIN TMAX, need rearrange data bit. can use pivot_wider function, can turn name variable values variable several named variable. Note measurement elements present given day, NA’s produced default. Optionally, filter rows calling ggplot(). Draw scatterplot daily TMIN vs TMAX station, colour determined month.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation","text":"Choose one stations, create new data variable data ghcnd yearly averages TMIN column (previous pivot_wider output), missing values removed filter().","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"within-sample-assessment","dir":"Articles","previous_headings":"Cross validation","what":"Within-sample assessment","title":"Tutorial 07: Data wrangling and cross validation","text":"Now, using whole data estimate linear model TMIN, lm() formula TMIN ~ 1 + Year, compute average 80% Interval score (use proper_score() used lab 6) prediction intervals TMIN observations data. See ?predict.lm documentation predict() method models estimated lm().","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07.html","id":"cross-validation-1","dir":"Articles","previous_headings":"Cross validation","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation","text":"now want compute 5 average 80% Interval scores 5-fold cross validation based random partition data 5 approximately equal parts. First add new column Group data defining partitioning, using mutate(). One approach compute random permutation index vector, use modulus operator %% reduce 5 values, ceiling() scaled indices. loop partition groups, estimating model leaving group , predicting scoring predictions group. Compare resulting scores one based whole data set. average cross validation score larger smaller?","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"lab session explore Data wrangling; restructuring data frames Graphical data exploration Cross validation Open github repository clone project Lab 2 4 lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.7.0 higher. Throughout tutorial, ’ll make use data wrangling plotting tools dplyr, magrittr, ggplot2 tidyverse packages, start code library(tidyverse) library(StatCompLab). Note exercises tutorial build , often one solution similar previous solutions, just additions /modifications. data wrangling functions used tutorial mutate add alter variables filter keep rows fulfilling given criteria group_by perform analyses within subgroups defined one variables summarise compute data summaries, usually subgroups select keep variables left_join join two data frames together, based common identifier variables pivot_wider split value variable new named variables based category variable pivot_longer gather several variables new single value variable, names stored new category variable %>%, “pipe” operator used glue sequence operations together feeding result operation first argument following function call head view first rows data frame. can useful debugging sequence “piped” expressions, placing last pipe sequence pull extract contents single variable Functions plotting, ggplot initialising plots geom_point drawing points facet_wrap splitting plot “facet” plots, based one category variables","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"scottish-weather-data","dir":"Articles","previous_headings":"","what":"Scottish weather data","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Global Historical Climatology Network https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily provides historical weather data collected globe. subset daily resolution data set available StatCompLab package containing data eight weather stations Scotland, covering time period 1 January 1960 31 December 2018. measurements missing, either due instrument problems data collection issues. Load data ghcnd_stations data frame 5 variables: ID: identifier code station Name: humanly readable station name Latitude: latitude station location, degrees Longitude: longitude station location, degrees Elevation: station elevation, metres sea level station data set small enough can view whole thing, e.g. knitr::kable(ghcnd_stations). can try find locations map (google maps online map systems can usually interpret latitude longitude searches). Solution: ghcnd_values data frame 7 variables: ID: station identifier code observation Year: year value measured Month: month value measured Day: day month value measured DecYear: “Decimal year”, measurement date converted fractional value, whole numbers correspond 1 January, fractional values correspond later dates within year. useful plotting modelling. Element: One “TMIN” (minimum temperature), “TMAX” (maximum temperature), “PRCP” (precipitation), indicating value Value variable represents Value: Daily measured temperature (degrees Celsius) precipitation (mm) values data object 502901 rows, don’t want try view whole object directly. Instead, can start summarising . Start counting many observations station , type measurement. shortest approach use count() function. generalisable approach use group_by(), summarise(), n() functions. See description ?count count() connected others. avoid create temporary named variables, end pipe operations call knitr::kable(), especially ’re working RMarkdown document. Solution:","code":"data(ghcnd_stations, package = \"StatCompLab\") data(ghcnd_values, package = \"StatCompLab\") knitr::kable(ghcnd_stations) ghcnd_values %>%   count(ID, Element) %>%   knitr::kable()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"exploratory-plotting","dir":"Articles","previous_headings":"","what":"Exploratory plotting","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":", looked station data weather measurements separately. plotting, least like access station names instead identifying codes, give humanly readable presentation. can accomplished left_join() function, can add copies rows one data frame another, one columns match. Create new variable, ghcnd observation contains measurements station data: Now plot daily minimum maximum temperature measurements connected lines function time (DecYear), different colour element, separate subplot station (facet_wrap(~variablename)), labeled station names. , avoid creating temporary named variable. Instead, feed initial data wrangling result ggplot() directly. Solution:  Due amount data, ’s difficult see clear patterns . Produce two figures, one showing yearly averages TMIN TMAX points, one showing monthly seasonal averages (months 1 12) TMIN TMAX, separately station. , avoid creating temporary named variable. previous code, insert calls group_by() summarise(), modify x-values aesthetics. common patterns yearly values, monthly seasonal values? Solution: yearly averages seem slight increasing trend:  monthly seasonal averages clearly show seasonal pattern. shapes similar stations, average amplitude varies bit:","code":"ghcnd <- left_join(ghcnd_values, ghcnd_stations, by = \"ID\") head(ghcnd) ## # A tibble: 6 × 11 ##   ID           Year Month   Day DecYear Element Value Name    Latitude Longitude ##   <chr>       <int> <int> <int>   <dbl> <chr>   <dbl> <chr>      <dbl>     <dbl> ## 1 UKE00105874  1960     1     1   1960  TMAX      3.9 BRAEMAR     57.0     -3.40 ## 2 UKE00105874  1960     1     1   1960  TMIN      3.3 BRAEMAR     57.0     -3.40 ## 3 UKE00105874  1960     1     2   1960. TMAX      7.2 BRAEMAR     57.0     -3.40 ## 4 UKE00105874  1960     1     2   1960. TMIN     -8.3 BRAEMAR     57.0     -3.40 ## 5 UKE00105874  1960     1     3   1960. TMAX      6.7 BRAEMAR     57.0     -3.40 ## 6 UKE00105874  1960     1     3   1960. TMIN     -6.7 BRAEMAR     57.0     -3.40 ## # … with 1 more variable: Elevation <dbl> ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   ggplot(aes(DecYear, Value, colour = Element)) +   geom_line() +   facet_wrap(~ Name) ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   group_by(ID, Name, Element, Year) %>%   summarise(Value = mean(Value), .groups = \"drop\") %>%   ggplot(aes(Year, Value, colour = Element)) +   geom_point() +   facet_wrap(~ Name) ghcnd %>%   filter(Element %in% c(\"TMIN\", \"TMAX\")) %>%   group_by(ID, Name, Element, Month) %>%   summarise(Value = mean(Value), .groups = \"drop\") %>%   ggplot(aes(Month, Value, colour = Element)) +   geom_point() +   facet_wrap(~ Name)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"scatter-plots","dir":"Articles","previous_headings":"Exploratory plotting","what":"Scatter plots","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"want scatter plot TMIN TMAX, need rearrange data bit. can use pivot_wider function, can turn name variable values variable several named variable. Note measurement elements present given day, NA’s produced default. Optionally, filter rows calling ggplot(). Draw scatterplot daily TMIN vs TMAX station, colour determined month. Solution:","code":"ghcnd %>%   pivot_wider(names_from = Element, values_from = Value) %>%   filter(!is.na(TMIN) & !is.na(TMAX)) %>%   ggplot(aes(TMIN, TMAX, colour = factor(Month))) +   geom_point() +   facet_wrap(~ Name)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Choose one stations, create new data variable data ghcnd yearly averages TMIN column (previous pivot_wider output), missing values removed filter(). Solution:","code":"data <- ghcnd %>%   filter(ID == \"UKE00105875\") %>%   pivot_wider(names_from = Element, values_from = Value) %>%   filter(!is.na(TMIN)) %>%   group_by(ID, Name, Year) %>%   summarise(TMIN = mean(TMIN), .groups = \"drop\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"within-sample-assessment","dir":"Articles","previous_headings":"Cross validation","what":"Within-sample assessment","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"Now, using whole data estimate linear model TMIN, lm() formula TMIN ~ 1 + Year, compute average 80% Interval score (use proper_score() used lab 6) prediction intervals TMIN observations data. See ?predict.lm documentation predict() method models estimated lm(). Solution:","code":"fit0 <- lm(TMIN ~ 1 + Year, data = data) pred0 <- predict(fit0, newdata = data,                  interval = \"prediction\", level = 0.8) score0 <- mean(proper_score(   \"interval\", data$TMIN,   lwr = pred0[, \"lwr\"], upr = pred0[,\"upr\"], alpha = 0.8))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial07Solutions.html","id":"cross-validation-1","dir":"Articles","previous_headings":"Cross validation","what":"Cross validation","title":"Tutorial 07: Data wrangling and cross validation (solutions)","text":"now want compute 5 average 80% Interval scores 5-fold cross validation based random partition data 5 approximately equal parts. First add new column Group data defining partitioning, using mutate(). One approach compute random permutation index vector, use modulus operator %% reduce 5 values, ceiling() scaled indices. Solution: loop partition groups, estimating model leaving group , predicting scoring predictions group. Compare resulting scores one based whole data set. average cross validation score larger smaller? Solution: average cross validation score problem usually larger one whole data set; ’s intended reduce risk underestimating prediction error, expect. Repeat random group allocation see much influences cross validation score average.","code":"data <-    data %>%   mutate(Group = sample(seq_len(nrow(data)), size = nrow(data), replace = FALSE),          Group = (Group %% 5) + 1) # Alternative: # data <-  #  data %>% #  mutate(Group = sample(seq_len(nrow(data)), size = nrow(data), replace = FALSE), #         Group = ceiling(Group / nrow(data) * 5)) scores <- numeric(5) for (grp in seq_len(5)) {   fit <- lm(TMIN ~ 1 + Year, data = data %>% filter(Group != grp))   pred <- predict(fit, newdata = data %>% filter(Group == grp),                      interval = \"prediction\", level = 0.8)   scores[grp] <- mean(proper_score(     \"interval\",     (data %>% filter(Group == grp)) %>% pull(\"TMIN\"),     lwr = pred[, \"lwr\"], upr = pred[,\"upr\"], alpha = 0.8)) } knitr::kable(data.frame(\"Whole data score\" = score0,                         \"Cross validation score\" = mean(scores)))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 08: Floating point computations and least squares","text":"lab session explore Speed matrix computations Floating point accuracy least squares estimation statistical linear models Open github repository clone project Lab 2 4 upgrade StatCompLab package lab, can work .R file, working code chunks .Rmd highly recommended. measuring relative running times different methods, ’ll use bench package. won’t need load package library. Instead, check bench::mark() runs without errors. doesn’t, ’s package installed, install install.packages(\"bench\") R Console. Suggested code setup code chunk: Solution: accompanying Tutorial08Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":"suppressPackageStartupMessages(library(tidyverse)) library(StatCompLab)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"numerical-speed","dir":"Articles","previous_headings":"","what":"Numerical speed","title":"Tutorial 08: Floating point computations and least squares","text":"Define following function computing vector norm: Try simple vector can verify result correct, e.g. now take look computational speed matrix operations. First construct vector \\(\\boldsymbol{b}\\) matrix \\(\\boldsymbol{}\\): Let’s check speed difference \\(\\boldsymbol{}^\\top\\boldsymbol{b}\\) \\((\\boldsymbol{b}^\\top\\boldsymbol{})^\\top\\) (convince two expressions equivalent exact arithmetic, possibly also floating point arithmetic). function t() construct transpose matrix. Note b vector (opposed single-column matrix) interpreted either column vector row vector, depending operation. b vector, .matrix(b) always single-column matrix, t(b) single-row matrix, b %*% interpreted t(b) %*% . method computing \\(\\boldsymbol{}^\\top \\boldsymbol{b}\\) faster? Use bench::mark() compare different ways evaluating expression. extracting variables expression, median, itr/sec bench::mark() output using select() function, can summarise results table. save typing, define function bench_table kable printing: \\(\\boldsymbol{}\\boldsymbol{B}\\boldsymbol{b}\\), order matrix&vector multiplication done important. Define new variables: Compare running times %*% B %*% b, (%*% B) %*% b, %*% (B %*% b)). three versions produce similar numerical results (bench::mark complain don’t) method fastest? ? computed results ? square matrix \\(\\boldsymbol{}\\) size \\(1000\\times 1000\\), compare cost solve() %*% b (Get matrix inverse \\(\\boldsymbol{}^{-1}\\), multiply \\(\\boldsymbol{b}\\)) cost solve(, b) (Solve linear system \\(\\boldsymbol{}\\boldsymbol{u}=\\boldsymbol{b}\\)).","code":"vec_norm <- function(x) {   sum(x^2)^0.5 } c(vec_norm(c(1, 2, 3)), sqrt(1 + 4 + 9)) ## [1] 3.741657 3.741657 n <- 100000 m <- 100 b <- rnorm(n) A <- matrix(rnorm(n * m), n, m) bench_table <- function(bm) {   knitr::kable(     bm %>%       select(expression, median, `itr/sec`)   ) } m <- 1000 n <- 1000 p <- 1000 A <- matrix(rnorm(m * n), m, n) B <- matrix(rnorm(n * p), n, p) b <- rnorm(p)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"numerical-least-squares-estimation","dir":"Articles","previous_headings":"","what":"Numerical Least Squares estimation","title":"Tutorial 08: Floating point computations and least squares","text":"’ll now investigate numerical issues numerical least squares estimation. Run following code generates synthetic observations linear model \\[ \\begin{aligned} y_i &= \\frac{x_i-100.5}{5} + (x_i-100.5)^2 + e_i, \\end{aligned} \\] \\(e_i \\sim \\mathsf{Normal}(0, \\sigma_e=0.1)\\), independent, \\(=1,\\dots,n=100\\). Let’s plot data stored data.frame: true values \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) standardised model formulation, \\[ \\begin{aligned} y_i &= \\mu_i + e_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i ? \\end{aligned} \\] Hint: Identify values matching terms expanding initial model definition . Create beta_true vector containing \\((\\beta_0, \\beta_1, \\beta_2)\\): Use \\(\\beta\\)-values plot quadratic true model predictor function function \\(x\\[100,101]\\), together observations, estimate provided lm(). Since observed \\(x\\) values relatively dense, don’t necessarily need special plot sequence \\(x\\) values, can reuse observed values, model predictions value can obtained fitted() method. lm() formula specification, use + (x^2) quadratic term. tells lm() first compute x^2, use result new covariate. () method can interpreted “use computed value instead possible special meanings expression”. Use model.matrix(mod1) function extract \\(\\boldsymbol{X}\\) matrix vector formulation model, \\(\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{e}\\), store X1. (See ?model.matrix information). use direct normal equations solve shown Lecture 8, \\((\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\), estimate \\(\\beta\\) parameters. work?","code":"## Set the \"seed\" for the random number sequences, so we can ## reproduce exactly the same random numbers every time we run the code set.seed(1) ## Simulate the data # x: 100 random numbers between 100 and 101 data <- data.frame(x = 100 + sort(runif(100))) # y: random variation around a quadratic function: data <- data %>%   mutate(y = (x - 100.5) / 5 + (x - 100.5)^2 + rnorm(n(), sd = 0.1)) ggplot(data) + geom_point(aes(x, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08.html","id":"shifted-covariate","dir":"Articles","previous_headings":"","what":"Shifted covariate","title":"Tutorial 08: Floating point computations and least squares","text":"whole model shifted right, given \\(x+1000\\) instead original \\(x\\)-values? model expression still able represent quadratic expression different \\(\\beta\\) values. Create new data set: Estimate model using lm() new data set. work? default lm() allow singular models, “fix” problem removing co-linear columns model matrix. can ask disallow singular models singular.ok argument. Define function cond_nr() taking matrix X input returning condition number, computed help svd (see Lecture 8 code example condition number). help model.matrix, examine condition numbers model matrices two cases previous tasks. lm() computes fit using QR decomposition approach, direct solution normal equations. second lm() fit bad? Plot second third columns model matrix X2 , use cor examine correlation (see ?cor). explain large condition number? Since linear model says simply expected value vector \\(\\mathsf{E}(\\boldsymbol{y})\\) lies space spanned columns \\(\\boldsymbol{X}\\), one possibility attempt arrive better conditioned \\(\\boldsymbol{X}\\) linear rescaling /recombination columns. always equivalent linear re-parameterization. Try model matrix model causes lm() fail. particular, column (except intercept column) subtract column mean (e.g., try sweep() colMeans() functions, see example code ). divide column (except intercept column) standard deviation. Find condition number new model matrix. Fit model model matrix using something like mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) Produce plot confirms resulting fit sensible now. Note: data split estimation test sets, transformation must applied test data, using scaling parameters derived observation data. Therefore scale() function isn’t useful . Instead, explicitly computing storing transformation information necessary. Compute condition numbers X2 X3. Also compute correlation column 2 3 X3 (subtract \\(1\\) see close \\(1\\) ). subtracting rescaling help? alternative fix subtract mean x value original x vector defining quadratic model. Try see happens condition number column correlations model matrix. First, construct modified data: Different methods modifying inputs model definitions require different calculations compensating model parameters, figured case separately. common effect change interpretation \\(\\beta_0\\) parameter.","code":"data2 <- data %>%   mutate(x = x + 1000) cor(X2[, 2:3]) ggplot() +   geom_point(aes(X2[, 2], X2[, 3])) +   ggtitle(paste(\"Correlation =\", cor(X2[,2], X2[,3]))) ## Very close to co-linear ## Data setup: data4 <- data2 %>%   mutate(x_shifted = x - mean(x))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"lab session explore Speed matrix computations Floating point accuracy least squares estimation statistical linear models Open github repository clone project Lab 2 4 upgrade StatCompLab package lab, can work .R file, working code chunks .Rmd highly recommended. measuring relative running times different methods, ’ll use bench package. won’t need load package library. Instead, check bench::mark() runs without errors. doesn’t, ’s package installed, install install.packages(\"bench\") R Console. Suggested code setup code chunk:","code":"suppressPackageStartupMessages(library(tidyverse)) library(StatCompLab)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"numerical-speed","dir":"Articles","previous_headings":"","what":"Numerical speed","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"Define following function computing vector norm: Try simple vector can verify result correct, e.g. now take look computational speed matrix operations. First construct vector \\(\\boldsymbol{b}\\) matrix \\(\\boldsymbol{}\\): Let’s check speed difference \\(\\boldsymbol{}^\\top\\boldsymbol{b}\\) \\((\\boldsymbol{b}^\\top\\boldsymbol{})^\\top\\) (convince two expressions equivalent exact arithmetic, possibly also floating point arithmetic). function t() construct transpose matrix. Note b vector (opposed single-column matrix) interpreted either column vector row vector, depending operation. b vector, .matrix(b) always single-column matrix, t(b) single-row matrix, b %*% interpreted t(b) %*% . method computing \\(\\boldsymbol{}^\\top \\boldsymbol{b}\\) faster? Use bench::mark() compare different ways evaluating expression. extracting variables expression, median, itr/sec bench::mark() output using select() function, can summarise results table. Solution: save typing, define function bench_table kable printing: Solution: t() %*% b method slowest implementation. Transposing matrix requires copying values new locations. Transposing vector doesn’t require copying. \\(\\boldsymbol{}\\boldsymbol{B}\\boldsymbol{b}\\), order matrix&vector multiplication done important. Define new variables: Compare running times %*% B %*% b, (%*% B) %*% b, %*% (B %*% b)). Solution: three versions produce similar numerical results (bench::mark complain don’t) method fastest? ? computed results ? Solution: One matrix-matrix multiplication expensive compared matrix-vector multiplication. results numerically close, one can likely express bounds terms .Machine$double.eps \\(m,n,p\\). square matrix \\(\\boldsymbol{}\\) size \\(1000\\times 1000\\), compare cost solve() %*% b (Get matrix inverse \\(\\boldsymbol{}^{-1}\\), multiply \\(\\boldsymbol{b}\\)) cost solve(, b) (Solve linear system \\(\\boldsymbol{}\\boldsymbol{u}=\\boldsymbol{b}\\)). Solution:","code":"vec_norm <- function(x) {   sum(x^2)^0.5 } c(vec_norm(c(1, 2, 3)), sqrt(1 + 4 + 9)) ## [1] 3.741657 3.741657 n <- 100000 m <- 100 b <- rnorm(n) A <- matrix(rnorm(n * m), n, m) bm <- bench::mark(   t(A) %*% b,   t(t(b) %*% A),   t(b %*% A) ) knitr::kable(bm %>% select(expression, median, `itr/sec`)) bench_table <- function(bm) {   knitr::kable(     bm %>%       select(expression, median, `itr/sec`)   ) } m <- 1000 n <- 1000 p <- 1000 A <- matrix(rnorm(m * n), m, n) B <- matrix(rnorm(n * p), n, p) b <- rnorm(p) bm <- bench::mark(   A %*% B %*% b,   (A %*% B) %*% b,   A %*% (B %*% b) ) bench_table(bm) bm <- bench::mark(   solve(A) %*% b,   solve(A, b),   check = FALSE # The results will differ by approximately 8e-13 ) bench_table(bm)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"numerical-least-squares-estimation","dir":"Articles","previous_headings":"","what":"Numerical Least Squares estimation","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"’ll now investigate numerical issues numerical least squares estimation. Run following code generates synthetic observations linear model \\[ \\begin{aligned} y_i &= \\frac{x_i-100.5}{5} + (x_i-100.5)^2 + e_i, \\end{aligned} \\] \\(e_i \\sim \\mathsf{Normal}(0, \\sigma_e=0.1)\\), independent, \\(=1,\\dots,n=100\\). Let’s plot data stored data.frame:  true values \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) standardised model formulation, \\[ \\begin{aligned} y_i &= \\mu_i + e_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i ? \\end{aligned} \\] Hint: Identify values matching terms expanding initial model definition . Solution: \\[ \\begin{aligned} \\beta_0 &= -\\frac{100.5}{5} + 100.5^2 = 10080.15 \\\\ \\beta_1 &= \\frac{1}{5} - 201 = -200.8 \\\\ \\beta_2 &= 1 \\end{aligned} \\] Create beta_true vector containing \\((\\beta_0, \\beta_1, \\beta_2)\\): Solution: Use \\(\\beta\\)-values plot quadratic true model predictor function function \\(x\\[100,101]\\), together observations, estimate provided lm(). Since observed \\(x\\) values relatively dense, don’t necessarily need special plot sequence \\(x\\) values, can reuse observed values, model predictions value can obtained fitted() method. lm() formula specification, use + (x^2) quadratic term. tells lm() first compute x^2, use result new covariate. () method can interpreted “use computed value instead possible special meanings expression”. Solution:  Use model.matrix(mod1) function extract \\(\\boldsymbol{X}\\) matrix vector formulation model, \\(\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{e}\\), store X1. (See ?model.matrix information). use direct normal equations solve shown Lecture 8, \\((\\boldsymbol{X}^\\top\\boldsymbol{X})^{-1}\\boldsymbol{X}^\\top\\boldsymbol{y}\\), estimate \\(\\beta\\) parameters. work? Solution:","code":"## Set the \"seed\" for the random number sequences, so we can ## reproduce exactly the same random numbers every time we run the code set.seed(1) ## Simulate the data # x: 100 random numbers between 100 and 101 data <- data.frame(x = 100 + sort(runif(100))) # y: random variation around a quadratic function: data <- data %>%   mutate(y = (x - 100.5) / 5 + (x - 100.5)^2 + rnorm(n(), sd = 0.1)) ggplot(data) + geom_point(aes(x, y)) beta_true <- c(10080.15, -200.8, 1) # beta_0 = -100.5/5 + 100.5^2 = 10080.15 # beta_1 = 1/5 - 201 = -200.8 # beta_2 = 1 data <- data %>%   mutate(mu_true = beta_true[1] + beta_true[2] * x + beta_true[3] * x^2) pl <- ggplot(data) +   geom_point(aes(x, y)) +   geom_line(aes(x, mu_true, col =\"True\"))  ## lm manages to estimate the regression: mod1 <- lm(y ~ x + I(x^2), data = data) ## Add the fitted curve: pl +   geom_line(aes(x, fitted(mod1), col = \"Estimated\")) X1 <- model.matrix(mod1) # To allow the tutorial document to keep running even in case of errors, wrap in try() try(   beta.hat <- solve(t(X1) %*% X1, t(X1) %*% data$y) ) ## Error in solve.default(t(X1) %*% X1, t(X1) %*% data$y) :  ##   system is computationally singular: reciprocal condition number = 3.9864e-19"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial08Solutions.html","id":"shifted-covariate","dir":"Articles","previous_headings":"","what":"Shifted covariate","title":"Tutorial 08: Floating point computations and least squares (solutions)","text":"whole model shifted right, given \\(x+1000\\) instead original \\(x\\)-values? model expression still able represent quadratic expression different \\(\\beta\\) values. Create new data set: Estimate model using lm() new data set. work? Solution:  lm() fails fit correct curve, without warning! default lm() allow singular models, “fix” problem removing co-linear columns model matrix. can ask disallow singular models singular.ok argument. Solution: Define function cond_nr() taking matrix X input returning condition number, computed help svd (see Lecture 8 code example condition number). Solution: help model.matrix, examine condition numbers model matrices two cases previous tasks. lm() computes fit using QR decomposition approach, direct solution normal equations. second lm() fit bad? Solution: Plot second third columns model matrix X2 , use cor examine correlation (see ?cor). explain large condition number?  Solution: cases columns virtually co-linear, leads small \\(d_3\\) value, therefore large condition number. Since linear model says simply expected value vector \\(\\mathsf{E}(\\boldsymbol{y})\\) lies space spanned columns \\(\\boldsymbol{X}\\), one possibility attempt arrive better conditioned \\(\\boldsymbol{X}\\) linear rescaling /recombination columns. always equivalent linear re-parameterization. Try model matrix model causes lm() fail. particular, column (except intercept column) subtract column mean (e.g., try sweep() colMeans() functions, see example code ). divide column (except intercept column) standard deviation. Find condition number new model matrix. Fit model model matrix using something like mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) Produce plot confirms resulting fit sensible now. Solution:  Note: data split estimation test sets, transformation must applied test data, using scaling parameters derived observation data. Therefore scale() function isn’t useful . Instead, explicitly computing storing transformation information necessary. Compute condition numbers X2 X3. Also compute correlation column 2 3 X3 (subtract \\(1\\) see close \\(1\\) ). Solution: subtracting rescaling help? Solution: Yes, condition number significantly reduced. correlation columns however still close 1, meaning matrix still close singular. alternative fix subtract mean x value original x vector defining quadratic model. Try see happens condition number column correlations model matrix. First, construct modified data: Solution:  Different methods modifying inputs model definitions require different calculations compensating model parameters, figured case separately. common effect change interpretation \\(\\beta_0\\) parameter.","code":"data2 <- data %>%   mutate(x = x + 1000) mod2 <- lm(y ~ x + I(x^2), data = data2) ggplot(data2) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod1), col = \"mod1\")) +   geom_line(aes(x, fitted(mod2), col = \"mod2\")) # lm() fails to fit the correct curve, without warning! try(   mod2b <- lm(y ~ x + I(x^2), data = data2, singular.ok = FALSE) ) ## Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :  ##   singular fit encountered # Now lm refuses to compute an estimate. cond_nr <- function(X) {   d <- svd(X)$d   max(d) / min(d) } cond_nr(X1) ## [1] 1560769713 # large, but somewhat manageable condition number  X2 <- model.matrix(mod2) cond_nr(X2) ## [1] 2.242481e+13 # very large condition number, # but not large enough to trigger a warning! cor(X2[, 2:3]) ##        x I(x^2) ## x      1      1 ## I(x^2) 1      1 ggplot() +   geom_point(aes(X2[, 2], X2[, 3])) +   ggtitle(paste(\"Correlation =\", cor(X2[,2], X2[,3]))) ## Very close to co-linear ## Solution code: mean_vec <- colMeans(X2[, 2:3]) sd_vec <- c(sd(X2[, 2]), sd(X2[, 3])) X3 <- cbind(   X2[, 1],   sweep(X2[, 2:3], 2, mean_vec, FUN = \"-\") ) X3[, 2:3] <-   sweep(X3[, 2:3], 2, sd_vec, FUN = \"/\") data3 <- data2 %>%   mutate(x1 = X3[, 1], x2 = X3[, 2], x3 = X3[, 3])  mod3 <- lm(y ~ x1 + x2 + x3 - 1, data = data3) ggplot(data3) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod3))) c(cond_nr(X2), cond_nr(X3)) ## [1] 2.242481e+13 1.791760e+04 c(cor(X2[, 2], X2[, 3]) - 1,   cor(X3[, 2], X3[, 3]) - 1) ## [1] -6.229745e-09 -6.229745e-09 ## Data setup: data4 <- data2 %>%   mutate(x_shifted = x - mean(x)) mod4 <- lm(y ~ x_shifted + I(x_shifted^2), data = data4) ggplot(data4) +   geom_point(aes(x, y)) +   geom_line(aes(x, fitted(mod4))) X4 <- model.matrix(mod4) cond_nr(X4) ## [1] 15.36915 cor(X4[, 2], X4[, 3]) ## [1] -0.09094755"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"lab session explore Exchangeability test prediction scores Pareto smoothed importance sampling Open one github lab repository clone projects lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.9.0 higher. Solution: accompanying Tutorial09Solutions tutorial/vignette documents contain solutions explicitly, make easier review material workshops.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"leave-one-out-prediction","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Leave-one-out prediction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"Revisit Tutorial 6 load 3D printer filament data: tutorial 6, two models B estimated Predictions scores done well 50% estimation/prediction data split. Implement function leave1out(data, model) performs leave-one-cross validation selected model. two models; \\(=1,\\dots,N\\), estimate model parameters using \\(\\{(x_j,y_j), j\\neq \\}\\), compute prediction information based \\(x_i\\) prediction model \\(F_i\\), compute required scores \\(S(F_i,y_i)\\). output data.frame \\(N\\) rows extends original data frame four additional columns mean, sd, se ds leave-one-prediction means, standard deviations, prediction scores Squared Error Dawid-Sebastiani scores. following code work code correct:","code":"data(\"filament1\", package = \"StatCompLab\") pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_A <- leave1out(filament1, model = \"A\") score_B <- leave1out(filament1, model = \"B\") ggplot() +   geom_point(aes(CAD_Weight, se, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, se, colour = \"B\"), data = score_B) ggplot() +   geom_point(aes(CAD_Weight, ds, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, ds, colour = \"B\"), data = score_B)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"exchangeability-test","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Exchangeability test","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"want investigate whether one model better predicting . equivalent, matter, average, randomly swap B prediction scores within leave-one-score pair \\((S^A_i, S^B_i)\\). Use test statistic \\(\\frac{1}{N}\\sum_{=1}^N (S^A_i - S^B_i)\\), make Monte Carlo estimate p-value test exchangeability model predictions B alternative hypothesis B better . First compute test statistic two prediction scores. Hints: particular test statistic, one possible approach first compute pairwise score differences generate randomisation samples sampling random sign changes. Compute test statistic randomly altered set values compare original test statistic. See lecture exchangeability information. Start \\(J=1000\\) iterations testing code. Increase 10000 precise results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09.html","id":"pareto-smoothed-importance-sampling","dir":"Articles","previous_headings":"","what":"Pareto smoothed importance sampling","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling","text":"Explore Pareto smoothed importance sampling (PSIS) method. Use t-distribution 10 degrees freedom, importance sampling distribution \\(x\\sim \\mathsf{Normal}(0, \\sigma^2)\\) different values \\(\\sigma\\). Use \\(N=100\\) samples. Use loo::psis() function compute Pareto smoothed importance log-weights diagnostics. effect different values \\(\\sigma\\) \\(k\\) parameter estimate? (\\(\\hat{k}\\) value $diagnostics$pareto_k field loo::psis() output.) Plot original smoothed log-weights function \\(x\\), stat_ecdf, different values \\(\\sigma\\), including \\(\\sigma=1\\) \\(\\sigma=2\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"lab session explore Exchangeability test prediction scores Pareto smoothed importance sampling Open one github lab repository clone projects lab, can work .R file, working code chunks .Rmd recommended. Make sure update StatCompLab package version 21.9.0 higher.","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"leave-one-out-prediction","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Leave-one-out prediction","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"Revisit Tutorial 6 load 3D printer filament data: tutorial 6, two models B estimated Predictions scores done well 50% estimation/prediction data split. Implement function leave1out(data, model) performs leave-one-cross validation selected model. two models; \\(=1,\\dots,N\\), estimate model parameters using \\(\\{(x_j,y_j), j\\neq \\}\\), compute prediction information based \\(x_i\\) prediction model \\(F_i\\), compute required scores \\(S(F_i,y_i)\\). output data.frame \\(N\\) rows extends original data frame four additional columns mean, sd, se ds leave-one-prediction means, standard deviations, prediction scores Squared Error Dawid-Sebastiani scores. Solution: following code work code correct:","code":"data(\"filament1\", package = \"StatCompLab\") pred_A <- filament1_predict(fit_A, newdata = filament1) pred_B <- filament1_predict(fit_B, newdata = filament1) score_A <- cbind(pred_A, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) score_B <- cbind(pred_B, filament1) %>%   mutate(     se = proper_score(\"se\", Actual_Weight, mean = mean),     ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd),     interval = proper_score(\"interval\", Actual_Weight,                             lwr = lwr, upr = upr, alpha = 0.1)   ) leave1out <- function(data, model) {   data <- data %>% mutate(mean = NA_real_, sd = NA_real_)   for (i in seq_len(nrow(data))) {     fit <- filament1_estimate(data[-i, , drop = FALSE], model)     pred <- filament1_predict(fit, newdata = data[i, , drop = FALSE])     data[i, \"mean\"] <- pred$mean     data[i, \"sd\"] <- pred$sd   }   data <- data %>%     mutate(       se = proper_score(\"se\", Actual_Weight, mean = mean),       ds = proper_score(\"ds\", Actual_Weight, mean = mean, sd = sd)     )   data } score_A <- leave1out(filament1, model = \"A\") score_B <- leave1out(filament1, model = \"B\") ggplot() +   geom_point(aes(CAD_Weight, se, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, se, colour = \"B\"), data = score_B) ggplot() +   geom_point(aes(CAD_Weight, ds, colour = \"A\"), data = score_A) +   geom_point(aes(CAD_Weight, ds, colour = \"B\"), data = score_B)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"exchangeability-test","dir":"Articles","previous_headings":"Filement model prediction comparison","what":"Exchangeability test","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"want investigate whether one model better predicting . equivalent, matter, average, randomly swap B prediction scores within leave-one-score pair \\((S^A_i, S^B_i)\\). Use test statistic \\(\\frac{1}{N}\\sum_{=1}^N (S^A_i - S^B_i)\\), make Monte Carlo estimate p-value test exchangeability model predictions B alternative hypothesis B better . First compute test statistic two prediction scores. Hints: particular test statistic, one possible approach first compute pairwise score differences generate randomisation samples sampling random sign changes. Compute test statistic randomly altered set values compare original test statistic. See lecture exchangeability information. Start \\(J=1000\\) iterations testing code. Increase 10000 precise results. Solution: see Square Error scores, two model predictions appear exchangeable, prediction uncertainty taken account Dawid-Sebastiani score, B appears better , average.","code":"score_diff <- data.frame(se = score_A$se - score_B$se,                          ds = score_A$ds - score_B$ds) statistic0 <- score_diff %>% summarise(se = mean(se), ds = mean(ds)) J <- 10000 statistic <- data.frame(se = numeric(J),                         ds = numeric(J)) for (loop in seq_len(J)) {   random_sign <- sample(c(-1, 1), size = nrow(score_diff), replace = TRUE)   statistic[loop, ] <- score_diff %>% summarise(se = mean(random_sign * se),                                                 ds = mean(random_sign * ds)) } p_values <-   statistic %>%   summarise(se = mean(se > statistic0$se),             ds = mean(ds > statistic0$ds)) # Estimates: p_values ##       se     ds ## 1 0.5011 0.0442 # Monte Carlo std.error:: sqrt(p_values * (1 - p_values) / J) ##            se          ds ## 1 0.004999988 0.002055392"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial09Solutions.html","id":"pareto-smoothed-importance-sampling","dir":"Articles","previous_headings":"","what":"Pareto smoothed importance sampling","title":"Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)","text":"Explore Pareto smoothed importance sampling (PSIS) method. Use t-distribution 10 degrees freedom, importance sampling distribution \\(x\\sim \\mathsf{Normal}(0, \\sigma^2)\\) different values \\(\\sigma\\). Use \\(N=100\\) samples. Use loo::psis() function compute Pareto smoothed importance log-weights diagnostics. effect different values \\(\\sigma\\) \\(k\\) parameter estimate? (\\(\\hat{k}\\) value $diagnostics$pareto_k field loo::psis() output.) Plot original smoothed log-weights function \\(x\\), stat_ecdf, different values \\(\\sigma\\), including \\(\\sigma=1\\) \\(\\sigma=2\\). Solution:","code":"sigma <- 1 x <- rnorm(100, sd = sigma) log_ratio <- dt(x, df = 10, log = TRUE) - dnorm(x, sd = sigma, log = TRUE) result <- loo::psis(log_ratio, r_eff = 1) ## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. result$diagnostics$pareto_k ## [1] 2.692393 result$diagnostics$n_eff ## [1] 98.14698 df <- data.frame(x = x,                  log_ratio = log_ratio,                  log_weight = result$log_weights) ggplot(df) +   geom_point(aes(x, log_ratio, color = \"Original\")) +   geom_point(aes(x, log_weight, color = \"Smoothed\")) ggplot(df) +   stat_ecdf(aes(log_ratio, color = \"Original\")) +   stat_ecdf(aes(log_weight, color = \"Smoothed\"))"},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Finn Lindgren. Author, maintainer.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lindgren F (2022). StatCompLab: Workshop Material UoE Statistical Computing. https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab.","code":"@Manual{,   title = {StatCompLab: Workshop Material For UoE Statistical Computing},   author = {Finn Lindgren},   year = {2022},   note = {https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab}, }"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"statcomplab","dir":"","previous_headings":"","what":"Workshop Material For UoE Statistical Computing","title":"Workshop Material For UoE Statistical Computing","text":"package collects workshop materials Statistical Computing (MATH10093) University Edinburgh 2021/22. tutorial documents browsable online https://finnlindgren.github.io/StatCompLab/ Contact Finn Lindgren, finn.lindgren@ed.ac.uk information. package version number system YearOfStudy.StudyWeek.MinorUpdate","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Workshop Material For UoE Statistical Computing","text":"can install (later upgrade) package GitHub : want view vignette versions tutorials within RStudio, need add build_vignettes argument:","code":"# If devtools isn't installed, first run install.packages(\"remotes\") remotes::install_github(\"finnlindgren/StatCompLab\") remotes::install_github(\"finnlindgren/StatCompLab\", build_vignettes = TRUE)"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Workshop Material For UoE Statistical Computing","text":"convenient way access tutorial documents run Tutorial tab upper right part RStudio. allows see tutorial text code hints code files, running code Console viewing plots. alternative methods also allows viewing documents, less convenient; vignettes show space plots, run_tutorial blocks Console window used run code.","code":"library(StatCompLab) # To install the vignette versions, add build_vignettes=TRUE to the install_github() call above. vignette(package = \"StatCompLab\") # List available vignettes vignette(\"Tutorial01\", \"StatCompLab\") # View a specific vignette # The following method blocks the Console from running other code; better to use the Tutorials pane instead library(learnr) available_tutorials(\"StatCompLab\") # List available tutorials run_tutorial(\"Tutorial01\", \"StatCompLab\") # Run a specific tutorial"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/StatEwcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"StatEwcdf ggproto object — StatEwcdf","title":"StatEwcdf ggproto object — StatEwcdf","text":"StatEwcdf ggproto object","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple binomial model likelihood — arch_loglike","title":"Multiple binomial model likelihood — arch_loglike","text":"Compute log-likelihood multiple Binom(N,phi) observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(param, y)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple binomial model likelihood — arch_loglike","text":"param Either vector two elements, N phi, data.frame two columns, named N phi y data vector","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple binomial model likelihood — arch_loglike","text":"log-likelihood N,phi combination. implementation internally uses log-gamma function provide differentiable function N, allows optim() treat N continuous variable. N < max(y), log-likelihood defined -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(   param = data.frame(N = c(5, 10, 20, 40),                      phi = c(0.1, 0.2, 0.3, 0.4)),   y = c(2, 4)) #> [1] -10.324930  -3.626867  -5.618058 -25.216655"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate filament models — filament1_estimate","title":"Estimate filament models — filament1_estimate","text":"Estimate filament models different variance structure","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate filament models — filament1_estimate","text":"","code":"filament1_estimate(data, model)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate filament models — filament1_estimate","text":"data data.frame variables filament1 data set. Must columns CAD_Weight Actual_Weight model Either \"\" log-linear variance model, \"B\" proportional scaling error model","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate filament models — filament1_estimate","text":"estimation object suitable use filament1_predict()","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate filament models — filament1_estimate","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict values for a filament model — filament1_predict","title":"Predict values for a filament model — filament1_predict","text":"Uses estimated filament model predict new observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict values for a filament model — filament1_predict","text":"","code":"filament1_predict(object, newdata, alpha = 0.05, df = Inf, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict values for a filament model — filament1_predict","text":"object estimation object, like output filament1_estimate() newdata data use prediction. Must column CAD_Weight alpha target coverage error prediction intervals, Default: 0.05 df degrees freedom t-quantiles prediction interval construction, Default: Inf ... Additional arguments, currently ignored","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict values for a filament model — filament1_predict","text":"data.frame variables mean, sd, lwr, upr, summarising prediction distribution row newdata.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict values for a filament model — filament1_predict","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive numerical optimisation explorer — optimisation","title":"Interactive numerical optimisation explorer — optimisation","text":"Shiny app exploring several numerical optimisation methods work","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive numerical optimisation explorer — optimisation","text":"","code":"optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate proper scores — proper_score","title":"Calculate proper scores — proper_score","text":"Calculates proper scores probabilistic predictions","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate proper scores — proper_score","text":"","code":"proper_score(type, obs, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate proper scores — proper_score","text":"type string indicating type score calculate. One \"se\", \"ae\", \"ds\", \"interval\". See Details . obs vector observations ... Additional named arguments needed type score, see Details .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate proper scores — proper_score","text":"vector calculated scores, one observation","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate proper scores — proper_score","text":"score type takes additional arguments define prediction model information used score calculations. Unless specified otherwise, extra arguments either scalars (applying predictions) vectors length obs vector. se Squared error score. Requires mean, defining prediction mean ae Absolute error score. Requires median, defining prediction median ds Dawid-Sebastiani score. Requires mean sd, defining mean standard deviation predictions interval Squared error score. Requires lwr upr, defining lower upper prediction interval endpoints, alpha, defining prediction error probability targeted prediction intervals scores \\(S_{type}(F_i,y_i)\\) defined lecture notes, obs[] equal \\(y_i\\) prediction distribution \\(F_i\\) defined additional named arguments.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate proper scores — proper_score","text":"","code":"# Five realisations of N(3, 3^2) obs <- rnorm(5, mean = 3, sd = 3)  # One prediction for each observation. Only one of the predictions describes # the true model F_mean <- 1:5 F_median <- 1:5 F_sd <- 1:5 F_lwr <- F_mean - F_sd * qnorm(0.9) F_upr <- F_mean - F_sd * qnorm(0.1)  # Compute the scores data.frame(   se = proper_score(\"se\", obs, mean = F_mean),   ae = proper_score(\"ae\", obs, median = F_median),   ds = proper_score(\"ds\", obs, mean = F_mean, sd = F_sd),   interval = proper_score(\"interval\", obs,                           lwr = F_lwr, upr = F_upr, alpha = 0.2) ) #>          se       ae        ds  interval #> 1 19.166933 4.378006 19.166933 33.527643 #> 2  2.968364 1.722894  2.128385  5.126206 #> 3 22.080253 4.698963  4.650586 16.232392 #> 4  1.532615 1.237988  2.868377 10.252413 #> 5 17.470746 4.179802  3.917706 12.815516"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute empirical weighted cumulative distribution — stat_ewcdf","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"Version ggplot2::stat_ecdf adds weights property observation, produce empirical weighted cumulative distribution function. empirical cumulative distribution function (ECDF) provides alternative visualisation distribution. Compared visualisations rely density (like geom_histogram()), ECDF require tuning parameters handles continuous discrete variables. downside requires training accurately interpret, underlying visual tasks somewhat challenging.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"stat_ewcdf(   mapping = NULL,   data = NULL,   geom = \"step\",   position = \"identity\",   ...,   n = NULL,   pad = TRUE,   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"n NULL, interpolate. NULL, number points interpolate . pad TRUE, pad ecdf additional points (-Inf, 0) (Inf, 1) na.rm FALSE (default), removes missing values warning.  TRUE silently removes missing values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"computed-variables","dir":"Reference","previous_headings":"","what":"Computed variables","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"x x data y cumulative density corresponding x","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"library(ggplot2)  n <- 100 df <- data.frame(   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),   g = gl(2, n),   w = c(rep(1/n, n), sort(runif(n))^sqrt(n)) ) ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\")   # Don't go to positive/negative infinity ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE)   # Multiple ECDFs ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf()  ggplot(df, aes(x, colour = g, weights = w)) +   stat_ewcdf() +   facet_wrap(vars(g), ncol = 1)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted sample quantiles — wquantile","title":"Weighted sample quantiles — wquantile","text":"Calculates empirical sample quantiles optional weights, given probabilities. Like quantile(), smallest observation corresponds probability 0 largest probability 1. Interpolation discrete values done type=7, quantile(). Use type=1 generate quantile values raw input samples.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted sample quantiles — wquantile","text":"","code":"wquantile(   x,   probs = seq(0, 1, 0.25),   na.rm = FALSE,   type = 7,   weights = NULL,   ... )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted sample quantiles — wquantile","text":"x numeric vector whose sample quantiles wanted. NA NaN values allowed numeric vectors unless na.rm TRUE. probs numeric vector probabilities values \\([0,1]\\). na.rm logical; true, NA NaN's removed x quantiles computed. type numeric, 1 interpolation, 7, interpolated quantiles. Default 7. weights numeric vector non-negative weights, length x, NULL. weights normalised sum 1. NULL, wquantile(x) behaves quantile(x), equal weight sample value. ... Additional arguments, currently ignored","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted sample quantiles — wquantile","text":"","code":"# Some random numbers x <- rnorm(100)  # Plain quantiles: quantile(x) #>          0%         25%         50%         75%        100%  #> -3.03015309 -0.77261370  0.02849534  0.79695900  2.38081778   # Larger values given larger weight, on average shifting the quantiles upward: wquantile(x, weights = sort(runif(length(x)))) #> [1] -3.03015309 -0.70506820 -0.05074821  0.79697406  2.38081778"}]
