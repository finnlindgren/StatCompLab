[{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example: Numerical statistics","text":"Lab 4, consider Poisson model observations \\(\\boldsymbol{y}=\\{y_1,\\dots,y_n\\}\\): \\[ \\begin{aligned} y_i & \\sim \\mathsf{Poisson}(\\lambda), \\quad\\text{independent $=1,\\dots,n$.} \\end{aligned} \\] joint probability mass function \\[ p(\\boldsymbol{y}|\\lambda) = \\exp(-n\\lambda) \\prod_{=1}^n \\frac{\\lambda^{y_i}}{y_i!} \\] lab, considered three parameterisation alternatives: \\(\\theta = \\lambda\\), \\(\\widehat{\\theta}_\\text{ML}=\\frac{1}{n}\\sum_{=1}^n y_i = \\overline{y}\\) \\(\\theta = \\sqrt{\\lambda}\\), \\(\\widehat{\\theta}_\\text{ML}=\\sqrt{\\overline{y}}\\) \\(\\theta = \\log(\\lambda)\\), \\(\\widehat{\\theta}_\\text{ML}=\\log\\left(\\overline{y}\\right)\\) know inverse expected Fisher information \\(\\lambda/n\\) case 1, \\(1/(4n)\\) case 2, \\(1/(n\\lambda)\\) case 3. lab, three functions, CI1, CI2, CI3, defined constructing confidence intervals based asymptotic Normal approximation Maximum Likelihood estimators parameterisations. project, use function pois_CI defined code.R file project repository. code file imported method shown RMdemo.Rmd file, displayed report. Look file documentation functions. task simulation study assess accuracy three interval construction methods. my_code.R file, define function multi_pois_CI(m, n, lambda, alpha,type) takes inputs m, indicating number simulations perform, n, number observations used sample, lambda, true parameter value, well alpha type interpretation pois_CI. function must return data frame m rows two columns, named Lower Upper containing confidence intervals, one row. Show mathematically widths \\(\\lambda\\)-intervals parameterisation type 1 2 always identical , given observation vector \\(\\boldsymbol{y}\\) confidence level \\(1-\\alpha\\). Note: ignore cases \\(\\theta\\)- \\(\\lambda\\)-intervals extend \\(0\\) (case 1, normal quantile \\(z\\) \\(\\overline{y} < z^2/n\\), case 2, \\(4\\overline{y} < z^2/n\\)). my_code.R file, define function tidy_multi_pois_CI(m, n, lambda, alpha) takes inputs multi_pois_CI except type argument. function must return data frame 3*m rows three columns, named Lower, Upper, Type containing confidence intervals three calls multi_pois_CI, Type column containing values 1, 2, 3 indicate method calculated interval. help tidy_multi_pois_CI, estimate coverage probability nominal confidence level \\(1-\\alpha\\)=90%, three interval types, \\(n=2\\), \\(\\lambda=3\\), using \\(m\\) least \\(100000\\). well coverage probabilities match nominal confidence level method? tidyverse package loaded, group_by summarise functions can used avoid direct indexing loops. Summarise discuss results. Plot empirical CDFs interval widths three methods, common figure. Use factor(Type) group information colour. Also compute median interval width method. Summarise discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic Sea","title":"StatComp Project Example: Numerical statistics","text":"Lab 3, investigated data archaeological excavation Gotland Baltic sea. 1920s, battle gravesite 1361 subject several archaeological excavations. total \\(493\\) femurs (thigh bones) (\\(256\\) left, \\(237\\) right) found. wanted figure many persons likely buried gravesite. must reasonably least \\(256\\), many ? assignment, use importance sampling estimate credible intervals problem. lab, seemed difficult extract much information sample, since two observations two parameters. investigate including data excavations type might decrease uncertainty number persons buried, improved information average detection probability, \\(\\phi\\). function arch_data code.R returns data frame data four different excavations (note: one real; three synthetic data generated assignment). assume model lab excavation, unknown \\(N_j\\), \\(j=1,\\dots,J\\), \\(J\\) excavations, common detection probability \\(\\phi\\). assume model , conditionally \\(\\{N_j\\}\\) \\(\\phi\\), \\((Y_{j,}|N_j,\\phi)\\sim\\mathsf{Bin}(N_j,\\phi)\\), \\(=1,2\\), independent, prior distributions \\(N_j\\sim\\mathsf{Geom}(\\xi)\\), \\(0<\\xi<1\\), \\(\\phi\\sim\\mathsf{Beta}(, b)\\), \\(,b>0\\), \\[ \\begin{aligned} p_{Y_{j,}|N_j=n,\\phi}(y) = \\mathsf{P}(Y_{j,}=y|N_j=n,\\phi) &= {n \\choose y} \\phi^y(1-\\phi)^{n-y}, \\quad y=0,\\dots,n, \\\\ p_{N_j}(n) = \\mathsf{P}(N_j=n) &= \\xi\\,(1-\\xi)^n,\\quad n=0,1,2,3,\\dots, \\\\ p_\\phi(\\phi) &= \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}, \\quad \\phi\\(0,1) , \\end{aligned} \\] \\(B(,b)\\) Beta function, see ?beta R.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"joint-probability-function-for-boldsymbolnboldsymboly","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Joint probability function for \\((\\boldsymbol{N},\\boldsymbol{Y})\\)","title":"StatComp Project Example: Numerical statistics","text":"Conditionally \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), observations \\(\\boldsymbol{Y}\\), conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\) \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) \\(\\widetilde{}=+\\sum_{j,} Y_{j,}\\) \\(\\widetilde{b}=b+2\\sum_j N_j -\\sum_{j,} Y_{j,}\\), sums go \\(j=1,\\dots,J\\) \\(=1,2\\). Show joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\), \\(N_j\\geq\\max(Y_{j,1},Y_{j,2})\\) \\(Y_{j,}=0,1,2,\\dots\\), given by1 \\[ p(\\boldsymbol{N},\\boldsymbol{Y}) = \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)} \\prod_{j=1}^J \\left[ p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\right], \\] \\(p(N_j)\\) probability mass function Geometric distribution prior \\(N_j\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"probability-function-implementation","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Probability function implementation","title":"StatComp Project Example: Numerical statistics","text":"help functions lbeta, lchoose, dgeom, define (my_code.R) function log_prob_NY(N,Y,xi,,b) arguments N (vector length \\(J\\)) \\(Y\\) (data frame size \\(J\\times 2\\), formatted like output arch_data(J)). arguments xi, , b model hyperparameters \\(\\xi\\), \\(\\), \\(b\\). function return logarithm \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\) combination \\(\\boldsymbol{N}\\) \\(\\boldsymbol{Y}\\) valid, otherwise return -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"importance-sampling-function","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Importance sampling function","title":"StatComp Project Example: Numerical statistics","text":"aim construct Bayesian credible interval \\(N_j\\) using importance sampling, similarly method used lab 4. , Gaussian approximation posterior parameter used. , instead use prior distribution \\(\\boldsymbol{N}\\) sampling distribution, samples \\(N_j^{[k]}\\sim\\mathsf{Geom}(\\xi)\\). Since observations \\(\\boldsymbol{Y}\\) fixed, posterior probability function \\(p(\\boldsymbol{N}|\\boldsymbol{Y})\\) \\(\\boldsymbol{N}\\) proportional \\(p(\\boldsymbol{N},\\boldsymbol{Y})\\). define logarithm unnormalised importance weights, \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[p(\\boldsymbol{N}^{[k]})]\\), \\(p(\\boldsymbol{N}^{[k]})\\) product \\(J\\) geometric distribution probabilities. Define (my_code.R) function arch_importance(K,Y,xi,,b) arguments Y, xi, , b , first argument K defining number samples generate. function return data frame \\(K\\) rows \\(J+1\\) columns, named N1, N2, etc, \\(j=1,\\dots,J\\), containing samples prior distributions \\(N_j\\), final column called Log_Weights, containing re-normalised log-importance-weights, constructed shifting \\(\\log[w^{[k]}]\\) subtracting largest \\(\\log[w^{[k]}]\\) value, lab 4.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"more-efficient-alternative","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea > Importance sampling function","what":"More efficient alternative","title":"StatComp Project Example: Numerical statistics","text":"Using \\(\\mathsf{Geom}(\\xi)\\) prior distribution \\(N_j\\) leads rather inefficient importance sampler, especially \\(J=4\\); weights become zero (\\(N_j^{[k]}<Y_{j,}\\)) near zeros, samples influence practical estimates credible intervals. alternative, can use sampling distributions better adapted posterior distributions. Let \\(N^\\text{min}_j=\\max(Y_{j,1}, Y_{j,2})\\) smallest allowed value sample, \\(j=1,\\dots,J\\). eliminate impossible combinations, weights (theory numerically) positive. improve method, can choose different values probability parameter samples, also adapt differently \\(j\\). Without motivation, can use values \\(\\xi_j=1/(1 + 4 N^\\text{min}_j)\\) sampling, \\(N_j^{[k]}=N^\\text{min}_j + \\mathsf{Geom}(\\xi_j)\\). Note: Due nature geometric distribution, also equivalent ordinary \\(\\mathsf{Geom}(\\xi_j)\\) conditioned least \\(N^\\text{min}_j\\). take different sampling method account, log-weights need defined \\(\\log[w^{[k]}]=\\log[p(\\boldsymbol{N}^{[k]},\\boldsymbol{Y})]-\\log[\\widetilde{p}(\\boldsymbol{N}^{[k]})]\\), \\(\\widetilde{p}(\\boldsymbol{N}^{[k]})=\\prod_{j=1}^J p_{\\mathsf{Geom}(\\xi_j)}(N_j^{[k]}-N^\\text{min}_j)\\), product \\(J\\) different geometric distribution probabilities. may adapt example code (details depend vectorisation method use): Using method, \\(K\\) doesn’t need larger \\(100000\\), small \\(K=1000\\) starts give reasonable values code testing purposes, \\(K=10000\\) take seconds run code fully vectorised (vapply function can used, basic -loop also fine case).","code":"# matrix of minimum allowed N for each j, repeated K times N_min <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J) xi_sample <- 1 / (1 + 4 * N_min) # Sample values and call log_prob_NY for each sample, # store N-values in a K-by-J matrix, and log(p_NY)-values in a vector log_PY N <- matrix(rgeom(..., prob = xi_sample), K, J) + N_min log_PY <- ... # Subtract the sampling log-probabilities log_PY - rowSums(dgeom(N - N_min, prob = xi_sample, log = TRUE))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-n_j","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(N_j\\)","title":"StatComp Project Example: Numerical statistics","text":"lab, let \\(\\xi=1/1001\\), \\(=b=1/2\\). Use arch_importance wquantile function type=1 (see help text information) compute credible intervals \\(N_j\\). Start including data single excavation, observe interval \\(N_1\\) changes data excavations added. Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example.html","id":"credible-intervals-for-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic Sea","what":"Credible intervals for \\(\\phi\\)","title":"StatComp Project Example: Numerical statistics","text":"Update2 arch_importance function add column Phi samples conditional distribution \\(\\mathsf{Beta}(\\widetilde{},\\widetilde{b})\\) row importance sample. Use updated function construct credible intervals \\(\\phi\\), one \\(J=1\\), one \\(J=4\\). Present discuss results.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-approximation-assessment","dir":"Articles","previous_headings":"","what":"Confidence interval approximation assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"investigate behaviour three approximate methods confidence interval construction expectation parameter \\(\\lambda\\) model observations \\(y_i\\sim\\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\). denote three methods given assignment \\(\\textrm{CI}_1\\), \\(\\textrm{CI}_2\\), \\(\\textrm{CI}_3\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"theoretical-interval-width-equivalence","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Theoretical interval width equivalence","title":"StatComp Project Example Solution: Numerical Statistics","text":"Let \\(z=z_{1-\\alpha/2}=-z_{\\alpha/2}\\), \\(z_p\\) (lower) \\(p\\)-quantile standard \\(\\textrm{N}(0,1)\\) distribution. , \\(\\overline{y} > z^2/n\\), Methods 1 2 pois_CI constructing confidence interval \\(\\lambda\\) observations \\(y_i\\sim \\textrm{Pois}(\\lambda)\\), \\(=1,\\dots,n\\), given \\[\\begin{align*} \\textrm{CI}_1 &= (\\overline{y} - z\\sqrt{\\overline{y}/n}, \\overline{y} + z\\sqrt{\\overline{y}/n}), \\\\ \\textrm{CI}_2 &= ([\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2, [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2). \\end{align*}\\] interval width \\(\\textrm{CI}_1\\) \\(2z\\sqrt{\\overline{y}/n}\\). \\(\\textrm{CI}_2\\), width \\[\\begin{align*} [\\sqrt{\\overline{y}} + z/\\sqrt{4n}]^2 - [\\sqrt{\\overline{y}} - z/\\sqrt{4n}]^2 &= [\\overline{y} + 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] - [\\overline{y} - 2z\\sqrt{\\overline{y}}/\\sqrt{4n} + z^2/(4n)] \\\\ &= 4z\\sqrt{\\overline{y}}/\\sqrt{4n} \\end{align*}\\] since remaining terms cancel . Finally \\(4/\\sqrt{4}=2\\), see width \\(\\textrm{CI}_2\\) interval \\(\\textrm{CI}_1\\), observed value \\(\\overline{y}>z^2/n\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-interval-coverage-and-width-assessment","dir":"Articles","previous_headings":"Confidence interval approximation assessment","what":"Confidence interval coverage and width assessment","title":"StatComp Project Example Solution: Numerical Statistics","text":"interested behaviour three confidence interval methods far away asymptotic limit. Instead assuming \\(n\\rightarrow\\infty\\) \\(\\lambda\\rightarrow\\infty\\) instead look case \\(n=2\\), \\(\\lambda=3\\), discrete nature sampling distribution maximum likelihood estimator \\(\\widehat{\\lambda}=\\overline{y}\\) still apparent. my_code.R file, ’ve defined two helper functions, multi_pos_CI tidy_multi_pois_CI, order simulate \\(m\\) realisations model, compute three types approximate confidence intervals desired confidence level (also called target nominal confidence level) 90%, corresponding nominal error probability \\(\\alpha=0.1\\). get reasonable precision results, use \\(m=100000\\). See code appendix short analysis code. assess quality methods, compute estimate coverage probability proportion \\(m\\) simulations intervals cover true \\(\\lambda\\) value, Coverage table . also compute median width intervals, denoted MedWidth. extra details, estimate probability lower endpoint large, OutsideLower, upper endpoint low, OutsideUpper. Finally, compute median lower upper endpoints, denoted MedLower MedUpper table. expected theoretical calculations intervals don’t reach \\(0\\) left, median width methods 1 2 identical, whereas method 3 widths slightly larger. see methods 1 3 low high coverage, respectively, compared nominal overage \\(90\\%\\), method 2 coverage close nominal, \\(89.58\\%\\). ’s clear method 1 make ’s mistakes upper end, much \\(15\\%\\) probability low upper endpoint. contrast, method 3 overly cautious upper end, just \\(1.5\\%\\) error probability upper tail, expense wider intervals, sometimes stretching way \\(+\\infty\\). Method 2 appears strike good balance coverage probability interval widths, lower tail error probability almost method 3 (ca \\(4\\%\\)), moving upper endpoint left, thereby reducing interval widths increasing upper tail error probability around \\(6\\%\\). result, method 2 nearly equal tail error probabilities (perfect equality 5%) test case \\(n=2\\), \\(\\lambda=3\\), appears best three methods capturing behaviour sampling distribution \\(\\lambda\\)-estimator. figure , ’ve plotted empirical cumulative distribution functions interval widths three methods, clearly shows discrete nature interval contructions, since based \\(\\overline{y}\\), clear discrete set possible outcomes. equal width property methods 1 2 breaks small values \\(\\overline{y}\\), due handle intervals gone zero simplistic implementation. summary, method 2 appears preferable, despite three methods based asympototic maximum likelihood theory. small sample sizes, asymptotic theory doesn’t tell whole story, instead one look carefully problem structure order get reliable estimates.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology-in-the-baltic-sea","dir":"Articles","previous_headings":"","what":"Archaeology in the Baltic sea","title":"StatComp Project Example Solution: Numerical Statistics","text":"Pairs counts \\((Y_{j,1},Y_{j,2})\\) left right femurs archaeological excavations modelled conditionally independent realisations \\(\\textrm{Binom}(N_j,\\phi)\\), \\(N_j\\) original number people buried excavtion site, \\(\\phi\\) probability finding buried femur. prior distribution \\(N_j\\), \\(j=1,\\dots,J\\), \\(\\textrm{Geom}(\\xi)\\), \\(\\xi=1/(1+1000)\\) (\\(\\textrm{E}(N_j)=1000\\) priori), detection probability, \\(\\phi\\sim\\textrm{Beta}(\\frac{1}{2},\\frac{1}{2})\\). denote \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\) \\(\\boldsymbol{Y}=\\{(Y_{1,1},Y_{1,2}),\\dots,(Y_{J,1},Y_{J,2})\\}\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"joint-probability-function","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Joint probability function","title":"StatComp Project Example Solution: Numerical Statistics","text":"Taking product prior density \\(\\phi\\), prior probabilities \\(N_j\\), conditional observation probabilities \\(N_{j,}\\) integrating \\(\\phi\\), get joint probability function \\((\\boldsymbol{N},\\boldsymbol{Y})\\) \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\int_0^1 p_\\phi(\\phi) \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 p(Y_{j,}|N_j,\\phi)\\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{-1}(1-\\phi)^{b-1}}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\phi^{Y_{j,}}(1-\\phi)^{N_j-Y_{j,}}   \\,\\mathrm{d}\\phi \\\\ &= \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(,b)} \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} , \\end{aligned} \\] \\(\\widetilde{} = +\\sum_{j=1}^J\\sum_{=1}^2 Y_{j,}\\) \\(\\widetilde{b} = b+\\sum_{j=1}^J\\sum_{=1}^2 (N_j - Y_{j,})\\). integral can renormalised integral distribution probability density function \\(\\textrm{Beta}(\\widetilde{},\\widetilde{b})\\) distribution: \\[ \\begin{aligned} \\textrm{P}(\\boldsymbol{N},\\boldsymbol{Y}) &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\int_0^1 \\frac{\\phi^{\\widetilde{}-1}(1-\\phi)^{\\widetilde{b}-1}}{B(\\widetilde{},\\widetilde{b})}   \\,\\mathrm{d}\\phi\\,   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} \\\\ &= \\frac{B(\\widetilde{},\\widetilde{b})}{B(,b)}   \\prod_{j=1}^J p(N_j) \\prod_{=1}^2 {N_j \\choose Y_{j,}} . \\end{aligned} \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"credible-intervals-for-n_j-and-phi","dir":"Articles","previous_headings":"Archaeology in the Baltic sea","what":"Credible intervals for \\(N_j\\) and \\(\\phi\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"order obtain posterior credible intervals unknown quantities \\(\\boldsymbol{N}=\\{N_1,\\dots,N_J\\}\\), \\(\\phi\\), implement importance sampling method sample \\(N_j\\) shifted Geometric distribution smallest value equal smallest valid \\(N_j\\), given observed counts, expectation corresponding \\(\\phi=1/4\\). sampling \\(\\boldsymbol{N}\\) values, corresponding importance weights \\(w_k\\), stored Log_Weights equal \\(\\log(w_k)\\) normalised subtracting larges \\(\\log(w_k)\\) value. Finally, \\(\\phi\\) values sampled directly conditional posterior distribution \\((\\phi|\\boldsymbol{N},\\boldsymbol{Y})\\sim\\textrm{Beta}\\left(\\frac{1}{2}+\\sum_{j,} Y_{,j},\\frac{1}{2}+\\sum_{j,} (N_j-Y_{,j})\\right)\\). implementation arch_importance() function my_code.R shown code appendix, produces importance sample size \\(K\\). achieve relatively reliable results, use \\(K=100000\\), \\(J=1\\), \\(2\\), \\(3\\), \\(4\\), takes ca 4 seconds computer. First, use stat_ewcdf plot emprirical cumulative distribution functions importance weighted posterior samples \\(N_1\\), \\(J=1,\\dots,4\\). can see random variability increases larger \\(J\\), indicating lower efficiency importance sampler. Also included CDF prior distribution \\(N_1\\). Except lower end distribution, ’s immediately clear effect observations distribution.  Next corresponding plot \\(\\phi\\). contrast \\(N_1\\), ’s clear observation pairs get, concentrated distribution \\(\\phi\\) gets. new observation pair also influences particular lower end distribution, lower quantiles clearly increasing \\(J=4\\) even though upper quantiles remain relatively stable \\(J\\geq 2\\).  Using wquantile extract lower upper \\(5\\%\\) quantiles obtain approximate \\(90\\%\\) posterior credible intervals \\(N_1\\) \\(\\phi\\) shown following table. can see credible intervals \\(N_1\\) strongly linked information posterior distribution \\(\\phi\\). Interestingly, due lowering lower endpoint \\(\\phi\\) \\(J=2\\) \\(3\\), width \\(N_1\\) interval actually increases add information second third. stark contrast common intuition expect uncertainty decrease collect data. behaviour partly due random variability inherent small sample, also due strong non-linearity estimation problem, products \\(\\phi N_j\\) much easier estimate individual \\(\\phi\\) \\(N_j\\) parameters. \\(J=4\\), interval width \\(N_1\\) finally decreases, well width \\(\\phi\\) interval. Looking values observations, can see \\(J=4\\) case much larger values cases, leads providing relatively information \\(\\phi\\) parameter. practical point view, ’s clear estimation problem easy solution. Even multiple excavations, assuming model appropriate value \\(\\phi\\) \\(\\phi\\) every excavation, gain modest amount additional information. positive side, model good job approximating reality, borrowing strength across multiple excavations improve estimates uncertainty information excavation. improvements, one likely need closely study excavation precedures, gain better understanding data generating process, might lead improved statistical methods archaeologically relevant outcomes.","code":"## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor? ## The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor?"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"effective-sample-size","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Effective sample size","title":"StatComp Project Example Solution: Numerical Statistics","text":"Using method computing effective importance sample size later course can see even efficient method, sampling inefficient \\(J=4\\), indicating improved choice importance distribution \\(N_j\\) improve precision accuracy results: , EffSampleSize approximate effective sample size \\((\\sum_k w_k)^2/\\sum w_k^2\\), RelEff relative efficiency, defined ratio EffSampleSize \\(K=100000\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"profile-likelihood-method-for-frequentist-confidence-interval-for-textrmbinomnp","dir":"Articles","previous_headings":"Archaeology in the Baltic sea > Extra","what":"Profile likelihood method for frequentist confidence interval for \\(\\textrm{Binom}(n,p)\\)","title":"StatComp Project Example Solution: Numerical Statistics","text":"(actually range(y) depends n may mean l0 justified example)","code":"## [1] 274"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"function-definitions","dir":"Articles","previous_headings":"Code appendix","what":"Function definitions","title":"StatComp Project Example Solution: Numerical Statistics","text":"","code":"# Finn Lindgren (s0000000, finnlindgren)  # Part 1 ####  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using one of the three methods implemented in pois_CI # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals #   type: 1, 2, 3, or 4, indicating which CI method to use # # Output: #   A data frame with `m` rows and columns `Lower` and `Upper`, with one #   confidence interval per row  multi_pois_CI <- function(m, n, lambda, alpha, type) {   Lower = numeric(m)   Upper = numeric(m)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     CI <- pois_CI(y = y, alpha = alpha, type = type)     Lower[m_idx] <- CI[1]     Upper[m_idx] <- CI[2]   }   result <- data.frame(Lower = Lower, Upper = Upper) }  tidy_multi_pois_CI_alt <- function(m, n, lambda, alpha) {   Lower <- matrix(0, m, 4)   Upper <- matrix(0, m, 4)   for (m_idx in seq_len(m)) {     y <- rpois(n, lambda = lambda)     for (type in 1:4) {       CI <- pois_CI(y = y, alpha = alpha, type = type)       Lower[m_idx, type] <- CI[1]       Upper[m_idx, type] <- CI[2]     }   }   data.frame(     Index = rep(1:m, times = 4),     Type = rep(1:4, each = m),     Lower = as.vector(Lower),     Upper = as.vector(Upper)   ) }  # Simulate samples of size n from Pois(lambda), m times, and construct # confidence intervals using all three methods implemented in pois_CI. # This implementation uses independent simulations for the three method, by # calling multi_pois_CI for each interval type. An alternative implementation # would be to use the same samples for all three methods. # # Input: #   m: The number of samples #   n: The size of each sample #   lambda: The True value of the lambda-parameter #   alpha: The target/nominal error probability for the confidence intervals # # Output: #   A data frame with `3m` rows and columns `Lower`, `Upper`, and `Type`, #   with one confidence interval per row, calculated with the method indicated #   in the `Type` column  tidy_multi_pois_CI <- function(m, n, lambda, alpha) {   result <-     rbind(       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 1         ),         Type = 1       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 2         ),         Type = 2       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 3         ),         Type = 3       ),       cbind(         multi_pois_CI(           m = m, n = n, lambda = lambda,           alpha = alpha, type = 4         ),         Type = 4       )     )   result }   # Part 2 ####  # Joint log-probability # # Usage: #   log_prob_NY(N, Y, xi, a, b) # # Calculates the joint probability for N and Y, when #   N_j ~ Geom(xi) #   phi ~ Beta(a, b) #   Y_{ji} ~ Binom(N_j, phi) #   For j = 1,...,J # # Inputs: #   N: vector of length J #   Y: data.frame or matrix of dimension Jx2 #   xi: The probability parameter for the N_j priors #   a, b: the parameters for the phi-prior # # Output: #   The log of the joint probability for N and Y #   For impossible N,Y combinations, returns -Inf  log_prob_NY <- function(N, Y, xi, a, b) {   # Convert Y to a matrix to allow more vectorised calculations:   Y <- as.matrix(Y)   if (any(N < Y) || any(Y < 0)) {     return(-Inf)   }   atilde <- a + sum(Y)   btilde <- b + 2 * sum(N) - sum(Y) # Note: b + sum(N - Y) would be more general   lbeta(atilde, btilde) - lbeta(a, b) +     sum(dgeom(N, xi, log = TRUE)) +     sum(lchoose(N, Y)) }  # Importance sampling for N,phi conditionally on Y # # Usage: #   arch_importance(K,Y,xi,a,b) # # Inputs: #   Y,xi,a,b have the same meaning and syntax as for log_prob_NY(N,Y,xi,a,b) #   The argument K defines the number of samples to generate. # # Output: #  A data frame with K rows and J+1 columns, named N1, N2, etc, #  for each j=1,…,J, containing samples from an importance sampling #  distribution for Nj, #  a column Phi with sampled phi-values, #  and a final column called Log_Weights, containing re-normalised #  log-importance-weights, constructed by shifting log(w[k]) by subtracting the #  largest log(w[k]) value, as in lab 4.  arch_importance <- function(K, Y, xi, a, b) {   Y <- as.matrix(Y)   J <- nrow(Y)   N_sample <- matrix(rep(pmax(Y[, 1], Y[, 2]), each = K), K, J)   xi_sample <- 1 / (1 + 4 * N_sample)    N <- matrix(rgeom(K * J, prob = xi_sample), K, J) + N_sample   colnames(N) <- paste0(\"N\", seq_len(J))    Log_Weights <- vapply(     seq_len(K),     function(k) log_prob_NY(N[k, , drop = TRUE], Y, xi, a, b),     0.0) -     rowSums(dgeom(N - N_sample, prob = xi_sample, log = TRUE))    phi <- rbeta(K, a + sum(Y), b + 2 * rowSums(N) - sum(Y))    cbind(as.data.frame(N),         Phi = phi,         Log_Weights = Log_Weights - max(Log_Weights)) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"confidence-intervals","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Confidence intervals","title":"StatComp Project Example Solution: Numerical Statistics","text":"Poisson parameter interval simulations: Summarise coverage probabilities interval widths: Plot CDFs CI width distributions:","code":"n <- 2 lambda_true <- 3 result <- tidy_multi_pois_CI_alt(100000, n, lambda_true, alpha = 0.1) result %>%   group_by(Type) %>%   summarise(     Coverage = mean((Lower <= lambda_true) & (lambda_true <= Upper)),     MedWidth = median(Upper - Lower),     OutsideLower = mean(lambda_true < Lower),     OutsideUpper = mean(lambda_true > Upper),     MedLower = median(Lower),     MedUpper = median(Upper)   ) %>%   knitr::kable()"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Project_Example_Solution.html","id":"archaeology","dir":"Articles","previous_headings":"Code appendix > Analysis code","what":"Archaeology","title":"StatComp Project Example Solution: Numerical Statistics","text":"Compute credible intervals \\(N_j\\) different amounts data, \\(J\\): completeness, compute credible intervals \\(N_j\\), \\(1\\leq J\\), \\(J=1,\\dots,4\\), analysis focuses \\(N_1\\) \\(\\phi\\): Restructure interval data.frame focus \\(N_1\\) \\(\\phi\\) :","code":"xi <- 1 / 1001 a <- 0.5 b <- 0.5  K <- 1000000 NW1 <- arch_importance(K, arch_data(1), xi, a, b) NW2 <- arch_importance(K, arch_data(2), xi, a, b) NW3 <- arch_importance(K, arch_data(3), xi, a, b) NW4 <- arch_importance(K, arch_data(4), xi, a, b) CI <- rbind(   NW1 %>%     summarise(       J = 1,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = NA,       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW2 %>%     summarise(       J = 2,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = NA,       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW3 %>%     summarise(       J = 3,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = NA,       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ),   NW4 %>%     summarise(       J = 4,       End = c(\"Lower\", \"Upper\"),       N1 = wquantile(N1,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N2 = wquantile(N2,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N3 = wquantile(N3,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       N4 = wquantile(N4,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 1, weights = exp(Log_Weights)       ),       Phi = wquantile(Phi,         probs = c(0.05, 0.95), na.rm = TRUE,         type = 7, weights = exp(Log_Weights)       )     ) ) CI_ <- CI %>%   pivot_longer(     cols = c(N1, N2, N3, N4, Phi),     names_to = \"Variable\",     values_to = \"value\"   ) %>%   pivot_wider(     names_from = End,     values_from = value   ) %>%   filter(!is.na(Lower)) %>%   mutate(Width = Upper - Lower) knitr::kable(CI_ %>% filter(Variable %in% c(\"N1\", \"Phi\")) %>%   arrange(Variable, J))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"deriving-the-hessian","dir":"Articles","previous_headings":"","what":"Deriving the Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Define individual terms \\(f_k=y_k\\log(1+e^{-\\eta_k}) + (N-y_k)\\log(1+e^{\\eta_k})\\), \\(f=\\sum_{k=1}^n f_k\\). know \\(\\partial\\eta_k/\\partial\\theta_1\\equiv 1\\) \\(\\partial\\eta_k/\\partial\\theta_2=k\\). derivatives respect \\(\\theta_1\\) \\(\\theta_2\\) can obtain chain rule: \\[\\begin{align*} \\frac{df_k}{d\\theta_i} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\frac{\\partial f_k}{\\partial\\eta_k} \\\\   &= \\frac{\\partial \\eta_k}{\\partial \\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k}}{1+e^{-\\eta_k}} +   (N-y_k) \\frac{e^{\\eta_k}}{1+e^{\\eta_k}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   y_k \\frac{-e^{-\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   (N-y_k) \\frac{e^{\\eta_k/2}}{e^{-\\eta_k/2}+e^{\\eta_k/2}}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   - y_k \\frac{e^{-\\eta_k/2}+e^{\\eta_k/2}}{e^{\\eta_k/2}+e^{-\\eta_k/2}} +   N \\frac{1}{e^{-\\eta_k}+1}   \\right] \\\\   &= \\frac{\\partial\\eta_k}{\\partial\\theta_i} \\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\ \\end{align*}\\] Since derivatives \\(\\eta_k\\) respect \\(\\theta_1\\) \\(\\theta_2\\) depend values \\(\\theta_1\\) \\(\\theta_2\\), second order derivatives \\[\\begin{align*} \\frac{d^2f_k}{d\\theta_i d\\theta_j} &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{\\partial}{\\partial\\eta_k}\\left[   \\frac{N}{e^{-\\eta_k}+1} - y_k   \\right] \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N e^{-\\eta_k}}{(e^{-\\eta_k}+1)^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{(e^{-\\eta_k/2}+e^{\\eta_k/2})^2}   \\\\   &=   \\frac{\\partial\\eta_k}{\\partial\\theta_i}   \\frac{\\partial\\eta_k}{\\partial\\theta_j}   \\frac{N}{4\\cosh(\\eta_k/2)^2}   \\\\ \\end{align*}\\] Plugging \\(\\theta\\)-derivatives gives Hessain contribution term \\(f_k\\) \\[   \\frac{N}{4\\cosh(\\eta_k/2)^2} \\begin{bmatrix}1 & k \\\\ k & k^2\\end{bmatrix} . \\]","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"positive-definite-hessian","dir":"Articles","previous_headings":"","what":"Positive definite Hessian","title":"Quiz 1 Solution (StatComp 2021/22)","text":"show total Hessian \\(f\\) positive definite \\(n \\geq 2\\), define vectors \\(\\boldsymbol{u}_k=\\begin{bmatrix}1 \\\\ k\\end{bmatrix}\\) \\(d_k=\\frac{N}{4\\cosh(\\eta_k/2)^2}\\). Define 2-\\(n\\) matrix \\(\\boldsymbol{U}=\\begin{bmatrix}\\boldsymbol{u}_1 & \\boldsymbol{u}_2 & \\cdots & \\boldsymbol{u}_n\\end{bmatrix}\\) diagonal matrix \\(\\boldsymbol{D}\\) \\(D_{ii}=d_i\\). product \\(\\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{U}^\\top\\) another way writing Hessian \\(f\\). Since vectors \\(\\boldsymbol{u}_k\\) non-parallel, \\(d_i\\) strictly positive combinations \\(\\theta_1\\) \\(\\theta_2\\), matrix full rank (rank 2) \\(n \\geq 2\\), positive definite.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"alternative-reasoning","dir":"Articles","previous_headings":"","what":"Alternative reasoning","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Hessian \\(f_k\\) positive semi-definite, since can written \\(d_k\\boldsymbol{u}_k\\boldsymbol{u}_k^\\top\\) \\(d_k > 0\\) vector \\(\\boldsymbol{u}_k\\). sum positive definite matrix positive semi-definite matrix positive definite, ’s sufficient prove sum first two terms positive definite. positive scaling constant \\(w\\), determinant \\[ \\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}+w\\begin{bmatrix}1 & 2\\\\2 & 4\\end{bmatrix} \\] \\((1+w)(1+4w)-(1+2w)^2=1+5w+4w^2-1-4w-4w^2=w > 0\\). means (positively) weighted sum two matrices positive definite (since positive semi-definite, positive determinant rules ot sum positive semi-definite). proves total Hessian positive definite \\(n\\geq 2\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Quiz1Solution.html","id":"remark","dir":"Articles","previous_headings":"","what":"Remark","title":"Quiz 1 Solution (StatComp 2021/22)","text":"Note first proof positive definiteness, didn’t actually need know specific values \\(\\boldsymbol{u}_k\\) vectors, proportional gradients fo \\(\\eta_k\\) respect \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)\\); sufficient non-parallel, ensuring \\(\\boldsymbol{U}\\) full rank. means linear model \\(\\eta_k\\) set parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_p)\\) leads positive definite Hessian model, collection gradient vectors \\((\\eta_1,\\dots,\\eta_n)\\) respect \\(\\boldsymbol{\\theta}\\) collective rank least \\(p\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"three-alternatives-for-poisson-parameter-confidence-intervals","dir":"Articles","previous_headings":"","what":"Three alternatives for Poisson parameter confidence intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"case 3, negated log-likelihood \\[ \\widetilde{l}(\\theta) = n e^\\theta - \\theta \\sum_{=1}^n y_i + \\text{constant} \\] 1st order derivative \\[ \\frac{\\partial}{\\partial\\theta}\\widetilde{l}(\\theta) = n e^\\theta - \\sum_{=1}^n y_i \\] shows \\(\\widehat{\\theta}_\\text{ML}=\\log(\\overline{y})\\), 2nd order derivative \\[ \\frac{\\partial^2}{\\partial\\theta^2}\\widetilde{l}(\\theta) = n e^\\theta \\] equal \\(n\\lambda\\) \\(y_i\\) values, inverse expected Hessian \\(1/(n\\lambda)\\).","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"interval-construction","dir":"Articles","previous_headings":"Three alternatives for Poisson parameter confidence intervals","what":"Interval construction","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"can test interval construction methods following code. methods always produce valid intervals. \\(\\overline{y}=0\\), first method produces single point “interval”. third method fails \\(\\overline{y}=0\\), due log zero. can happen \\(n\\) \\(\\lambda\\) close zero.","code":"CI1 <- function(y, alpha = 0.05) {   n <- length(y)   lambda_hat <- mean(y)   theta_interval <-     lambda_hat - sqrt(lambda_hat / n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0) } CI2 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- sqrt(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(4 * n) * qnorm(c(1 - alpha / 2, alpha / 2))   pmax(theta_interval, 0)^2 } CI3 <- function(y, alpha = 0.05) {   n <- length(y)   theta_hat <- log(mean(y))   theta_interval <-     theta_hat - 1 / sqrt(exp(theta_hat) * n) * qnorm(c(1 - alpha / 2, alpha / 2))   exp(theta_interval) } y <- rpois(n = 5, lambda = 2) print(y) CI <- rbind(   \"Method 1\" = CI1(y),   \"Method 2\" = CI2(y),   \"Method 3\" = CI3(y) ) colnames(CI) <- c(\"Lower\", \"Upper\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"bayesian-credible-intervals","dir":"Articles","previous_headings":"","what":"Bayesian credible intervals","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"probability theory, know \\(p(\\theta)=p(\\lambda) \\frac{d\\lambda(\\theta)}{d\\theta}\\), \\[ p(\\theta) = \\exp(-\\lambda) \\exp(\\theta) = \\exp\\left( \\theta-ae^\\theta \\right) \\]","code":"n <- 5 lambda <- 10 y <- rpois(n, lambda) y # Actual values will depend on if set.seed() was used at the beginning of the document ## [1] 11  7 11  8  8"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"importance-sampling","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Importance sampling","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"First, sample Gaussian approximation posterior distribution. avoid numerical problems, calculate log unnormalised weights, shift results get numerically sensible scale exponentiate. credible interval \\(\\theta\\) can now extracted quantiles weighted sample, \\(\\lambda\\)-interval obtained transformation \\(\\lambda=\\exp(\\theta)\\): code steps following:","code":"a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval ## [1] 1.873696 2.448506 lambda_interval <- exp(theta_interval) lambda_interval ## [1]  6.512323 11.571043 a <- 1/5 m <- 20000 x <- rnorm(m, mean = log(1+sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y))) log_weights <- (x * (1 + sum(y)) - (a + n) * exp(x)) -  dnorm(x, mean = log(1 + sum(y)) - log(a + n), sd = 1 / sqrt(1 + sum(y)), log = TRUE) weights <- exp(log_weights - max(log_weights)) theta_interval <- wquantile(x, probs = c(0.025, 0.975), weights = weights) theta_interval lambda_interval <- exp(theta_interval) lambda_interval"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/T4sol.html","id":"cumulative-distribution-function-comparison","dir":"Articles","previous_headings":"Bayesian credible intervals","what":"Cumulative distribution function comparison","title":"Tutorial 04: Uncertainty and integration (full solution)","text":"figure shows empirical distributions unweighted weighted samples \\(\\{x_k\\}\\) (weights \\(\\{w_k\\}\\)), together theoretical posterior empirical distribution function. weighted, importance sampling, version virtually indistinguishable true posterior distribution. unweighted sample close true posterior distribution; model, Gaussian approximation posterior distribution \\(\\log(\\lambda)\\) excellent approximation even add importance sampling step.  lower figure, importance weights normalised average, values 1 mean corresponding sample values occur often true, target, distribution raw samples, values 1 mean sample values occur less frequently target distribution. scaled weights shown logarithmic scale.","code":"# Plotting updated to include a plot of the importance weights p1 <-   ggplot(data.frame(lambda = exp(x), weights = weights)) +   ylab(\"CDF\") +   geom_function(fun = pgamma, args = list(shape = 1 + sum(y), rate = a + n),                 mapping = aes(col = \"Theory\")) +   stat_ewcdf(aes(lambda, weights = weights, col = \"Importance\")) +   stat_ecdf(aes(lambda, col = \"Unweighted\")) +   scale_x_log10(limits = c(3, 20)) +   labs(colour = \"Type\")  p2 <- ggplot(data.frame(lambda = exp(x), weights = weights),              mapping = aes(lambda, weights / mean(weights))) +   ylab(\"Importance weights\") +   geom_point() +   geom_line() +   geom_hline(yintercept = 1) +   scale_y_log10() +   scale_x_log10(limits = c(3, 20))  # The patchwork library provides a versatile framework # for combining multiple ggplot figures into one. library(patchwork) (p1 / p2) ## Warning: The following aesthetics were dropped during statistical transformation: ## weights ## ℹ This can happen when ggplot fails to infer the correct grouping structure in ##   the data. ## ℹ Did you forget to specify a `group` aesthetic or to convert a numerical ##   variable into a factor?"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation.","code":"newdata <- data.frame(z = -10:20)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models","text":"can now use something like following code plot results: Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"local-computer-rstudio-installation","dir":"Articles","previous_headings":"Startup","what":"Local computer RStudio installation","title":"Tutorial 01: R and linear models (solutions)","text":"’ve followed links Learn page technical setup installed R RStudio, option running RStudio computer. Choose folder/location file system store course files, RStudio, create new Project (File\\(\\rightarrow\\)New Project\\(\\rightarrow\\)New Directory\\(\\rightarrow\\)New Project) Make sure choose location part path contains blank spaces, since can sometimes cause problems. Name folder Lab01, example. Next time want work course files, use File\\(\\rightarrow\\)Open Project instead (, system linked .Rproj files RStudio, can open RStudio opening .Rproj file project folder) RStudio Console window, run devtools::install_github(\"finnlindgren/StatCompLab\", dependencies = TRUE) Go Learn page open Lecture01.pdf lecture slides can refer easily lab. Read pages formulas functions continue lab.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"some-useful-menu-options-buttons-and-keyboard-shortcuts","dir":"Articles","previous_headings":"Startup","what":"Some useful menu options, buttons, and keyboard shortcuts:","title":"Tutorial 01: R and linear models (solutions)","text":"Configuring code syntax diagnostics: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Diagnostics allows turn margin notes automatically alert potential code problems. Run current line code (selected section code) step next line: Press Run button top right corner editor window, Press Ctrl+Enter Run code current script file: Press Source button top right corner editor window, Press Ctrl+Shift+S, Press Ctrl+Shift+Enter Note: first two options disable normal automatic value printing; ’ll see ouput explicitly fed print(). third option displays code results Console window. styler package (already available rstudio.cloud. Use install.packages(\"styler\") R ’re running local installation). enable options reformat script code (usually) readable code, pressing Addins button main program menu choosing e.g. Style active file.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"first-steps","dir":"Articles","previous_headings":"Startup","what":"First steps","title":"Tutorial 01: R and linear models (solutions)","text":"Create new R script file: File\\(\\rightarrow\\)New File\\(\\rightarrow\\)R Script Save new R script file: File\\(\\rightarrow\\)Save, Press Ctrl+S, choose descriptive filename (e.g. lab_1_code.R) lab, remember save script file regularly, avoid losing work ( unsaved changes may still remembered system, much safer explicitly save files). several ways view tutorial document; either StatCompLab webpage, running learnr::run_tutorial(\"Tutorial01\", package = \"StatCompLab\") RStudio, vignette(\"Tutorial01\", package = \"StatCompLab\"). (Note: run_tutorial option contains additional interactive questions exercises supported two methods.)","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"linear-model-estimation","dir":"Articles","previous_headings":"","what":"Linear model estimation","title":"Tutorial 01: R and linear models (solutions)","text":"start simple linear model \\[ \\begin{aligned} y_i &= \\beta_0 + z_i \\beta_z + e_i , \\end{aligned} \\] \\(y_i\\) observed values, \\(z_i\\) observed values (“covariates”) believe linear relationship \\(y_i\\), \\(e_i\\) observation noise components variance \\(\\sigma_e^2\\), \\(\\beta_0\\), \\(\\beta_z\\), \\(\\sigma_e^2\\) model parameters. First, generate synthetic data use developing code estimating models type. Enter following code script file run e.g. Ctrl+Enter: rep(), length(), rnorm() ? Look help pages running ?rep, ?rnorm, ?length interactive Console window, searching Help pane. can plot data “base graphics”,  ggplot,  simple plot, ggplot approach may seem complicated necessary, complex plots ggplot make plotting much easier. code , first supply data set, add information plot , using grammar graphics. See ?aes information parameters controlling aesthetics geom adds points plot.","code":"z <- rep(1:10, times = 10) data <- data.frame(z = z, y = 2 * z + rnorm(length(z), sd = 4)) plot(data$z, data$y) # Put the first two lines (library and theme_set) at the beginning of your script file. # They only need to be run once in your R session! # Load the ggplot2 package library(ggplot2) # Change the default theme (avoids grey background in the plots): theme_set(theme_bw()) # Now we can plot: ggplot(data) + geom_point(aes(z, y))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"estimate-the-model","dir":"Articles","previous_headings":"Linear model estimation","what":"Estimate the model","title":"Tutorial 01: R and linear models (solutions)","text":"Use lm() function estimate model save estimated model variable called mod.","code":"mod <- lm(y ~ z, data)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plot-the-result","dir":"Articles","previous_headings":"Linear model estimation","what":"Plot the result","title":"Tutorial 01: R and linear models (solutions)","text":"Now want plot linear predictor model function \\(z\\), \\(z=-10\\) \\(z=20\\). First, create new data.frame: Note don’t corresponding \\(y\\)-values yet! Use predict(), plot(), geom_line() plot linear predictor original data points single figure. Also add true predictor help geom_abline(intercept = ..., slope = ...) (use col parameter set different color!). Create new data frame supply directly geom_line override original data: Structure code adding linebreak + operation. Solution:","code":"newdata <- data.frame(z = -10:20) data_pred <- cbind(newdata,                    data.frame(fit = predict(mod, newdata))) ggplot(data) +   geom_point(aes(z, y)) +   geom_line(aes(z, fit), data = data_pred) +   geom_abline(intercept = 0, slope = 2, col = \"red\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"prediction-intervals","dir":"Articles","previous_headings":"","what":"Prediction intervals","title":"Tutorial 01: R and linear models (solutions)","text":"Now modify plotting code also show prediction intervals. Hint: use interval=\"prediction\" parameter predict(), combine output (now matrix, need convert data.frame first) newdata help cbind. resulting data.frame variables named z, fit, lwr, upr, whole object called pred. Solution: Use geom_ribbon(data = ???, aes(x = ???, ymin = ???, ymax = ???), alpha = 0.25) function add prediction intervals figure. alpha parameter sets “transparency” ribbon. Experiment different values 0 1. result look similar :  “base graphics”, following produce similar result, without shaded bands:","code":"pred <- cbind(   newdata,   predict(mod, newdata, interval = \"prediction\") ) plot(newdata$z, pred[, \"fit\"], type = \"l\", ylim = range(pred)) lines(newdata$z, pred[, \"lwr\"], lty = 2) lines(newdata$z, pred[, \"upr\"], lty = 2)  points(data$z, data$y, pch = 20) abline(0, 2, col = 2)"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models","dir":"Articles","previous_headings":"","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Note: section introduces functions discussed detail week 2. Let’s say want redo data analysis using different models (, know true model generated synthetic data!). Instead copying plotting code new model, let’s create function instead! Start following skeleton code, fill missing bits: Solution: xname set call function specific data? able run following code regenerate previous figure:  Remark: can use + geom_* technique add plot drawn plot_predictions even though don’t directly call ggplot() ? Every function returns last computed object output. case, ’s ggplot object, can use just spelled call ggplot(). also means principle write function adds features plot, taking existing ggplot object input, making complex plotting structured modular.","code":"plot_predictions <- function(x, newdata, xname = \"x\") {   pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   ggplot(pred) +     geom_line(aes_string(xname, \"fit\")) +     geom_ribbon(aes_string(x = xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25) } plot_predictions(mod, newdata, xname = \"z\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") ## Warning: `aes_string()` was deprecated in ggplot2 3.0.0. ## ℹ Please use tidy evaluation ideoms with `aes()`"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"further-plotting-details","dir":"Articles","previous_headings":"Multiple models","what":"Further plotting details","title":"Tutorial 01: R and linear models (solutions)","text":"can change plot labels add title adding calls xlab(), ylab(), ggtitle() plot: passes additional parameters plot() function. provided solution generalised version plotting function, also adds confidence intervals predictor curve plot. Solution:","code":"# Extended version of the solution: plot_prediction <- function(x, newdata, xname = \"x\",                             xlab = NULL, ylab = NULL, ...) {   if (is.null(xlab)) {     xlab <- xname   }   if (is.null(ylab)) {     ylab <- \"Response\"   }    pred <- cbind(newdata,                 predict(x, newdata, interval = \"prediction\"))   pl <- ggplot() +     geom_line(data = pred,               aes_string(xname, \"fit\")) +     geom_ribbon(data = pred,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Also add the confidence intervals for the predictor curve   conf <- cbind(newdata,                 predict(x, newdata, interval = \"confidence\"))   pl <- pl +     geom_ribbon(data = conf,                 aes_string(xname, ymin = \"lwr\", ymax = \"upr\"), alpha = 0.25)    # Add lables:   pl + xlab(xlab) + ylab(ylab) }"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"quadratic-model","dir":"Articles","previous_headings":"Multiple models","what":"Quadratic model","title":"Tutorial 01: R and linear models (solutions)","text":"Use new function plot predictions quadratic model () syntax means don’t create separate data variable equal square z-values; lm() function create internally. result look similar :","code":"mod2 <- lm(y ~ 1 + z + I(z ^ 2), data) plot_prediction(mod2, newdata, xname = \"z\", ylab = \"y\") +   geom_point(data = data, aes(z, y)) +   geom_abline(intercept = 0, slope = 2, col = \"red\") +   ggtitle(\"Confidence and prediction intervals\")"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"multiple-models-1","dir":"Articles","previous_headings":"Multiple models","what":"Multiple models","title":"Tutorial 01: R and linear models (solutions)","text":"Finally, let’s estimate plot predictions four polynomial models, first creating list formulas: Look lecture slides use lapply() technique estimate four models store result variable called mods. Solution:","code":"formulas <- c(y ~ 1,               y ~ z,               y ~ z + I(z^2),               y ~ z + I(z^2) + I(z^3)) mods <- lapply(formulas, function(x) lm(x, data))"},{"path":"https://finnlindgren.github.io/StatCompLab/articles/Tutorial01Solutions.html","id":"plotting-multiple-results","dir":"Articles","previous_headings":"Multiple models","what":"Plotting multiple results","title":"Tutorial 01: R and linear models (solutions)","text":"can now use something like following code plot results:  Look help text seq_along()! Storing plot object, pl, explicitly “printing” sometimes needed ensure plots generated report document. Note: look ways combining multiple plots later. information displayed plots match intuition different models?","code":"for (k in seq_along(formulas)) {   pl <-     plot_prediction(mods[[k]], newdata, xname = \"z\", ylab = \"y\") +       geom_point(data = data, aes(z, y)) +       geom_abline(intercept = 0, slope = 2, col = \"red\") +       ggtitle(as.character(formulas[k]))   print(pl) }"},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Finn Lindgren. Author, maintainer.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lindgren F (2023). StatCompLab: Workshop Material UoE Statistical Computing. https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab.","code":"@Manual{,   title = {StatCompLab: Workshop Material For UoE Statistical Computing},   author = {Finn Lindgren},   year = {2023},   note = {https://finnlindgren.github.io/StatCompLab, https://github.com/finnlindgren/StatCompLab}, }"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"statcomplab","dir":"","previous_headings":"","what":"Workshop Material For UoE Statistical Computing","title":"Workshop Material For UoE Statistical Computing","text":"package collects workshop materials Statistical Computing (MATH10093) University Edinburgh 2021/22. tutorial documents browsable online https://finnlindgren.github.io/StatCompLab/ Contact Finn Lindgren, finn.lindgren@ed.ac.uk information. package version number system YearOfStudy.StudyWeek.MinorUpdate","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Workshop Material For UoE Statistical Computing","text":"can install (later upgrade) package GitHub : want view vignette versions tutorials within RStudio, need add build_vignettes argument:","code":"# If devtools isn't installed, first run install.packages(\"remotes\") remotes::install_github(\"finnlindgren/StatCompLab\") remotes::install_github(\"finnlindgren/StatCompLab\", build_vignettes = TRUE)"},{"path":"https://finnlindgren.github.io/StatCompLab/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Workshop Material For UoE Statistical Computing","text":"convenient way access tutorial documents run Tutorial tab upper right part RStudio. allows see tutorial text code hints code files, running code Console viewing plots. alternative methods also allows viewing documents, less convenient; vignettes show space plots, run_tutorial blocks Console window used run code.","code":"library(StatCompLab) # To install the vignette versions, add build_vignettes=TRUE to the install_github() call above. vignette(package = \"StatCompLab\") # List available vignettes vignette(\"Tutorial01\", \"StatCompLab\") # View a specific vignette # The following method blocks the Console from running other code; better to use the Tutorials pane instead library(learnr) available_tutorials(\"StatCompLab\") # List available tutorials run_tutorial(\"Tutorial01\", \"StatCompLab\") # Run a specific tutorial"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/StatEwcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"StatEwcdf ggproto object — StatEwcdf","title":"StatEwcdf ggproto object — StatEwcdf","text":"StatEwcdf ggproto object","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple binomial model likelihood — arch_loglike","title":"Multiple binomial model likelihood — arch_loglike","text":"Compute log-likelihood multiple Binom(N,phi) observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(param, y)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple binomial model likelihood — arch_loglike","text":"param Either vector two elements, N phi, data.frame two columns, named N phi y data vector","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiple binomial model likelihood — arch_loglike","text":"log-likelihood N,phi combination. implementation internally uses log-gamma function provide differentiable function N, allows optim() treat N continuous variable. N < max(y), log-likelihood defined -Inf.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/arch_loglike.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiple binomial model likelihood — arch_loglike","text":"","code":"arch_loglike(   param = data.frame(N = c(5, 10, 20, 40),                      phi = c(0.1, 0.2, 0.3, 0.4)),   y = c(2, 4)) #> [1] -10.324930  -3.626867  -5.618058 -25.216655"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate filament models — filament1_estimate","title":"Estimate filament models — filament1_estimate","text":"Estimate filament models different variance structure","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate filament models — filament1_estimate","text":"","code":"filament1_estimate(data, model)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate filament models — filament1_estimate","text":"data data.frame variables filament1 data set. Must columns CAD_Weight Actual_Weight model Either \"\" log-linear variance model, \"B\" proportional scaling error model","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate filament models — filament1_estimate","text":"estimation object suitable use filament1_predict()","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate filament models — filament1_estimate","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict values for a filament model — filament1_predict","title":"Predict values for a filament model — filament1_predict","text":"Uses estimated filament model predict new observations","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict values for a filament model — filament1_predict","text":"","code":"filament1_predict(object, newdata, alpha = 0.05, df = Inf, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict values for a filament model — filament1_predict","text":"object estimation object, like output filament1_estimate() newdata data use prediction. Must column CAD_Weight alpha target coverage error prediction intervals, Default: 0.05 df degrees freedom t-quantiles prediction interval construction, Default: Inf ... Additional arguments, currently ignored","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict values for a filament model — filament1_predict","text":"data.frame variables mean, sd, lwr, upr, summarising prediction distribution row newdata.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/filament1_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict values for a filament model — filament1_predict","text":"","code":"if (FALSE) { if(interactive()){  #EXAMPLE1  } }"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactive numerical optimisation explorer — optimisation","title":"Interactive numerical optimisation explorer — optimisation","text":"Shiny app exploring several numerical optimisation methods work","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/optimisation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactive numerical optimisation explorer — optimisation","text":"","code":"optimisation()"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate proper scores — proper_score","title":"Calculate proper scores — proper_score","text":"Calculates proper scores probabilistic predictions","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate proper scores — proper_score","text":"","code":"proper_score(type, obs, ...)"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate proper scores — proper_score","text":"type string indicating type score calculate. One \"se\", \"ae\", \"ds\", \"interval\". See Details . obs vector observations ... Additional named arguments needed type score, see Details .","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate proper scores — proper_score","text":"vector calculated scores, one observation","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate proper scores — proper_score","text":"score type takes additional arguments define prediction model information used score calculations. Unless specified otherwise, extra arguments either scalars (applying predictions) vectors length obs vector. se Squared error score. Requires mean, defining prediction mean ae Absolute error score. Requires median, defining prediction median ds Dawid-Sebastiani score. Requires mean sd, defining mean standard deviation predictions interval Squared error score. Requires lwr upr, defining lower upper prediction interval endpoints, alpha, defining prediction error probability targeted prediction intervals scores \\(S_{type}(F_i,y_i)\\) defined lecture notes, obs[] equal \\(y_i\\) prediction distribution \\(F_i\\) defined additional named arguments.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/proper_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate proper scores — proper_score","text":"","code":"# Five realisations of N(3, 3^2) obs <- rnorm(5, mean = 3, sd = 3)  # One prediction for each observation. Only one of the predictions describes # the true model F_mean <- 1:5 F_median <- 1:5 F_sd <- 1:5 F_lwr <- F_mean - F_sd * qnorm(0.9) F_upr <- F_mean - F_sd * qnorm(0.1)  # Compute the scores data.frame(   se = proper_score(\"se\", obs, mean = F_mean),   ae = proper_score(\"ae\", obs, median = F_median),   ds = proper_score(\"ds\", obs, mean = F_mean, sd = F_sd),   interval = proper_score(\"interval\", obs,                           lwr = F_lwr, upr = F_upr, alpha = 0.2) ) #>             se        ae        ds  interval #> 1 2.257158e+01 4.7509554 22.571577 37.257141 #> 2 3.516977e-01 0.5930410  1.474219  5.126206 #> 3 1.146117e+01 3.3854355  3.470688  7.689309 #> 4 7.889524e-04 0.0280883  2.772638 10.252413 #> 5 1.023489e+00 1.0116764  3.259815 12.815516"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute empirical weighted cumulative distribution — stat_ewcdf","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"Version ggplot2::stat_ecdf adds weights property observation, produce empirical weighted cumulative distribution function. empirical cumulative distribution function (ECDF) provides alternative visualisation distribution. Compared visualisations rely density (like geom_histogram()), ECDF require tuning parameters handles continuous discrete variables. downside requires training accurately interpret, underlying visual tasks somewhat challenging.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"stat_ewcdf(   mapping = NULL,   data = NULL,   geom = \"step\",   position = \"identity\",   ...,   n = NULL,   pad = TRUE,   na.rm = FALSE,   show.legend = NA,   inherit.aes = TRUE )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"n NULL, interpolate. NULL, number points interpolate . pad TRUE, pad ecdf additional points (-Inf, 0) (Inf, 1) na.rm FALSE (default), removes missing values warning.  TRUE silently removes missing values.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"computed-variables","dir":"Reference","previous_headings":"","what":"Computed variables","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"x x data y cumulative density corresponding x","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/stat_ewcdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute empirical weighted cumulative distribution — stat_ewcdf","text":"","code":"library(ggplot2)  n <- 100 df <- data.frame(   x = c(rnorm(n, 0, 10), rnorm(n, 0, 10)),   g = gl(2, n),   w = c(rep(1/n, n), sort(runif(n))^sqrt(n)) ) ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\") #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Don't go to positive/negative infinity ggplot(df, aes(x, weights = w)) + stat_ewcdf(geom = \"step\", pad = FALSE) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?   # Multiple ECDFs ggplot(df, aes(x, colour = g, weights = w)) + stat_ewcdf() #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?  ggplot(df, aes(x, colour = g, weights = w)) +   stat_ewcdf() +   facet_wrap(vars(g), ncol = 1) #> Warning: The following aesthetics were dropped during statistical transformation: #> weights #> ℹ This can happen when ggplot fails to infer the correct grouping structure in #>   the data. #> ℹ Did you forget to specify a `group` aesthetic or to convert a numerical #>   variable into a factor?"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted sample quantiles — wquantile","title":"Weighted sample quantiles — wquantile","text":"Calculates empirical sample quantiles optional weights, given probabilities. Like quantile(), smallest observation corresponds probability 0 largest probability 1. Interpolation discrete values done type=7, quantile(). Use type=1 generate quantile values raw input samples.","code":""},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted sample quantiles — wquantile","text":"","code":"wquantile(   x,   probs = seq(0, 1, 0.25),   na.rm = FALSE,   type = 7,   weights = NULL,   ... )"},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted sample quantiles — wquantile","text":"x numeric vector whose sample quantiles wanted. NA NaN values allowed numeric vectors unless na.rm TRUE. probs numeric vector probabilities values \\([0,1]\\). na.rm logical; true, NA NaN's removed x quantiles computed. type numeric, 1 interpolation, 7, interpolated quantiles. Default 7. weights numeric vector non-negative weights, length x, NULL. weights normalised sum 1. NULL, wquantile(x) behaves quantile(x), equal weight sample value. ... Additional arguments, currently ignored","code":""},{"path":[]},{"path":"https://finnlindgren.github.io/StatCompLab/reference/wquantile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted sample quantiles — wquantile","text":"","code":"# Some random numbers x <- rnorm(100)  # Plain quantiles: quantile(x) #>          0%         25%         50%         75%        100%  #> -2.52881995 -0.56453344  0.02975243  0.67716620  1.71539407   # Larger values given larger weight, on average shifting the quantiles upward: wquantile(x, weights = sort(runif(length(x)))) #> [1] -2.528819945 -0.576258004 -0.007017336  0.604435147  1.715394074"}]
