---
title: "Tutorial 03: Monte Carlo integration"
output: learnr::tutorial
runtime: shiny_prerendered
description: "Statistical Computing: Lab tutorial 1"
tutorial:
  id: "shinyapps.finnlindgren.StatCompTutorial03"
  version: 1.0
in-header:
  - \newcommand{\proper}[1]{\mathsf{#1}}
  - \newcommand{\pP}{\proper{P}}
  - \newcommand{\pE}{\proper{E}}
  - \newcommand{\pVar}{\proper{Var}}
  - \newcommand{\pPo}{\proper{Poisson}}
  - \newcommand{\pN}{\proper{Normal}}
  - \newcommand{\mmd}{\mathrm{d}}
  - \newcommand{\md}{\,\mmd}
---

```{r setup, include=FALSE}
solutions <- TRUE
begin_sol <- function() {if (!solutions) {"<!--\n"} else {"\n<hr />**Solution:**\n"}}
end_sol <- function() {if (!solutions) {"-->\n"} else {"\n<hr />\n"}}

library(learnr)
library(ggplot2)
knitr::opts_chunk$set(
  echo = FALSE,
  dev = "png",
  dev.args = list(type = "cairo-png")
)
require(gradethis)
learnr::tutorial_options(
    exercise.timelimit = 120,
    exercise.checker = grade_learnr,
    exercise.startover = TRUE,
    exercise.diagnostics = TRUE,
    exercise.completion = TRUE
#    exercise.error.check.code = exercise.error.check.code.
  )

library(ggplot2)
theme_set(theme_bw())
set.seed(12345L)
```

## Introduction

* In this lab session you will explore Monte Carlo integration and importance sampling.
* Open your github repository clone project from Lab 2 (either on [https://rstudio.cloud](https://rstudio.cloud) or on your own computer
* Save your work for this lab in one or several new files in the project, and commit and push the changes to github (see Lab 2 for more information)

Note: The accompanying `Tutorial03Solutions` tutorial document contains the
solutions explicitly, to make it easier to review the material after the workshops.

## Overdispersed Poisson distribution

We'll use an _overdispersed Poisson distribution_ as the first example. In an ordinary Poisson distribution $Y\sim\pPo(\lambda)$ with expectation $\lambda$, the variance is also $\lambda$. The _coefficient of variation_ is defined as the ratio between the standard deviation and the expectation, which gives $1/\sqrt{\lambda}$. In real applications, one often observes data where the coefficient of variation is larger than the Poisson model would give.  One way of dealing with that is to add a _latent random effect_; a hidden layer of random values that "nudges" the expectation for each observation, which increases the observed coefficient of variation. The model can be written in two steps:
$$
\begin{aligned}
X & \sim \pN(0,\sigma^2), \\
(Y|X=x) & \sim \pPo[\exp(\mu + x)],
\end{aligned}
$$
where the conditional expectation for $Y$ given $X=x$ and $\mu$ is $\lambda(\mu,x)=\exp(\mu+x)$, and $\mu$ is a (fixed) parameter. The
_marginal_ expectation (but till conditionally on $\mu$ and $\sigma$) can be obtained via the tower property (law of total expectation),
$$
\begin{aligned}
\pE(Y|\mu,\sigma) &= \pE[\pE(Y|X)] = \pE[\lambda(\mu,X)] = \pE[\exp(\mu+X)] = \exp(\mu + \sigma^2/2)
\end{aligned}
$$
where the last step comes from the expectation of a log-Normal distribution.
For multiple observations, we can write the model as
$$
\begin{aligned}
x_i & \sim \pN(0,\sigma^2), \text{ independent for each $i=1,\dots,n$,}\\
(y_i|x_i) & \sim \pPo[\exp(\mu + x_i)], \text{ conditionally independent for each $i=1,\dots,n$}.
\end{aligned}
$$

Consider the following simulation code:
```{r echo=TRUE,eval=TRUE}
Y <- rpois(30, lambda = 2 * exp(-1/2 + rnorm(30, sd = 2)))
```
```{r quiz1}
question(
  "What parameter value combination is used in the simulation?",
  answer("$\\mu=-1/2$, $\\sigma=2$"),
  answer("$\\mu=-1/2$, $\\sigma=4$"),
  answer("$\\mu=-1/2+\\log(2)$, $\\sigma=2$", correct = TRUE),
  answer("$\\mu=-1/2+\\log(2)$, $\\sigma=4$"),
  allow_retry = TRUE
)
```

1. **Theory exercise**:
Using the tower property (law of total variance), theoretically derive an expression for $\pVar(Y)$. You can get the needed log-Normal variance expression from [https://en.wikipedia.org/wiki/Log-normal_distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)

`r begin_sol()`
$$
\begin{aligned}
\pVar(Y|\mu,\sigma) &= \pE[\pVar(Y|X)] + \pVar[\pE(Y|X)]
\\&=
\pE[\lambda(\mu,X)] + \pVar[\lambda(\mu,X)]
\\&=
\pE[\exp(\mu+X)] +\pVar[\exp(\mu+X)]
\\&=
\exp(\mu + \sigma^2/2)
+
(\exp(\sigma^2)-1) \exp(2\mu + \sigma^2)
\\&=
\exp(\mu + \sigma^2/2)
\left[
1
+
(\exp(\sigma^2)-1) \exp(\mu + \sigma^2/2)
\right]
\end{aligned}
$$
Since the second factor is always $\geq 1$, this shows that the variance is never smaller than the expectation.
`r end_sol()`


2. **Monte Carlo integration**:
Use Monte Carlo integration to approximate the probability mass function
$$
\begin{aligned}
p_Y(m|\mu,\sigma)=\pP(Y=m|\mu,\sigma) &= \int_{-\infty}^\infty \pP(Y=m|\mu,\sigma,X=x) p_X(x|\sigma) \md x
\end{aligned}
$$
for $m=0,1,2,3,\dots,15$, with $\mu=\log(2)-1/2$ and $\sigma=1$.

For this to be efficient, you should vectorise the calculations by evaluating the conditional
probability function for $Y$ over all the $m$ values at once, for each simulated value
$x^{[k]}\sim\pN(\mu,\sigma^2)$ value, for $k=1,2,\dots,K$. Use $K=10000$ samples.

Plot the resulting probability mass function $p_Y(m|\mu,\sigma)$ together with the
ordinary Poisson probability mass function with the same expectation value, $\lambda = 2$.
(Use the theory above to convince yourself that these two models for $Y$ have the same expectation.)

```{r poisson-mc,exercise=TRUE,exercise.lines=22}

```

```{r poisson-mc-hint-1,eval=FALSE}
# Vectorised over m
mu <- log(2) - 1/2
K <- 10000
m <- 0:15
P_Y <- numeric(length(m))
for (loop in seq_len(K)) {
  ...
}
P_Y <- P_Y / K

# Plot the results
suppressPackageStartupMessages(library(ggplot2))
ggplot(data.frame(m = m,
                  P_Y = P_Y,
                  P_Poisson = dpois(m, lambda = 2))) +
  ...
```
```{r poisson-mc-solution,eval=FALSE}
# Vectorised over m
mu <- log(2) - 1/2
K <- 10000
m <- 0:15
P_Y <- numeric(length(m))
for (loop in seq_len(K)) {
  x <- rnorm(1, sd = 1)
  P_Y <- P_Y + dpois(m, lambda = exp(mu + x))
}
P_Y <- P_Y / K

# Plot the results
suppressPackageStartupMessages(library(ggplot2))
ggplot(data.frame(m = m,
                  P_Y = P_Y,
                  P_Poisson = dpois(m, lambda = 2))) +
  geom_point(aes(m, P_Y, col = "MC")) +
  geom_point(aes(m, P_Poisson, col = "Poisson")) +
  geom_line(aes(m, P_Y, col = "MC")) +
  geom_line(aes(m, P_Poisson, col = "Poisson"))
```

`r begin_sol()`
```{r poisson-mc-show,echo=solutions,eval=solutions,ref.label="poisson-mc-solution"}
```

In the plot, the overdispersed Poisson probability function (that we just computed) is compared with the plain Poisson probability function for the same expectation value. Lines between values are included for clarity.
Note:
In this case, the second approach is faster, but in other situations the first approach is required, since it doesn't require storing all the random numbers at the same time, thus allowing for a much larger $K=n_{\text{mc}}$ value to be used.
`r end_sol()`

3. **Reusable function**:
Generalise the code to a function `doverpois` (for "d"ensity for over-dispersed Poisson)
that takes `m`, `mu`, `sigma`, and `K` as input
and returns a `data.frame` suitable for use with `ggplot`.

```{r poisson-mc2,exercise=TRUE,exercise.lines=12}

```

```{r poisson-mc2-solution,eval=FALSE}
doverpois <- function(m, mu, sigma, K) {
  P_Y <- numeric(length(m))
  for (loop in seq_len(K)) {
    x <- rnorm(1, sd = sigma)
    P_Y <- P_Y + dpois(m, lambda = exp(mu + x))
  }
  P_Y <- P_Y / K

  data.frame(m = m,
             P_Y = P_Y,
             P_Poisson = dpois(m, lambda = exp(mu + sigma^2/2)))
}
doverpois
```

`r begin_sol()`
```{r poisson-mc2-show,echo=solutions,eval=solutions,ref.label="poisson-mc2-solution",results="hide"}
```
`r end_sol()`

Use the function to plot results for $\mu=\log(8)-1/8$, $\sigma = 1/2$, with $m=0,1,\dots,30$ by adding geoms to
```{r eval=FALSE,echo=TRUE}
ggplot(doverpois(m = 0:30, mu = log(8) - 0.125, sigma = 0.5, K = 10000))
```

`r begin_sol()`
```{r poisson-mc2-plot,echo=solutions,eval=solutions}
suppressPackageStartupMessages(library(ggplot2))
ggplot(doverpois(m = 0:30, mu = log(8)-0.125, sigma = 0.5, K = 10000)) +
  geom_point(aes(m, P_Y, col = "MC")) +
  geom_point(aes(m, P_Poisson, col = "Poisson")) +
  geom_line(aes(m, P_Y, col = "MC")) +
  geom_line(aes(m, P_Poisson, col = "Poisson"))
```
`r end_sol()`
