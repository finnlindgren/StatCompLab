<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>StatComp Project 2 (2022/23): Hints ‚Ä¢ StatCompLab</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="StatComp Project 2 (2022/23): Hints">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">StatCompLab</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">25.4.9</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/Project_Example_Solution.html">StatComp Project Example Solution: Numerical Statistics</a></li>
    <li><a class="dropdown-item" href="../articles/Project_Example.html">StatComp Project Example: Numerical statistics</a></li>
    <li><a class="dropdown-item" href="../articles/Project01.html">StatComp Project 1 (2022/23): Simulation and sampling</a></li>
    <li><a class="dropdown-item" href="../articles/Project01Hints.html">StatComp Project 1 (2022/23): Hints</a></li>
    <li><a class="dropdown-item" href="../articles/Project02.html">StatComp Project 2: Scottish weather</a></li>
    <li><a class="dropdown-item" href="../articles/Project02Hints.html">StatComp Project 2 (2022/23): Hints</a></li>
    <li><a class="dropdown-item" href="../articles/Quiz1Solution.html">Quiz 1 Solution (StatComp 2021/22)</a></li>
    <li><a class="dropdown-item" href="../articles/T4sol.html">Tutorial 04: Uncertainty and integration (full solution)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial01.html">Tutorial 01: R and linear models</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial01Solutions.html">Tutorial 01: R and linear models (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial02.html">Tutorial 02: Optimization</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial02Solutions.html">Tutorial 02: Optimization (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial03.html">Tutorial 03: Monte Carlo integration</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial03Solutions.html">Tutorial 03: Monte Carlo integration (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial04.html">Tutorial 04: Uncertainty and integration</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial04Solutions.html">Tutorial 04: Uncertainty and integration (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial06.html">Tutorial 06: Prediction assessment with proper scores</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial06Solutions.html">Tutorial 06: Prediction assessment with proper scores (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial07.html">Tutorial 07: Data wrangling and cross validation</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial07Solutions.html">Tutorial 07: Data wrangling and cross validation (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial08.html">Tutorial 08: Floating point computations and least squares</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial08Solutions.html">Tutorial 08: Floating point computations and least squares (solutions)</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial09.html">Tutorial 09: Bootstrap and Pareto smoothed importance sampling</a></li>
    <li><a class="dropdown-item" href="../articles/Tutorial09Solutions.html">Tutorial 09: Bootstrap and Pareto smoothed importance sampling (solutions)</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/finnlindgren/StatCompLab/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>StatComp Project 2 (2022/23): Hints</h1>
                        <h4 data-toc-skip class="author">Finn
Lindgren</h4>
            
            <h4 data-toc-skip class="date">2022-04-03</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/finnlindgren/StatCompLab/blob/main/vignettes/Project02Hints.Rmd" class="external-link"><code>vignettes/Project02Hints.Rmd</code></a></small>
      <div class="d-none name"><code>Project02Hints.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="monte-carlo-standard-errors-and-confidence-intervals">Monte Carlo standard errors and confidence intervals<a class="anchor" aria-label="anchor" href="#monte-carlo-standard-errors-and-confidence-intervals"></a>
</h2>
<p>In plain Monte Carlo estimation of an expectation value, we can
usually construct approximate confidence intervals by estimating the
standard deviation of the estimator and construct an interval based on a
Normal distribution for the estimator. However, this breaks down in some
cases. For example, when using a randomisation test to estimate a
p-value, the Normal approximation only works if the p-value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
or the number of samples
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
are large enough, so that we actually observe non-zero counts.
Fortunately, if we observe zero counts, we can construct a confidence
interval using an exact method instead of relying on the Normal
approximation.</p>
<div class="section level3">
<h3 id="monte-carlo-randomisation-test-variability">Monte Carlo randomisation test variability<a class="anchor" aria-label="anchor" href="#monte-carlo-randomisation-test-variability"></a>
</h3>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
be the unknown p-value, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚àº</mo><mrow><mi>ùñ°</mi><mi>ùóÇ</mi><mi>ùóá</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X\sim\mathsf{Bin}(N,p)</annotation></semantics></math>
be the random variable for how many times we observe a randomised test
statistic as extreme as or more extreme than the observed test
statistic. We observe
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X=x</annotation></semantics></math>
and estimate the p-value with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>=</mo><mi>x</mi><mi>/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\widehat{p}=x/N</annotation></semantics></math>.
Then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>ùñ§</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>N</mi><mi>p</mi><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mrow><mi>ùñµ</mi><mi>ùñ∫</mi><mi>ùóã</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>N</mi><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>ùñ§</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>p</mi><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mrow><mi>ùñµ</mi><mi>ùñ∫</mi><mi>ùóã</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>N</mi></mfrac><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathsf{E}(X) &amp;= N p, \\
\mathsf{Var}(X) &amp;= N p (1 - p), \\
\mathsf{E}(\widehat{p}) &amp;= p, \\
\mathsf{Var}(\widehat{p}) &amp;= \frac{p(1-p)}{N} .
\end{aligned}
</annotation></semantics></math> We see that the variance decreases
towards
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚Üí</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p\rightarrow 0</annotation></semantics></math>.
We can control the expectation of the absolute error. Due to Jensen‚Äôs
inequality (you may or may not have heard of that!)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùñ§</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚â§</mo><msqrt><mrow><mi>ùñ§</mi><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">|</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">]</mo></mrow></mrow></msqrt><mo>=</mo><msqrt><mrow><mrow><mi>ùñµ</mi><mi>ùñ∫</mi><mi>ùóã</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msqrt><mo>‚â§</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msqrt><mi>N</mi></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathsf{E}(|\widehat{p}-p|)\leq\sqrt{\mathsf{E}[|\widehat{p}-p|^2]}=\sqrt{\mathsf{Var}(\widehat{p})}\leq\frac{1}{2\sqrt{N}}</annotation></semantics></math>,
where the last step uses that the variance is maximised for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">p=1/2</annotation></semantics></math>.
Thus, to guarantee
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùñ§</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚â§</mo><mi>œµ</mi></mrow><annotation encoding="application/x-tex">\mathsf{E}(|\widehat{p}-p|)\leq \epsilon</annotation></semantics></math>
for some
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon&gt;0</annotation></semantics></math>,
we can choose
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â•</mo><mfrac><mn>1</mn><mrow><mn>4</mn><msup><mi>œµ</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">N \geq \frac{1}{4 \epsilon^2}</annotation></semantics></math>.</p>
<p>The relative error has variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ùñµ</mi><mi>ùñ∫</mi><mi>ùóã</mi></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>‚àí</mo><mi>p</mi></mrow><mi>p</mi></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mfrac><mrow><mn>1</mn><mo>‚àí</mo><mi>p</mi></mrow><mrow><mi>N</mi><mi>p</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathsf{Var}\left[\frac{\widehat{p}-p}{p}\right]=\frac{1-p}{Np}</annotation></semantics></math>
which goes to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>‚àû</mi><annotation encoding="application/x-tex">\infty</annotation></semantics></math>
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚Üí</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p\rightarrow 0</annotation></semantics></math>,
so we cannot control the relative error uniformly for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
by increasing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.
We therefore need to be careful when assessing results for small values
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="normal-approximation">Normal approximation<a class="anchor" aria-label="anchor" href="#normal-approximation"></a>
</h3>
<p>When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x&gt;0</annotation></semantics></math>,
a basic approximate 95% confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msub><mi>I</mi><mi>p</mi></msub><mo>=</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo>¬±</mo><msub><mi>z</mi><mn>0.975</mn></msub><msqrt><mfrac><mrow><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mover><mi>p</mi><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mi>N</mi></mfrac></msqrt><mo>=</mo><mfrac><mi>x</mi><mi>N</mi></mfrac><mo>¬±</mo><msub><mi>z</mi><mn>0.975</mn></msub><msqrt><mfrac><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>‚àí</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mi>N</mi><mn>3</mn></msup></mfrac></msqrt><mi>.</mi></mrow><annotation encoding="application/x-tex">
CI_p = \widehat{p} \pm z_{0.975} \sqrt{\frac{\widehat{p} (1-\widehat{p})}{N}}
= \frac{x}{N} \pm z_{0.975} \sqrt{\frac{x (N-x)}{N^3}} .
</annotation></semantics></math> With the approximation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>0.975</mn></msub><mo>‚âà</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">z_{0.975}\approx 2</annotation></semantics></math>,
we can limit the interval width with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><msqrt><mfrac><mn>1</mn><mrow><mn>4</mn><mi>N</mi></mrow></mfrac></msqrt><mo>‚â§</mo><mi>œµ</mi></mrow><annotation encoding="application/x-tex">4\sqrt{\frac{1}{4N}}\leq \epsilon</annotation></semantics></math>
by taking
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â•</mo><mfrac><mn>4</mn><msup><mi>œµ</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">N\geq \frac{4}{\epsilon^2}</annotation></semantics></math>.
For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>=</mo><mn>0.02</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.02</annotation></semantics></math>,
we get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â•</mo><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">N\geq 10,000</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="exact-interval-for-x0">Exact interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#exact-interval-for-x0"></a>
</h3>
<p>When the observed count is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math>,
we can go back to the definition of a confidence interval and how we can
construct confidence intervals by ‚Äúinverting a test‚Äù; the interval is
taken to be the set for which the corresponding null hypothesis is not
rejected.</p>
<p>Imagine that we reject the hypothesis
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mi>p</mi><mo>=</mo><msub><mi>p</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">H_0:p=p_0</annotation></semantics></math>
for some
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mn>0</mn></msub><annotation encoding="application/x-tex">p_0</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùñØ</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>‚â§</mo><mi>x</mi><mo>‚à£</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>&lt;</mo><mn>0.025</mn></mrow><annotation encoding="application/x-tex">\mathsf{P}_X(X \leq x \mid p_0) &lt; 0.025</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùñØ</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>‚â•</mo><mi>x</mi><mo>‚à£</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>&lt;</mo><mn>0.025</mn></mrow><annotation encoding="application/x-tex">\mathsf{P}_X(X \geq x \mid p_0) &lt; 0.025</annotation></semantics></math>
(to nominally give equal tail error probability). When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math>,
the second probability is equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>,
so that condition is never used. The test is therefore only rejected
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùñØ</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>‚à£</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>&lt;</mo><mn>0.025</mn></mrow><annotation encoding="application/x-tex">\mathsf{P}_X(X = 0 \mid p_0) &lt; 0.025</annotation></semantics></math>.
We solve for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mn>0</mn></msub><annotation encoding="application/x-tex">p_0</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>ùñØ</mi><mi>X</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>‚à£</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>N</mi></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>&lt;</mo><mn>0.025</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mn>1</mn><mo>‚àí</mo><msub><mi>p</mi><mn>0</mn></msub><mo>&lt;</mo><msup><mn>0.025</mn><mrow><mn>1</mn><mi>/</mi><mi>N</mi></mrow></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>p</mi><mn>0</mn></msub><mo>&gt;</mo><mn>1</mn><mo>‚àí</mo><msup><mn>0.025</mn><mrow><mn>1</mn><mi>/</mi><mi>N</mi></mrow></msup><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathsf{P}_X(X = 0 \mid p_0) = (1-p_0)^N &amp;&lt; 0.025 \\
1-p_0 &lt; 0.025^{1/N} \\
p_0 &gt; 1 - 0.025^{1/N} .
\end{aligned}
</annotation></semantics></math> The set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mn>0</mn></msub><annotation encoding="application/x-tex">p_0</annotation></semantics></math>
values for which the test is <em>not</em> rejected is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>0</mn></msub><mo>‚â§</mo><mn>1</mn><mo>‚àí</mo><msup><mn>0.025</mn><mrow><mn>1</mn><mi>/</mi><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">p_0 \leq 1-0.025^{1/N}</annotation></semantics></math>,
so when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math>
we can define the confidence interval for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msub><mi>I</mi><mi>p</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>‚àí</mo><msup><mn>0.025</mn><mrow><mn>1</mn><mi>/</mi><mi>N</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
CI_p = (0, 1-0.025^{1/N}) .
</annotation></semantics></math> To limit the width of such confidence
intervals to at most some
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>,
we need
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚àí</mo><msup><mn>0.025</mn><mrow><mn>1</mn><mi>/</mi><mi>N</mi></mrow></msup><mo>‚â§</mo><mi>œµ</mi></mrow><annotation encoding="application/x-tex">1-0.025^{1/N}\leq \epsilon</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â•</mo><mfrac><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.025</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>œµ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">N\geq \frac{\log(0.025)}{\log(1-\epsilon)}</annotation></semantics></math>.
This grows much more slowly than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>4</mn><msup><mi>œµ</mi><mn>2</mn></msup></mfrac><annotation encoding="application/x-tex">\frac{4}{\epsilon^2}</annotation></semantics></math>
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œµ</mi><mo>‚Üí</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon\rightarrow 0</annotation></semantics></math>,
so we can safely use the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
that‚Äôs required to bound the Normal approximation interval width and
still fulfill the interval width criterion for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math>
interval construction.</p>
</div>
<div class="section level3">
<h3 id="bayesian-credible-interval-construction">Bayesian credible interval construction<a class="anchor" aria-label="anchor" href="#bayesian-credible-interval-construction"></a>
</h3>
<p>An alternative approach is to construct a Bayesian credible interval
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚àº</mo><mrow><mi>ùñ¥</mi><mi>ùóá</mi><mi>ùóÇ</mi><mi>ùñø</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p\sim\mathsf{Unif}(0,1)</annotation></semantics></math>
a priori. The posterior density for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is then proportional to
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùñØ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false" form="prefix">|</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>N</mi><mi>x</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>p</mi><mi>x</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>N</mi><mo>‚àí</mo><mi>x</mi></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathsf{P}(X=x|p) = {N \choose x} p^x(1-p)^{N-x},
</annotation></semantics></math> which shows that the posterior
distribution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ùñ°</mi><mi>ùñæ</mi><mi>ùóç</mi><mi>ùñ∫</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>x</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>N</mi><mo>‚àí</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{Beta}(1+x, 1+N-x)</annotation></semantics></math>,
and a credible interval is provided by the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.025</mn><annotation encoding="application/x-tex">0.025</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.975</mn><annotation encoding="application/x-tex">0.975</annotation></semantics></math>
quantiles. In R,
<code>qbeta(c(0.025, 0.975), shape1 = 1 + x, shape2 = 1 + N - x)</code>.
This construction works for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚â•</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N\geq 1</annotation></semantics></math>
and all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>‚â§</mo><mi>x</mi><mo>‚â§</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">0\leq x \leq N</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="prediction-standard-deviations">Prediction standard deviations<a class="anchor" aria-label="anchor" href="#prediction-standard-deviations"></a>
</h2>
<p>The <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> function can provide prediction standard
errors for the linear predictor, with <code>se.fit</code>, but those are
only half the story when predicting new data. The standard errors only
include the uncertainty information about the linear predictor curve.
For full prediction uncertainty, we need to take the observation
variation into account, which <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> estimated via the
variance of the residuals. Since the residuals for new observations is
assumed to be conditionally independent of the predictor curve, the
prediction variance can be estimated as the sum of the square of the
prediction standard error and the residual variance, if the degrees of
freedom is large. For the help text for <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> we see that
when <code>se.fit=TRUE</code>, the output list contains the elements</p>
<ul>
<li>
<code>fit</code>: vector or matrix (depending on the
<code>interval</code> argument)</li>
<li>
<code>se.fit</code>: standard error of predicted means</li>
<li>
<code>residual.scale</code>: residual standard deviations</li>
<li>
<code>df</code>: degrees of freedom for residual</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># When creating a tibble, the construct can use variables defined</span></span>
<span><span class="co"># first in the later variables; here we use x when constructing y:</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>             y <span class="op">=</span> <span class="fl">2</span> <span class="op">+</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">10</span>, sd <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Estimate a model:</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span></span>
<span><span class="co"># Compute prediction mean and standard deviations and add to df_pred:</span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">fit</span>, newdata <span class="op">=</span> <span class="va">df_pred</span>, se.fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="va">df_pred</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">fit</span>,</span>
<span>         se.fit <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">se.fit</span>,</span>
<span>         sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">pred</span><span class="op">$</span><span class="va">se.fit</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">pred</span><span class="op">$</span><span class="va">residual.scale</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_pred</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">se.fit</span>, colour <span class="op">=</span> <span class="st">"se.fit"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">sd</span>, colour <span class="op">=</span> <span class="st">"sd"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">"Std. deviations"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Project02Hints_files/figure-html/unnamed-chunk-1-1.png" width="700"></p>
<p>If we also ask for prediction intervals, we need to modify the code a
bit. From comparing the interval width results from
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> with those from an interval assuming
t-distributions, we see that they are identical up to floating point
accuracy.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute prediction mean and standard deviations and add to df_pred:</span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">fit</span>, newdata <span class="op">=</span> <span class="va">df_pred</span>, se.fit <span class="op">=</span> <span class="cn">TRUE</span>, interval <span class="op">=</span> <span class="st">"prediction"</span><span class="op">)</span></span>
<span><span class="va">df_pred</span> <span class="op">&lt;-</span> <span class="va">df_pred</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">fit</span><span class="op">[</span>, <span class="st">"fit"</span><span class="op">]</span>,</span>
<span>         lwr <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">fit</span><span class="op">[</span>, <span class="st">"lwr"</span><span class="op">]</span>,</span>
<span>         upr <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">fit</span><span class="op">[</span>, <span class="st">"upr"</span><span class="op">]</span>,</span>
<span>         se.fit <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">se.fit</span>,</span>
<span>         sd <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">pred</span><span class="op">$</span><span class="va">se.fit</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">pred</span><span class="op">$</span><span class="va">residual.scale</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df_pred</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html" class="external-link">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">upr</span> <span class="op">-</span> <span class="va">lwr</span> <span class="op">-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html" class="external-link">qt</a></span><span class="op">(</span><span class="fl">0.975</span>, <span class="va">pred</span><span class="op">$</span><span class="va">df</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html" class="external-link">qt</a></span><span class="op">(</span><span class="fl">0.025</span>, <span class="va">pred</span><span class="op">$</span><span class="va">df</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">sd</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">"Interval width difference"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">"x"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Project02Hints_files/figure-html/unnamed-chunk-2-1.png" width="700"></p>
</div>
<div class="section level2">
<h2 id="handling-non-gaussian-precipitation">Handling non-Gaussian precipitation<a class="anchor" aria-label="anchor" href="#handling-non-gaussian-precipitation"></a>
</h2>
<p>While the assessment methods requested in the project description are
valid for non-Gaussian predictions, the non-Gaussianity of the
precipitation data is still very noticeable on the monthly average
scale. An effect of this is that the constant variance assumption of a
basic <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> model isn‚Äôt a good fit to the data. To improve
this, you can take the square root of the monthly averages before
applying the modelling in part 2. You may also do all the model and
prediction assessment on this square-root version of the data.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Finn Lindgren.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
