---
title: 'StatComp Project 1 (2021/22): Simulation and sampling'
author: "Finn Lindgren"
output:
  rmarkdown::html_vignette:
    number_sections: yes
  pdf_document:
    number_sections: yes
pkgdown:
  number_sections: yes
description: 'Statistical Computing (2021/22): Coursework project 1'
vignette: |
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{StatComp Project 1 (2021/22): Simulation and sampling}
  %\VignetteEngine{knitr::rmarkdown}
in-header:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\wh}[1]{\widehat{#1}}
  - \newcommand{\ol}[1]{\overline{#1}}
  - \newcommand{\wt}[1]{\widetilde{#1}}
  - \newcommand{\mv}[1]{\boldsymbol{#1}}
  - \newcommand{\proper}[1]{\mathsf{#1}}
  - \newcommand{\pP}{\proper{P}}
  - \newcommand{\pE}{\proper{E}}
  - \newcommand{\pVar}{\proper{Var}}
  - \newcommand{\pPo}{\proper{Poisson}}
  - \newcommand{\pN}{\proper{Normal}}
  - \newcommand{\pBin}{\proper{Bin}}
  - \newcommand{\pBeta}{\proper{Beta}}
  - \newcommand{\pGeom}{\proper{Geom}}
  - \newcommand{\pExp}{\proper{Exp}}
  - \newcommand{\pLogExp}{\proper{LogExp}}
  - \newcommand{\pGamma}{\proper{Gamma}}
  - \newcommand{\mmd}{\mathrm{d}}
  - \newcommand{\md}{\,\mmd}
---

```{r setup, include=FALSE}
.tutorial <- FALSE
.solutions <- FALSE
begin_sol <- function(sol = .solutions, quiet = FALSE) {
  if (!sol) {
    "<!--\n"
  } else if (quiet) {
    "\n<hr />\n"
  } else {
    "\n<hr />**Solution:**\n"
  }
}
end_sol <- function(sol = .solutions) {
  if (!sol) {
    "-->\n"
  } else {
    "\n<hr />\n"
  }
}

library(StatCompLab)
if (.tutorial) {
  library(learnr)
  require(gradethis)
  learnr::tutorial_options(
    exercise.timelimit = 120,
    exercise.checker = grade_learnr,
    exercise.startover = TRUE,
    exercise.diagnostics = TRUE,
    exercise.completion = TRUE
    #    exercise.error.check.code = exercise.error.check.code.
  )
}
library(ggplot2)
knitr::opts_chunk$set(
  echo = FALSE,
  fig.width = 6
)

library(ggplot2)
theme_set(theme_bw())
set.seed(1234L)
suppressPackageStartupMessages(library(tidyverse))
```

This document can either be accessed in html form on [https://finnlindgren.github.io/StatCompLab/](https://finnlindgren.github.io/StatCompLab/), as
a vignette in `StatCompLab` version 21.5.0 or later, with `vignette("Project01", "StatCompLab")`, or in pdf format on Learn.

1. You have been assigned a `project01-username` repository on 
[https://github.com/StatComp21/](https://github.com/StatComp21/). Use `git` to
_clone_ the repository (either to [https://rstudio.cloud](https://rstudio.cloud)
or your own computer) in the same way as the `lab*-username` repositories for tutorials 2 and 4.
2. The repository contains a template RMarkdown document, `report.Rmd`,
that you should expand with your answers and solutions for this project assignment,
and a code files for function definitions, `code.R`, that contains some predefined functions, and you should add your own function definitions and function documentation
to that file. Do not change these filenames.
3. The function definitions should be included in the `report.Rmd` file in
exactly the way already shown in the document. This will make the function
defintions available throughout the document, but only shown in the code appendix.
4. The main text should be a coherent presentation of theory and discussion of methods and results, showing code for code chunks that perform computations and analysis,
but not code for code chunks that generate figures or tables.
Use the `echo=TRUE` and `echo=FALSE` to control what code is visible.
5. The `styler` package Addin for restyling code for better and consistent readability
works for both `.R` and `.Rmd` files.
6. The `Project01Hints` vignette contains some useful tips.

The assignment is submitted by pushing your work to the github repository
before your submission deadline.
Make sure you include your name, student number, and github username in both `report.Rmd`
and `code.R`. When submitting, also include a generated html version of the report,
with filename `report.html`. Normally, generated images are automatically
included in the html file itself; otherwise they need to be added to the repository as well.

It is strongly recommended to commit and push to github frequently.
If you're uncertain about whether you're pushing all the needed submission files
to github, ask Finn to check the repository contents _at least_ a week before the
submission deadline.

The submission deadline is Monday 7 March 2022, at 16:00 (Edinburgh time).
See the Assessment page on Learn for extensions and late submission penalty details.

The project will be marked as a whole, with marks between $0$ and $40$.
A marking guide will be posted on Learn.

\newpage







# Confidence interval approximation assessment

As in Lab 4, consider the Poisson model for observations $\bm{y}=\{y_1,\dots,y_n\}$:
$$
\begin{aligned}
y_i & \sim \pPo(\lambda), \quad\text{independent for $i=1,\dots,n$.}
\end{aligned}
$$
that has joint probability mass function
$$
p(\bm{y}|\lambda) = \exp(-n\lambda) \prod_{i=1}^n \frac{\lambda^{y_i}}{y_i!}
$$
In the lab, one of the considered parameterisation alternatives was

1. $\theta = \lambda$, and $\wh{\theta}_\text{ML}=\frac{1}{n}\sum_{i=1}^n y_i = \ol{y}$

The confidence interval construction was motivated by the ratio $\frac{\wh{\lambda}-\lambda}{\sqrt{\lambda/n}}$, which was approximated by
$\frac{\wh{\lambda}-\lambda}{\sqrt{\wh{\lambda}/n}}$. We now wish to see if we
would achieve a better construction by using the non-approximated version.

Solve the inequalities in
$$
a = z_{\alpha/2} < \frac{\wh{\lambda}-\lambda}{\sqrt{\lambda/n}} < z_{1-\alpha/2} = b
$$
with respect to $\lambda$, and implement the resulting confidence interval construction
in a function (in `code.R`; also document the function) taking arguments `y` and `alpha`.
Also include another function for the method used in the lab, with the same call syntax.

Hint: It may help to do the mathematical derivation in several steps; e.g.
by introducing $x=\sqrt{\lambda}$, solve two separate quadratic equations (on with $a$ and one with $b$) and then work out for what intervals the two inequalities are fulfilled. For that, you likely will need the fact that for $\alpha<0.5$, we have $z_{\alpha/2}<0$ and $\z_{1-\alpha/2}>0$.

The function `estimate_coverage`, defined in `code.R`, can be used to estimate
the coverage of the construction of confidence intervals from samples $\pPo(\lambda)$,
given an interval construction R function.
Use this to discuss the following questions:

Which of the two alternatives has coverage closest to the nominal coverage of 90%
for the case $n=2$, $\lambda=3$? What happens for larger values of $n$?



\newpage

# 3D printer materials prediction

The aim is estimate the parameters of a Bayesian statistical model of material use in a 3D printer.^[Special thanks to Sten Lindgren for providing the printer data.] The printer uses rolls of _filament_ that gets heated and squeezed through a moving nozzle, gradually building objects. The objects are first designed in a CAD program (Computer Aided Design), that also estimates how much material will be required to print the object.

The data can be loaded with `data("filament1", package = "StatCompLab")`, and contains information about one
3D-printed object per row. The columns are

* `Index`: an observation index
* `Date`: printing dates
* `Material`: the printing material, identified by its colour
* `CAD_Weight`: the object weight (in gram) that the CAD software calculated
* `Actual_Weight`: the actual weight of the object (in gram) after printing

If the CAD system and printer were both perfect, the `CAD_Weight` and `Actual_Weight` values would be equal for each object.
 In reality, there is both random variation, for example due to varying humidity and temperature, and systematic deviations due to the CAD system not having perfect information about the properties of the printing materials.
 
When looking at the data (see below) it's clear that the variability of the data
is larger for larger values of `CAD_Weight`.  The printer operator has made a simple
physics analysis, and settled on a model where the connection between `CAD_Weight` and `Actual_Weight` follows a linear model, and the variance increases with square of `CAD_Weight`.
If we denote the CAD weight for observations $i$ by `x_i`, and the corresponding actual
weight by $y_i$, the model can be defined by

$$
y_i \sim \pN[\beta_1 + \beta_2 x_i, \beta_3 + \beta_4 x_i^2)] .
$$
To ensure positivity of the variance, the parameterisation $\mv{\theta}=[\theta_1,\theta_2,\theta_3,\theta_4]=[\beta_1,\beta_2,\log(\beta_3),\log(\beta_4)]$ is introduced, and
the printer operator assigns independent prior distributions as follows:
$$
\begin{aligned}
\theta_1 &\sim \pN(0,\gamma_1), \\
\theta_2 &\sim \pN(1,\gamma_2), \\
\theta_3 &\sim \pLogExp(\gamma_3), \\
\theta_4 &\sim \pLogExp(\gamma_4),
\end{aligned}
$$
where $\pLogExp(a)$ denotes the logarithm of an exponentially distributed random variable
with rate parameter $a$, as seen in Tutorial 4.
The $\mv{\gamma}=(\gamma_1,\gamma_2,\gamma_3,\gamma_4)$ values are positive parameters.

The printer operator reasons that due to random fluctuations in the material properties (such as the density) and room temperature should lead to a _relative_ error
instead of an additive error, and this is what lead them to the model, as an approximation of that.
The basic physics assumption is that the error in the CAD software calculation of the weight is proportional to the weight itself.

Start by loading the data and plot it.
```{r load-data-solution,eval=FALSE}
data("filament1", package = "StatCompLab")
ggplot(filament1,
       aes(CAD_Weight, Actual_Weight, colour = Material)) +
  geom_point()
```

## Prior density

With the help of `dnorm` and the `dlogexp` function (see the `code.R` file for documentation),
define and document (in `code.R`) a function `log_prior_density` with arguments `theta` and `params`,
where theta is the $\mv{\theta}$ parameter vector, and `params` is the vector of $\mv{\gamma}$ parameters. Your function should evaluate the logarithm of the joint
prior density $p(\mv{\theta})$ for the four $\theta_i$ parameters.

## Observation likelihood

With the help of `dnorm`, define and document a function `log_like`, taking arguments `theta`, `x`, and `y`, that evaluates the observation log-likelihood
$p(\mv{y}|\mv{\theta})$ for the model defined above.

## Posterior density

Define and document a function `log_posterior_density` with arguments
`theta`, `x`, `y`, and `params`, that evaluates the logarithm of the
posterior density $p(\mv{\theta}|\mv{y})$,
apart from some unevaluated normalisation constant.

## Posterior mode and Gaussian approximation

Let all $\gamma_i=1$, $i=1,2,3,4$, and use `optim()` together with the `log_posterior_density` and filament data to find the posterior mode for $\mv{\theta}$.
Also evaluate the inverse of the negated Hessian at the mode, in order to obtain a multivariate Normal approximation $\pN(\mv{\mu},\mv{S})$ to the posterior distribution for $\mv{\theta}$.
See the documentation for `optim` for how to do maximisation instead of minimisation.

## Importance sampling function

The aim is to construct a 90% Bayesian credible interval for each $\beta_j$ using
importance sampling, similarly to the method used in lab 4. There, a one dimensional
Gaussian approximation of the posterior of a parameter was used.
Here, we will instead use a multivariate Normal approximation as the importance sampling distribution.  The functions
`rmvnorm` and `dmvnorm` in the `mvtnorm` package can be used to sample and
evaluate densities.

Define and document a function `do_importance` taking arguments `N` (the number of samples
to generate), `mu` (the mean vector for the importance distribution), and `S` (the covariance matrix),
and other additional parameters that are needed by the function code.

The function should output a `data.frame` with five columns, `beta1`, `beta2`, `beta3`, `beta4`, `log_weights`, containing the $\beta_i$ samples and normalised $\log$-importance-weights,
so that `sum(exp(log_weights))` is $1$. Use the `log_sum_exp` function to compute
the needed normalisation information.

## Importance sampling

Use your defined functions to compute an importance sample of size $N=10000$.
Plot the empirical weighted CDFs together with the un-weighted CDFs for each parameter,
with the help of `stat_ewcdf`, and discuss the results.  To achieve simpler `ggplot`
code, you may find `pivot_longer(???, starts_with("beta"))` and `facet_wrap(vars(name))` useful.

Construct 90% credible intervals for each of the four model parameters, based on the importance sample. In addition to `wquantile` and `pivot_longer`,
the methods `group_by` and `summarise`
are helpful. You may wish to define a function `make_CI` taking arguments `x`, `weights`, and `prob` (to control the intended coverage probability), generating a 1-row, 2-column `data.frame` to help structure the code.

Discuss the results both from the sampling method point of view and the 3D printer application point of view
(this may also involve e.g. plotting prediction intervals based on
point estimates of the parameters, and plotting the importance log-weights
to see how they depend on the sampled $\beta$-values, like in the `T4sol` document).



