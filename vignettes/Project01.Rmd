---
title: 'StatComp Project 1 (2020/21): Numerical statistics'
author: "Finn Lindgren"
output:
  html_document:
    number_sections: yes
  pdf_document:
    number_sections: yes
description: 'Statistical Computing (2020/21): Coursework project 1'
in-header:
- \newcommand{\bm}[1]{\boldsymbol{#1}}
- \newcommand{\wh}[1]{\widehat{#1}}
- \newcommand{\ol}[1]{\overline{#1}}
- \newcommand{\wt}[1]{\widetilde{#1}}
- \newcommand{\mv}[1]{\boldsymbol{#1}}
- \newcommand{\proper}[1]{\mathsf{#1}}
- \newcommand{\pP}{\proper{P}}
- \newcommand{\pE}{\proper{E}}
- \newcommand{\pVar}{\proper{Var}}
- \newcommand{\pPo}{\proper{Poisson}}
- \newcommand{\pN}{\proper{Normal}}
- \newcommand{\pBin}{\proper{Bin}}
- \newcommand{\pBeta}{\proper{Beta}}
- \newcommand{\pGeom}{\proper{Geom}}
- \newcommand{\pExp}{\proper{Exp}}
- \newcommand{\pGamma}{\proper{Gamma}}
- \newcommand{\mmd}{\mathrm{d}}
- \newcommand{\md}{\,\mmd}
---

```{r setup, include=FALSE}
.tutorial <- FALSE
.solutions <- FALSE
begin_sol <- function(sol = .solutions, quiet = FALSE) {
  if (!sol) {
    "<!--\n"
  } else if (quiet) {
    "\n<hr />\n"
  } else {
    "\n<hr />**Solution:**\n"
  }
}
end_sol <- function(sol = .solutions) {
  if (!sol) {
    "-->\n"
  } else {
    "\n<hr />\n"
  }
}

library(StatCompLab)
if (.tutorial) {
  library(learnr)
  require(gradethis)
  learnr::tutorial_options(
    exercise.timelimit = 120,
    exercise.checker = grade_learnr,
    exercise.startover = TRUE,
    exercise.diagnostics = TRUE,
    exercise.completion = TRUE
    #    exercise.error.check.code = exercise.error.check.code.
  )
}
library(ggplot2)
knitr::opts_chunk$set(
  echo = FALSE,
  dev = "png",
  dev.args = list(type = "cairo-png"),
  fig.width = 6
)

library(ggplot2)
theme_set(theme_bw())
set.seed(1234L)
```

This document can either be accessed in html form on (https://finnlindgren.github.io/StatCompLab/), as
a vignette in `StatCompLab` version 0.5.0 or later, with `vignette("Project01", "StatCompLab")`, or in pdf format on Learn.

1. You have been assigned a `project01-username` repository on 
(https://github.com/StatComp20/(https://github.com/StatComp20/). Use `git` to
_clone_ the repository (either to [https://rstudio.cloud](https://rstudio.cloud)
or your own computer) in the same way as the `lab02-username` repository in Lab 2.
2. The repository contains a template RMarkdown document, `report.Rmd`,
that you should expand with your answers and solutions for this project assignment,
and two code files, `code.R` containing some predefined functions, and `my_code.R`
that should be used for some of your own code. Do not change these filenames.
3. Some of the code solutions (as indicated for those items) should be placed
in the `my_code.R` file, and included in the report as indicated in
the template document (also see the `RMdemo.Rmd`, `my_code.R`, and `T4sol.Rmd`
document examples from the Week 4 lecture and example materials on Learn).
4. Code that is placed directly into code chunks in the `report.Rmd` file should be
shown in a separate section at the end of the report, so that the main text is
a coherent presentation of theory and discussion of methods and results.
In the code presentation section, include a brief code comment
for each chunk. Also include brief documentation as code comments before each function
defined in `my_code.R`.
5. The `styler` package Addin for restyling code for better and consistent readability
works for both `.R` and `.Rmd` files.

The assignment is submitted by pushing your work to the github repository
before your submission deadline.
Make sure you include your name, student number, and github username in both `report.Rmd`
and `my_code.R`. When submitting, also include a generated html version of the report,
with filename `project.html`. Normally, generated images are automatically
included in the html file itself; otherwise they need to be added to the repository as well.

It is strongly recommended to commit and push to github frequently.
If you're uncertain about whether you're pushing all the needed submission files
to github, ask Finn to check the repository contents _at least_ a week before the
submisison deadline.

The submission deadline is Monday 1 March 2021, at 16:00 (Edinburgh time).
See the Assessment page on Learn for extensions and late submission penalty details.

The 10 sub-tasks contribute equally to the total mark, but
the project will be marked as a whole, between $0$ and $40$.
A marking guide will be posted on Learn.

\newpage







# Confidence interval approximation assessment

As in Lab 4, consider the Poisson model for observations $\bm{y}=\{y_1,\dots,y_n\}$:
$$
\begin{aligned}
y_i & \sim \pPo(\lambda), \quad\text{independent for $i=1,\dots,n$.}
\end{aligned}
$$
that has joint probability mass function
$$
p(\bm{y}|\lambda) = \exp(-n\lambda) \prod_{i=1}^n \frac{\lambda^{y_i}}{y_i!}
$$
In the lab, we considered three parameterisation alternatives:

1. $\theta = \lambda$, and $\wh{\theta}_\text{ML}=\frac{1}{n}\sum_{i=1}^n y_i = \ol{y}$
2. $\theta = \sqrt{\lambda}$, and $\wh{\theta}_\text{ML}=\sqrt{\ol{y}}$
3. $\theta = \log(\lambda)$, and $\wh{\theta}_\text{ML}=\log\left(\ol{y}\right)$

We know that the inverse expected Fisher information
is $\lambda/n$ for case 1, $1/(4n)$ for case 2, and $1/(n\lambda)$ for case 3.

In the lab, three functions, CI1, CI2, and CI3, were defined for constructing
confidence intervals based on asymptotic Normal approximation for the Maximum Likelihood estimators in each of the parameterisations.  For this project, you should use the function
`pois_CI` that is defined in the `code.R` file in your project repository.
The code file should be imported with the method shown in the `RMdemo.Rmd` file,
but not displayed in the report. Look in the file for documentation of the functions.

The task is to do a simulation study to assess the accuracy of the three interval
construction methods.

1. In your `my_code.R` file, define a function
`multi_pois_CI(m, n, lambda, alpha,type)` that takes inputs
`m`, indicating the number of simulations to perform, `n`, the number of
observations used for each sample, and `lambda`, the true parameter value, as
well as `alpha` and `type` with the same interpretation as for `pois_CI`.
The function must return a data frame with `m` rows and two columns, named
`Lower` and `Upper` containing the confidence intervals, one for each row.
2. Show mathematically that the widths of the $\lambda$-intervals for
parameterisation type 1 and 2 are always identical to each other, for any given
observation vector $\bm{y}$ and confidence level $1-\alpha$.
3. In your `my_code.R` file, define a function
`tidy_multi_pois_CI(m, n, lambda, alpha)` that takes the same inputs as `multi_pois_CI` except for the `type` argument. The function must return a data frame with `3*m` rows and three
columns, named
`Lower`, `Upper`, and `Type` containing the confidence intervals from three
calls to `multi_pois_CI`, with the `Type` column containing values 1, 2, or 3 to indicate
which method calculated each interval.
4. With the help of `tidy_multi_pois_CI`, estimate the coverage probability for
nominal confidence level $1-\alpha$=90%,
for each of the three interval types, for $n=2$, $\lambda=3$, using $m$ at least $100000$.
How well do the coverage probabilities match the nominal confidence level for each method?
With the `tidyverse` package loaded, the `group_by` and `summarise` functions
can be used to avoid direct indexing or loops.
Summarise and discuss the results.
5. Plot the empirical CDFs for the interval widths for the three methods, in a common figure. Use `factor(Type)` to group the information by colour. Also compute the median interval width for each method. Summarise and discuss the results.


\newpage

# Archaeology in the Baltic Sea

In Lab 3, you investigated data from an archaeological excavation on Gotland in
the Baltic sea. In the 1920s, a battle gravesite from 1361 was subject to
several archaeological excavations.  A
total of $493$ femurs (thigh bones)
($256$ left, and $237$ right) were found.  We wanted to figure out how many
persons were likely buried at the gravesite.  It must reasonably have
been at least $256$, but how many more? In this assignment, you will use
importance sampling to estimate credible intervals for this problem.

In the lab, it seemed difficult to extract much information from the sample, since there
were only two observations and two parameters.  You will here investigate if
including data from other excavations of the same type might decrease the uncertainty about
the number of persons buried, through improved information about the average detection
probability, $\phi$. The function `arch_data` in `code.R` returns a data frame
with data for up to four different excavations (note: only one of them is real; the other
three are synthetic data generated for this assignment).

We assume the same model as in the lab for each excavation, with an unknown $N_j$, $j=1,\dots,J$, for each of the $J$ excavations, and a common detection probability $\phi$. We assume them model that, conditionally on $\{N_j\}$ and $\phi$,
$(Y_{j,i}|N_j,\phi)\sim\pBin(N_j,\phi)$, $i=1,2$, all independent, and prior distributions
$N_j\sim\pGeom(\xi)$, $0<\xi<1$, and $\phi\sim\pBeta(a, b)$, $a,b>0$,
$$
\begin{aligned}
p_{Y_{j,i}|N_j=n,\phi}(y) = \pP(Y_{j,i}=y|N_j=n,\phi) &= {n \choose y} \phi^y(1-\phi)^{n-y},
\quad y=0,\dots,n, \\
p_{N_j}(n) = \pP(N_j=n) &= \xi\,(1-\xi)^n,\quad n=0,1,2,3,\dots, \\
p_\phi(\phi) &= \frac{\phi^{a-1}(1-\phi)^{b-1}}{B(a,b)}, \quad \phi\in(0,1) ,
\end{aligned}
$$
where $B(a,b)$ is the _Beta function_, see `?beta` in R.


## Joint probability function for $(\bm{N},\bm{Y})$

Conditionally on $\bm{N}=\{N_1,\dots,N_J\}$, and the observations $\bm{Y}$,
the conditional posterior distribution for $(\phi|\bm{N},\bm{Y})$ is $\pBeta(\wt{a},\wt{b})$ where $\wt{a}=a+\sum_{j,i} Y_{j,i}$ and $\wt{b}=b+2\sum_j N_j -\sum_{j,i} Y_{j,i}$, and the sums go over $j=1,\dots,J$ and $i=1,2$.

Show that the joint probability function for $(\bm{N},\bm{Y})$, with
$N_j\geq\max(Y_{j,1},Y_{j,2})$ and $Y_{j,i}=0,1,2,\dots$,
is given by^[Binomial coefficients can be notated with `{N \choose k}` in LaTeX formulas.]
$$
p(\bm{N},\bm{Y}) = \frac{B(\wt{a},\wt{b})}{B(a,b)} \prod_{j=1}^J \left[
p(N_j)
\prod_{i=1}^2
{N_j \choose Y_{j,i}}
{N_j \choose Y_{j,i}}
\right],
$$
where $p(N_j)$ is the probability mass function for the Geometric distribution prior
for $N_j$.

## Probability function implementation

With the help of the functions `lbeta`, `lchoose`, and `dgeom`, define (in `my_code.R`)
a function `log_prob_NY(N,Y,xi,a,b)` with arguments `N` (a vector of length $J$) and $Y$
(a data frame of size $J\times 2$, formatted like the output from `arch_data(J)`).
The arguments `xi`, `a`, `b` are the model hyperparameters $\xi$, $a$, and $b$.
The function should return the logarithm of $p(\bm{N},\bm{Y})$ when
the combination of $\bm{N}$ and $\bm{Y}$ is valid, and should otherwise return `-Inf`.

## Importance sampling function

The aim is to construct a Bayesian credible interval for each $N_j$ using
importance sampling, similarly to the method used in lab 4. There, a Gaussian
approximation of the posterior of a parameter was used. Here, we will instead use
the prior distribution for $\bm{N}$ as sampling distribution, with
samples $N_j^{[k]}\sim\pGeom(\xi)$.

Since the observations $\bm{Y}$ are fixed, the posterior probability function $p(\bm{N}|\bm{Y})$ for $\bm{N}$ is proportional to $p(\bm{N},\bm{Y})$.

We define the logarithm of unnormalised importance weights, $\log[w^{[k]}]=\log[p(\bm{N}^{[k]},\bm{Y})]-\log[p(\bm{N}^{[k]})]$,
where $p(\bm{N}^{[k]})$ is the
product of $J$ geometric distribution probabilities.

Define (in `my_code.R`) a function `arch_importance(K,Y,xi,a,b)` with the same arguments `Y`, 
`xi`, `a`, and `b` as before, but with the first argument `K` defining the number of
samples to generate. The function should return a data frame with $K$ rows and
$J+1$ columns,
named `N1`, `N2`, etc, for each $j=1,\dots,J$, containing samples from the prior distributions for $N_j$,
and a final column called `Log_Weights`,
containing re-normalised log-importance-weights, constructed by shifting $\log[w^{[k]}]$ by
subtracting the largest $\log[w^{[k]}]$ value, as in lab 4.

## Credible intervals for $N_j$

As in the lab, let $\xi=1/1001$, and $a=b=1/2$.
Use `arch_importance` and the `wquantile` function with `type=1` (see the
help text for information) to compute credible intervals for $N_j$.
Start by including only the data from a single excavation, and observe how the
interval for $N_1$ changes when data from the other excavations are added.
Present and discuss the results.

## Credible intervals for $\phi$

Update^[Note: your final submission should only contain the updated function.] the `arch_importance` function to add a column `Phi` with samples from the
conditional distribution $\pBeta(\wt{a},\wt{b})$ for each row of the importance sample.
Use the updated function to construct credible intervals for $\phi$, one for $J=1$, and one for $J=4$. Present and discuss the results.
